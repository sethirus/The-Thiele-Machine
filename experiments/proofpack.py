"""Archivist utilities for constructing proofpack manifests and receipts.

This module centralises the tasks performed by the Archivist role in the
zero-trust workflow.  It provides helpers that

* collect deterministic file metadata for proofpack artefacts,
* emit a canonical ``manifest.json`` with SHA-256 digests,
* down-sample the verifier aggregate into a human-readable summary, and
* seal a receipt signed with an Ed25519 key so auditors can verify the pack
  without trusting textual claims.

All JSON outputs use sorted keys and explicit ``\n`` terminators so that the
generated bytes remain stable across reruns.  The helpers avoid any network
access and operate purely on caller-provided files.
"""

from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime, timezone
import hashlib
import json
from pathlib import Path
from typing import Iterable, Mapping, MutableMapping, Sequence

from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PrivateKey

from tools import receipts


@dataclass(frozen=True)
class ManifestEntry:
    """Metadata for a single file captured inside a proofpack."""

    relative_path: str
    sha256: str
    size_bytes: int


def _ensure_relative(path: Path, root: Path) -> str:
    try:
        return path.relative_to(root).as_posix()
    except ValueError:
        return path.as_posix()


def compute_sha256(path: Path) -> str:
    """Return the SHA-256 digest for ``path`` using buffered IO."""

    digest = hashlib.sha256()
    with path.open("rb") as handle:
        for chunk in iter(lambda: handle.read(65536), b""):
            digest.update(chunk)
    return digest.hexdigest()


def collect_manifest_entries(root: Path, files: Sequence[Path]) -> list[ManifestEntry]:
    """Return sorted manifest entries for ``files`` relative to ``root``."""

    root = root.resolve()
    entries: list[ManifestEntry] = []
    for file_path in files:
        path = file_path.resolve()
        if not path.exists():
            raise FileNotFoundError(f"Manifest attachment does not exist: {path}")
        relative = _ensure_relative(path, root)
        entries.append(
            ManifestEntry(
                relative_path=relative,
                sha256=compute_sha256(path),
                size_bytes=path.stat().st_size,
            )
        )
    entries.sort(key=lambda entry: entry.relative_path)
    return entries


def _manifest_digest(entries: Sequence[ManifestEntry]) -> str:
    digest = hashlib.sha256()
    for entry in entries:
        digest.update(entry.relative_path.encode("utf-8"))
        digest.update(b"\0")
        digest.update(entry.sha256.encode("ascii"))
        digest.update(b"\0")
        digest.update(str(entry.size_bytes).encode("ascii"))
        digest.update(b"\n")
    return digest.hexdigest()


def _format_timestamp(value: datetime | str | None) -> str:
    if value is None:
        timestamp = datetime.now(timezone.utc)
    elif isinstance(value, datetime):
        timestamp = value.astimezone(timezone.utc)
    else:
        return str(value)
    if timestamp.tzinfo is None:
        timestamp = timestamp.replace(tzinfo=timezone.utc)
    return timestamp.isoformat().replace("+00:00", "Z")


def write_manifest(
    root: Path,
    entries: Sequence[ManifestEntry],
    *,
    run_tag: str,
    metadata: Mapping[str, object] | None = None,
    out_path: Path | None = None,
    created_at: datetime | str | None = None,
) -> Path:
    """Write ``manifest.json`` capturing ``entries`` relative to ``root``."""

    manifest: MutableMapping[str, object] = {
        "spec_version": "1.0",
        "run_tag": run_tag,
        "created_at": _format_timestamp(created_at),
        "entries": [
            {
                "path": entry.relative_path,
                "sha256": entry.sha256,
                "bytes": entry.size_bytes,
            }
            for entry in entries
        ],
        "manifest_digest_sha256": _manifest_digest(entries),
    }
    if metadata:
        manifest["metadata"] = dict(metadata)

    target = out_path or (root / "manifest.json")
    target.parent.mkdir(parents=True, exist_ok=True)
    with target.open("w", encoding="utf-8") as handle:
        json.dump(manifest, handle, indent=2, sort_keys=True)
        handle.write("\n")
    return target


def build_summary_payload(aggregated: Mapping[str, object]) -> MutableMapping[str, object]:
    """Condense the unified verifier payload into an archivist summary."""

    summary: MutableMapping[str, object] = {
        "status": bool(aggregated.get("status")),
        "parameters": {
            key: aggregated.get(key)
            for key in ("epsilon", "delta", "eta", "delta_aic")
            if key in aggregated
        },
        "phases": {},
    }

    phases_obj = aggregated.get("phases", {})
    if isinstance(phases_obj, Mapping):
        for phase_name, payload in sorted(phases_obj.items()):
            if not isinstance(payload, Mapping):
                continue
            runs = payload.get("runs", [])
            failing = []
            if isinstance(runs, Iterable):
                for run in runs:
                    if not isinstance(run, Mapping):
                        continue
                    if not run.get("status", False):
                        failing.append({
                            "run_dir": run.get("run_dir"),
                            "error": run.get("error"),
                        })
            summary["phases"][phase_name] = {
                "status": bool(payload.get("status")),
                "run_count": len(payload.get("runs", [])) if isinstance(runs, Iterable) else 0,
                "failing_runs": failing,
            }
    public_obj = aggregated.get("public_data")
    if isinstance(public_obj, Mapping):
        datasets = public_obj.get("datasets", [])
        failing: list[dict[str, object | None]] = []
        dataset_count = 0
        if isinstance(datasets, Sequence):
            dataset_count = len(datasets)
            for dataset in datasets:
                if not isinstance(dataset, Mapping):
                    continue
                if not dataset.get("status", True):
                    failing.append(
                        {
                            "dataset_dir": dataset.get("dataset_dir"),
                            "error": dataset.get("error"),
                        }
                    )
        summary["public_data"] = {
            "status": bool(public_obj.get("status")),
            "dataset_count": dataset_count,
            "failing_datasets": failing,
        }
    turbulence_obj = aggregated.get("turbulence")
    if isinstance(turbulence_obj, Mapping):
        datasets = turbulence_obj.get("datasets", [])
        failing: list[dict[str, object | None]] = []
        dataset_count = 0
        if isinstance(datasets, Sequence):
            dataset_count = len(datasets)
            for dataset in datasets:
                if not isinstance(dataset, Mapping):
                    continue
                if not dataset.get("status", True):
                    failing.append(
                        {
                            "dataset_dir": dataset.get("dataset_dir"),
                            "error": dataset.get("error"),
                        }
                    )
        summary["turbulence"] = {
            "status": bool(turbulence_obj.get("status")),
            "dataset_count": dataset_count,
            "failing_datasets": failing,
        }
    return summary


def write_summary(payload: Mapping[str, object], path: Path) -> Path:
    """Write ``summary.json`` with canonical formatting."""

    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as handle:
        json.dump(dict(payload), handle, indent=2, sort_keys=True)
        handle.write("\n")
    return path


def _format_number(value: object, *, style: str = "scalar") -> str:
    try:
        number = float(value)
    except (TypeError, ValueError):
        return str(value)
    if style == "percent":
        return f"{number * 100:.1f}%"
    if number == 0.0:
        return "0"
    magnitude = abs(number)
    if magnitude < 1e-3 or magnitude >= 1e3:
        return f"{number:.2e}"
    return f"{number:.3g}"


def _format_highlight_lines(phase: str, highlights: Mapping[str, object] | None) -> list[str]:
    if not isinstance(highlights, Mapping) or not highlights:
        return []
    lines: list[str] = []

    def add(line: str) -> None:
        if line:
            lines.append(line)

    if phase == "landauer":
        if "max_bits_gap" in highlights:
            add(f"max |ΔW/(kT ln 2) − Σμ| = {_format_number(highlights['max_bits_gap'])} bits")
        if "mean_bits_gap" in highlights:
            add(f"mean |ΔW/(kT ln 2) − Σμ| = {_format_number(highlights['mean_bits_gap'])} bits")
        if "tolerance_pass_rate" in highlights:
            add(f"tolerance pass rate = {_format_number(highlights['tolerance_pass_rate'], style='percent')}")
        if "metadata_digest_ok" in highlights:
            add("metadata digests verified" if highlights["metadata_digest_ok"] else "metadata digest mismatch")
    elif phase == "einstein":
        if "max_abs_residual" in highlights:
            add(f"max |D − μkT| = {_format_number(highlights['max_abs_residual'])}")
        if "mean_abs_residual" in highlights:
            add(f"mean |D − μkT| = {_format_number(highlights['mean_abs_residual'])}")
        if "max_abs_drift_velocity" in highlights:
            add(f"max |drift| = {_format_number(highlights['max_abs_drift_velocity'])}")
        if "metadata_digest_ok" in highlights:
            add("metadata digests verified" if highlights["metadata_digest_ok"] else "metadata digest mismatch")
    elif phase == "entropy":
        low = highlights.get("min_slope_ci")
        high = highlights.get("max_slope_ci")
        if low is not None and high is not None:
            add(f"Theil–Sen slope CI ≈ [{_format_number(low)}, {_format_number(high)}]")
        if "min_rho" in highlights:
            add(f"min Spearman ρ = {_format_number(highlights['min_rho'])}")
        if "max_p_value" in highlights:
            add(f"max Spearman p-value = {_format_number(highlights['max_p_value'])}")
        if "metadata_digest_ok" in highlights:
            add("metadata digests verified" if highlights["metadata_digest_ok"] else "metadata digest mismatch")
    elif phase == "cwd":
        if "min_penalty_margin_bits" in highlights:
            add(f"min (Δwork − I) = {_format_number(highlights['min_penalty_margin_bits'])} bits")
        if "min_blind_penalty_bits" in highlights:
            add(f"min blind penalty = {_format_number(highlights['min_blind_penalty_bits'])} bits")
        if "destroyed_guard_triggered" in highlights and highlights["destroyed_guard_triggered"]:
            add("destroyed guard triggered (success capped)")
        if "metadata_digest_ok" in highlights:
            add("metadata digests verified" if highlights["metadata_digest_ok"] else "metadata digest mismatch")
    elif phase == "cross_domain":
        if "blind_min_delta_aic" in highlights:
            add(f"blind ΔAIC ≥ {_format_number(highlights['blind_min_delta_aic'])}")
        if "sighted_max_abs_slope" in highlights:
            add(f"sighted |slope| ≤ {_format_number(highlights['sighted_max_abs_slope'])}")
        if "destroyed_mean_structure" in highlights:
            add(f"destroyed structure ≈ {_format_number(highlights['destroyed_mean_structure'])}")
        if "metadata_digest_ok" in highlights:
            add("metadata digests verified" if highlights["metadata_digest_ok"] else "metadata digest mismatch")
    elif phase == "public_data":
        add(f"datasets analysed = {int(highlights.get('dataset_count', 0))}")
        if "max_oos_error" in highlights:
            add(f"max OOS error = {_format_number(highlights['max_oos_error'], style='percent')}")
        if "blind_min_delta_aic" in highlights:
            add(f"min blind ΔAIC = {_format_number(highlights['blind_min_delta_aic'])}")
        if "destroyed_max_rho" in highlights:
            add(f"max destroyed ρ = {_format_number(highlights['destroyed_max_rho'])}")
    elif phase == "public_data_dataset":
        if "oos_error" in highlights:
            add(f"OOS error = {_format_number(highlights['oos_error'], style='percent')}")
        if "blind_delta_aic" in highlights:
            add(f"blind ΔAIC = {_format_number(highlights['blind_delta_aic'])}")
        slope_ci = highlights.get("sighted_slope_ci")
        if isinstance(slope_ci, Sequence) and len(slope_ci) >= 2:
            add(f"sighted slope CI ≈ [{_format_number(slope_ci[0])}, {_format_number(slope_ci[1])}]")
        if "destroyed_rho" in highlights:
            add(f"destroyed ρ = {_format_number(highlights['destroyed_rho'])}")
    elif phase == "turbulence":
        add(f"datasets analysed = {int(highlights.get('dataset_count', 0))}")
        if "pass_rate" in highlights:
            add(f"pass rate = {_format_number(highlights['pass_rate'], style='percent')}")
        if "min_delta_aic" in highlights:
            add(f"min blind ΔAIC = {_format_number(highlights['min_delta_aic'])}")
        if "mean_rho_drop" in highlights:
            add(f"mean ρ drop = {_format_number(highlights['mean_rho_drop'])}")
    elif phase == "turbulence_dataset":
        if "sighted_rho" in highlights:
            add(f"sighted ρ = {_format_number(highlights['sighted_rho'])}")
        if "sighted_runtime_slope" in highlights:
            add(f"sighted runtime slope = {_format_number(highlights['sighted_runtime_slope'])}")
        if "blind_delta_aic" in highlights:
            add(f"blind ΔAIC = {_format_number(highlights['blind_delta_aic'])}")
        if "rho_drop" in highlights:
            add(f"ρ drop = {_format_number(highlights['rho_drop'])}")
    return lines


def render_human_summary(
    aggregated: Mapping[str, object],
    manifest: Mapping[str, object],
    *,
    protocol_notes: Sequence[str] | None = None,
) -> str:
    """Return a Markdown narrative describing the bundled proofpack."""

    status_flag = "PASS" if aggregated.get("status") else "FAIL"
    run_tag = manifest.get("run_tag", "unknown")
    created_at = manifest.get("created_at", "unknown")

    thresholds: Mapping[str, object] = aggregated.get("parameters", {})  # type: ignore[assignment]
    if not isinstance(thresholds, Mapping):
        thresholds = {}

    attachment_entries = manifest.get("entries", [])
    attachment_count = 0
    if isinstance(attachment_entries, Sequence):
        attachment_count = len(attachment_entries)

    lines: list[str] = []
    lines.append(f"# Thiele proofpack summary — {run_tag}")
    lines.append("")
    lines.append(f"- **Verdict:** {status_flag}")
    lines.append(f"- **Created:** {created_at}")
    if attachment_count:
        lines.append(f"- **Bundled artefacts:** {attachment_count}")

    if thresholds:
        lines.append("- **Verifier thresholds:**")
        for key, value in sorted(thresholds.items()):
            lines.append(f"  - {key}: {value}")

    if protocol_notes:
        lines.append("- **Notes:**")
        for note in protocol_notes:
            lines.append(f"  - {note}")

    phases_obj = aggregated.get("phases", {})
    if isinstance(phases_obj, Mapping) and phases_obj:
        lines.append("")
        lines.append("## Phase diagnostics")
        for phase_name, payload in sorted(phases_obj.items()):
            if not isinstance(payload, Mapping):
                continue
            phase_status = "PASS" if payload.get("status") else "FAIL"
            lines.append("")
            lines.append(f"### {phase_name.replace('_', ' ').title()} — {phase_status}")

            run_entries = payload.get("runs")
            if isinstance(run_entries, Sequence) and run_entries:
                for run in run_entries:
                    if not isinstance(run, Mapping):
                        continue
                    run_status = "PASS" if run.get("status") else "FAIL"
                    label = run.get("protocol") or run.get("run_dir") or "run"
                    details: list[str] = []
                    if "trial_count" in run and run.get("trial_count") is not None:
                        details.append(f"trials={run['trial_count']}")
                    if "verifier_json" in run:
                        details.append(f"verifier={run['verifier_json']}")
                    if run.get("error"):
                        details.append(f"error={run['error']}")
                    joined = ", ".join(str(item) for item in details if item)
                    if joined:
                        lines.append(f"- {label}: {run_status} ({joined})")
                    else:
                        lines.append(f"- {label}: {run_status}")
                    for highlight in _format_highlight_lines(phase_name, run.get("highlights")):
                        lines.append(f"  - {highlight}")

            protocol_runs = payload.get("protocol_runs")
            if isinstance(protocol_runs, Mapping) and protocol_runs:
                lines.append("- Protocol directories:")
                for protocol, rel_path in sorted(protocol_runs.items()):
                    lines.append(f"  - {protocol}: {rel_path}")

            if payload.get("error"):
                lines.append(f"- Error: {payload['error']}")

            for highlight in _format_highlight_lines(phase_name, payload.get("highlights")):
                lines.append(f"- Highlight: {highlight}")

    failing_sections: list[str] = []
    if isinstance(phases_obj, Mapping):
        for phase_name, payload in phases_obj.items():
            if not isinstance(payload, Mapping):
                continue
            runs = payload.get("runs", [])
            if isinstance(runs, Sequence):
                for run in runs:
                    if isinstance(run, Mapping) and not run.get("status", True):
                        failing_sections.append(f"{phase_name}:{run.get('protocol') or run.get('run_dir')}")
            if not payload.get("status", True):
                failing_sections.append(phase_name)

    lines.append("")
    lines.append("## Verdict")
    if failing_sections:
        lines.append("One or more verifiers failed: " + ", ".join(sorted(failing_sections)))
    else:
        lines.append("All verifiers report THIELE_OK for the bundled artefacts.")

    public_data_obj = aggregated.get("public_data")
    if isinstance(public_data_obj, Mapping):
        lines.append("")
        lines.append("## Public data")
        for highlight in _format_highlight_lines("public_data", public_data_obj.get("highlights")):
            lines.append(f"- {highlight}")
        datasets = public_data_obj.get("datasets")
        if isinstance(datasets, Sequence) and datasets:
            for dataset in datasets:
                if not isinstance(dataset, Mapping):
                    continue
                dataset_label = dataset.get("dataset") or dataset.get("dataset_dir") or "dataset"
                status_label = "PASS" if dataset.get("status") else "FAIL"
                verifier = dataset.get("verifier_json")
                if verifier:
                    lines.append(f"- {dataset_label}: {status_label} (verifier={verifier})")
                else:
                    lines.append(f"- {dataset_label}: {status_label}")
                for highlight in _format_highlight_lines("public_data_dataset", dataset.get("highlights")):
                    lines.append(f"  - {highlight}")
                if dataset.get("error"):
                    lines.append(f"  - error={dataset['error']}")

    turbulence_obj = aggregated.get("turbulence")
    if isinstance(turbulence_obj, Mapping):
        lines.append("")
        lines.append("## Turbulence data")
        for highlight in _format_highlight_lines("turbulence", turbulence_obj.get("highlights")):
            lines.append(f"- {highlight}")
        datasets = turbulence_obj.get("datasets")
        if isinstance(datasets, Sequence) and datasets:
            for dataset in datasets:
                if not isinstance(dataset, Mapping):
                    continue
                dataset_label = dataset.get("dataset") or dataset.get("dataset_dir") or "dataset"
                status_label = "PASS" if dataset.get("status") else "FAIL"
                verifier = dataset.get("verifier_json")
                if verifier:
                    lines.append(f"- {dataset_label}: {status_label} (verifier={verifier})")
                else:
                    lines.append(f"- {dataset_label}: {status_label}")
                for highlight in _format_highlight_lines("turbulence_dataset", dataset.get("highlights")):
                    lines.append(f"  - {highlight}")
                if dataset.get("error"):
                    lines.append(f"  - error={dataset['error']}")

    return "\n".join(lines).rstrip() + "\n"


def write_human_summary(text: str, path: Path) -> Path:
    """Persist a Markdown human summary with canonical newline termination."""

    path.parent.mkdir(parents=True, exist_ok=True)
    if not text.endswith("\n"):
        text = text + "\n"
    path.write_text(text, encoding="utf-8")
    return path


def write_protocol_document(
    thresholds: Mapping[str, object],
    path: Path,
    *,
    notes: Sequence[str] | None = None,
) -> Path:
    """Emit ``protocol.json`` capturing verifier tolerances and notes."""

    payload: MutableMapping[str, object] = {
        "spec_version": "1.0",
        "thresholds": dict(thresholds),
    }
    if notes:
        payload["notes"] = list(notes)

    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as handle:
        json.dump(payload, handle, indent=2, sort_keys=True)
        handle.write("\n")
    return path


@dataclass
class ReceiptSigner:
    """Lightweight Ed25519 helper for sealing receipt digests."""

    private_key: Ed25519PrivateKey

    @classmethod
    def from_private_bytes(cls, data: bytes) -> "ReceiptSigner":
        return cls(Ed25519PrivateKey.from_private_bytes(data))

    @classmethod
    def from_hex(cls, hex_key: str) -> "ReceiptSigner":
        return cls.from_private_bytes(bytes.fromhex(hex_key))

    @classmethod
    def load(cls, path: Path) -> "ReceiptSigner":
        return cls.from_private_bytes(path.read_bytes())

    @property
    def public_key_hex(self) -> str:
        public_bytes = self.private_key.public_key().public_bytes(
            encoding=serialization.Encoding.Raw,
            format=serialization.PublicFormat.Raw,
        )
        return public_bytes.hex()

    def sign_digest(self, digest_hex: str) -> str:
        return self.private_key.sign(bytes.fromhex(digest_hex)).hex()


def write_receipt(
    entries: Sequence[ManifestEntry],
    signer: ReceiptSigner,
    *,
    manifest_digest: str,
    run_tag: str,
    out_path: Path,
) -> Path:
    """Create a μ-ledger style receipt for the provided manifest entries."""

    steps = []
    step_hashes: list[str] = []
    for idx, entry in enumerate(entries):
        step = {
            "idx": idx,
            "transition": "manifest_entry",
            "run_tag": run_tag,
            "artifact_path": entry.relative_path,
            "sha256": entry.sha256,
            "bytes": entry.size_bytes,
            "mu_delta": 0.0,
        }
        step_hash = receipts.compute_step_hash(step)
        step_with_hash = dict(step)
        step_with_hash["step_hash"] = step_hash
        steps.append(step_with_hash)
        step_hashes.append(step_hash)

    digest_hex = receipts.compute_global_digest(step_hashes)
    receipt = {
        "spec_version": "1.0",
        "kernel_pubkey": signer.public_key_hex,
        "steps": steps,
        "global_digest": digest_hex,
        "signature": signer.sign_digest(digest_hex),
        "manifest_digest_sha256": manifest_digest,
        "run_tag": run_tag,
    }

    out_path.parent.mkdir(parents=True, exist_ok=True)
    with out_path.open("w", encoding="utf-8") as handle:
        json.dump(receipt, handle, indent=2, sort_keys=True)
        handle.write("\n")
    return out_path


__all__ = [
    "ManifestEntry",
    "ReceiptSigner",
    "build_summary_payload",
    "collect_manifest_entries",
    "compute_sha256",
    "write_manifest",
    "write_protocol_document",
    "write_receipt",
    "write_summary",
]

