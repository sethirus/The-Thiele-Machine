# Halting boundary verification

This report packages the regression harness that checks whether the Python
Thiele VM leaks any information beyond a straightforward step-bounded
interpreter.  All artefacts are generated by the scripts under `tools/` so a
fresh checkout can reproduce the evidence without manual intervention.

## Curated stress tests

`tools/halting_stress_test.py` exercises five toy register-machine programs that
probe halting behaviour from multiple angles:

| Program | Scenario | Expected outcome |
| --- | --- | --- |
| `halt_immediately` | Terminates in one step without touching state. | Halts |
| `countdown_three` | Decrements a counter to zero then halts. | Halts |
| `self_loop` | Immediate `GOTO 0` self-loop. | Loops |
| `branch_on_input` | Halts iff a flag register starts at zero. | Loops under the recorded input |
| `diagonal_like` | Branches on its own encoding, mimicking a diagonal self-query. | Halts under the recorded input |

Running the harness with a 32-step budget records the baseline interpreter’s
verdict, the VM verdict, and whether they agree for every program.  The latest
JSON artefact lives at `artifacts/halting/results.json`.

## Enumerative survey

`tools/halting_survey.py` augments the curated set by randomly sampling Thiele
programs up to a configurable length, interpreting them with the same
baseline-versus-VM comparison.  The default configuration samples 40 programs of
length ≤ 3 with a 64-step budget.  Summary statistics and any anomalies are
written to `artifacts/halting/survey_summary.json`.

## Verification entry point

`tools/verify_halting_boundary.py` regenerates both artefacts, prints a compact
summary table, and exits with status 1 if *any* disagreement appears.  It is also
invoked by `tools/verify_end_to_end.py` so the halting boundary is enforced in
the integrated regression run.  A passing invocation certifies that, within the
configured bounds, the VM and baseline interpreter remain in lockstep.
