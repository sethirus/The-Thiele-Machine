\section{Physics Models and Algorithmic Primitives}

% ============================================================================
% FIGURE: Chapter Roadmap
% ============================================================================
\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=1.8, 
    node distance=3cm,
    model/.style={rectangle, draw, rounded corners, minimum width=4.6cm, minimum height=1.4cm, align=center, fill=blue!10},
    algo/.style={rectangle, draw, rounded corners, minimum width=4.6cm, minimum height=1.4cm, align=center, fill=green!15},
    bridge/.style={rectangle, draw, rounded corners, minimum width=4.6cm, minimum height=1.4cm, align=center, fill=yellow!20},
    arrow/.style={->, >=Stealth, thick}
]
    % Physics models
    \node[font=\normalsize\bfseries] at (-3, 2.5) {Physics Models};
    \node[model, align=center, text width=3.5cm] (wave) at (-4, 1.5) {Wave\\Propagation};
    \node[model, align=center, text width=3.5cm] (diss) at (-2, 1.5) {Dissipative\\Systems};
    \node[model, align=center, text width=3.5cm] (discrete) at (-3, 0.5) {Discrete\\Lattices};
    
    % Algorithmic primitives
    \node[font=\normalsize\bfseries] at (3, 2.5) {Algorithmic Primitives};
    \node[algo, align=center, text width=3.5cm] (period) at (2, 1.5) {Period\\Finding};
    \node[algo, align=center, text width=3.5cm] (gcd) at (4, 1.5) {Euclidean\\GCD};
    \node[algo, align=center, text width=3.5cm] (mod) at (3, 0.5) {Modular\\Arithmetic};
    
    % Bridge
    \node[bridge, minimum width=7.2cm, align=center, text width=3.5cm] (bridge) at (0, -1) {\textbf{Bridge Modules}\\Domain $\rightarrow$ Kernel};
    
    % Arrows
    \draw[arrow, shorten >=2pt, shorten <=2pt] (wave) -- (bridge);
    \draw[arrow, shorten >=2pt, shorten <=2pt] (diss) -- (bridge);
    \draw[arrow, shorten >=2pt, shorten <=2pt] (discrete) -- (bridge);
    \draw[arrow, shorten >=2pt, shorten <=2pt] (period) -- (bridge);
    \draw[arrow, shorten >=2pt, shorten <=2pt] (gcd) -- (bridge);
    \draw[arrow, shorten >=2pt, shorten <=2pt] (mod) -- (bridge);
    
    % Conservation laws
    \node[draw, rounded corners, fill=gray!10, font=\normalsize, text width=3cm, align=center] at (-3, -0.5) {Conservation\\laws proven};
    
    % Shor reduction
    \node[draw, rounded corners, fill=gray!10, font=\normalsize, text width=3cm, align=center] at (3, -0.5) {Shor reduction\\formalized};
\end{tikzpicture}
\caption{Chapter D roadmap: physics models with conservation laws and algorithmic primitives with Shor reduction, connected via bridge modules.}

\paragraph{Understanding Figure~\ref{fig:ch12-roadmap}:}

This roadmap diagram visualizes the dual nature of Chapter~12 (Appendix D): connecting \textbf{abstract physics models} with \textbf{concrete algorithmic primitives}, both grounded through \textbf{bridge modules} that translate domain-specific concepts into kernel semantics. This chapter demonstrates that computation is not merely abstract mathematics but a \textit{physical} process subject to physical laws (Landauer principle, locality, conservation), while simultaneously formalizing quantum-inspired algorithms (Shor's factoring) that exploit partition structure for exponential speedups.

\textbf{Visual elements:} The diagram shows two symmetric columns: \textbf{left side} labeled ``Physics Models'' contains three blue boxes (``Wave Propagation,'' ``Dissipative Systems,'' ``Discrete Lattices'') with gray annotation box below stating ``Conservation laws proven''; \textbf{right side} labeled ``Algorithmic Primitives'' contains three green boxes (``Period Finding,'' ``Euclidean GCD,'' ``Modular Arithmetic'') with gray annotation box below stating ``Shor reduction formalized.'' At the bottom center is a large yellow box labeled ``\textbf{Bridge Modules} Domain $\rightarrow$ Kernel'' spanning the width. Black arrows point from all six model/primitive boxes toward the central bridge box, indicating both physics models and algorithmic primitives require bridging to kernel semantics.

\textbf{The two columns and bridge infrastructure:}

\begin{itemize}
    \item \textbf{Physics Models (left column, 3 blue boxes):} Formally verified Coq models demonstrating physical laws emerge from computational structure. These are \textit{not} metaphors but machine-checked proofs showing computational dynamics exhibit physics-like behavior: (1) \textbf{Wave Propagation:} 1D wave dynamics model with left/right-moving amplitudes on discrete lattice. Proven conservation laws: energy $E = \sum_i (L_i^2 + R_i^2)$ conserved, momentum $P = \sum_i (R_i - L_i)$ conserved, dynamics reversible ($\texttt{wave\_step\_inv}(\texttt{wave\_step}(s)) = s$). Implementation: \texttt{WaveCell} record with \texttt{left\_amp}/\texttt{right\_amp} fields, \texttt{wave\_step} function using \texttt{rotate\_left}/\texttt{rotate\_right}, theorems \texttt{wave\_energy\_conserved}, \texttt{wave\_momentum\_conserved}, \texttt{wave\_step\_reversible} in \path{coq/physics/WaveModel.v}. Embedding into kernel proven in \path{coq/thielemachine/coqproofs/WaveEmbedding.v}. (2) \textbf{Dissipative Systems:} Model of irreversible dynamics connecting to $\mu$-monotonicity (entropy increase, information erasure). Captures systems where energy dissipates as heat (Landauer principle validation). (3) \textbf{Discrete Lattices:} Model of emergent spacetime from computational steps (discrete spacetime as lattice-based dynamics). Gray annotation ``Conservation laws proven'' confirms these models have formal proofs of conservation (energy/momentum for wave, entropy for dissipative, locality for lattice).
    
    \item \textbf{Algorithmic Primitives (right column, 3 green boxes):} Concrete number-theoretic algorithms forming the mathematical foundation of Shor's factoring algorithm, all formally verified in Coq: (1) \textbf{Period Finding:} Core subroutine of Shor's algorithm finding smallest $r$ such that $a^r \equiv 1 \pmod{N}$. Definitions: \texttt{is\_period(r)} proposition ($r > 0 \land \forall k, \texttt{pow\_mod}(k+r) = \texttt{pow\_mod}(k)$), \texttt{minimal\_period(r)} (smallest valid period), \texttt{shor\_candidate(r)} computing $\gcd(a^{r/2} - 1, N)$ as potential factor. Example: factoring $N=21$ with $a=2$ finds period $r=6$, computes $\gcd(2^3-1, 21) = \gcd(7, 21) = 7$, extracts factors $3 \times 7$. (2) \textbf{Euclidean GCD:} Classical algorithm computing greatest common divisor in $O(\log \min(a,b))$ time. Implementation: recursive \texttt{gcd\_euclid(a, b)} with base case $b=0 \to a$, recursive case $b>0 \to \gcd(b, a \bmod b)$. Proven theorems: \texttt{gcd\_euclid\_divides\_left} ($\gcd(a,b) | a$), \texttt{gcd\_euclid\_divides\_right} ($\gcd(a,b) | b$). (3) \textbf{Modular Arithmetic:} Efficient modular exponentiation via repeated squaring. Definition: \texttt{mod\_pow(n, base, exp)} computes $\text{base}^{\text{exp}} \bmod n$ in $O(\log \text{exp})$ time avoiding overflow. Proven lemma: \texttt{mod\_pow\_mult} (exponents add: $a^{b+c} \equiv a^b \cdot a^c \pmod{n}$). Gray annotation ``Shor reduction formalized'' confirms the mathematical heart of Shor's algorithm is machine-verified: given period $r$, extract factors via GCD (theorem \texttt{shor\_reduction} in \path{coq/shor\_primitives/PeriodFinding.v}).
    
    \item \textbf{Bridge Modules (central yellow box, bottom):} Infrastructure connecting high-level domain concepts to low-level kernel traces via receipt channels. Bridge modules define: (1) channel selectors (opcode-based filtering: e.g., \texttt{RAND\_TRIAL\_OP := 1001}), (2) payload extraction from matching receipts, (3) decode lemmas proving filter-map equivalence (e.g., \texttt{decode\_is\_filter\_payloads} for randomness bridge in \path{coq/bridge/Randomness\_to\_Kernel.v}). Six bridge files total: randomness (C-RAND), tomography (C-TOMO), entropy (C-ENTROPY), causation (C-CAUSAL), wave embedding, Shor reduction. Arrows from all six model/primitive boxes converge on bridge box indicating both physics models and algorithmic primitives require bridging: wave model embeds into kernel via partition structure (each cell becomes module, conservation laws transfer), Shor primitives bridge via receipt-annotated traces (period-finding steps emit receipts, verifier reconstructs computation).
\end{itemize}

\textbf{Key insight visualized:} Chapter~12 establishes the Thiele Machine operates at the intersection of physics and algorithms: \textit{downward} (physics models show computational structure exhibits physical laws like conservation, reversibility, locality), \textit{upward} (algorithmic primitives show domain-specific algorithms like Shor's factoring formalize as kernel traces), \textit{bridging} (bridge modules make both connections explicit and verifiable). This dual perspective validates two core thesis claims: (1) computation \textit{is} physics (not metaphor---wave dynamics, dissipation, spacetime emergence are machine-checked proofs), (2) quantum-inspired algorithms work via partition structure revelation (Shor's exponential speedup comes from revealing period structure, which costs $\mu$).

\textbf{How to read this diagram:} Start with the left column ``Physics Models'' showing three blue boxes: Wave Propagation (left/right amplitudes with conserved energy/momentum), Dissipative Systems (irreversible dynamics with $\mu$-monotonicity), Discrete Lattices (emergent spacetime from computational steps). Gray annotation below confirms ``Conservation laws proven'' for all three models. Move to right column ``Algorithmic Primitives'' showing three green boxes: Period Finding (core of Shor's algorithm finding $r$ where $a^r \equiv 1 \pmod{N}$), Euclidean GCD (classical algorithm computing $\gcd$ in $O(\log \min(a,b))$ time), Modular Arithmetic (efficient exponentiation avoiding overflow). Gray annotation below confirms ``Shor reduction formalized'' as machine-verified theorem connecting period to factors. Both columns converge via arrows on central yellow ``Bridge Modules'' box at bottom, indicating physics models and algorithmic primitives both require explicit translation to kernel semantics via receipt channels and decode lemmas. The bridge makes abstract models (wave dynamics, period finding) \textit{executable} and \textit{verifiable} in kernel traces.

\textbf{Role in thesis:} This diagram establishes Chapter~12's organizing principle: demonstrate computation-physics duality through formal models. The physics models (wave/dissipative/lattice) validate the claim that $\mu$-conservation mirrors thermodynamic laws (Landauer principle: erasure costs $\mu$, validated experimentally in Chapter~11; locality: partition boundaries enforce no-signaling; reversibility: wave dynamics are invertible). The algorithmic primitives (period finding/GCD/modular arithmetic) formalize Shor's algorithm as partition-aware computation: finding period $r$ of $a^k \bmod N$ reveals multiplicative structure (costs $\mu$ for revelation), enabling polynomial-time factoring where classical methods require sub-exponential time. Bridge modules ensure both models and algorithms are \textit{not} informal analogies but \textit{executable kernel traces} with verifiable receipts. This connects to: Chapter~3's kernel semantics (which bridge modules target as translation destination), Chapter~9's verifier system (which provides TRS-1.0 receipt protocol used by bridges), Chapter~10's proof corpus (which includes wave/Shor proofs as part of 206-file zero-admit corpus), Chapter~11's experiments (which validate conservation laws and complexity gaps empirically). The ``Conservation laws proven'' and ``Shor reduction formalized'' annotations emphasize these are \textit{not} claims but \textit{theorems}---machine-checked by Coq compiler.

\label{fig:ch12-roadmap}
\end{figure}

\subsection{Computation as Physics}

A central claim of this thesis is that computation is not merely an abstract mathematical process---it is a \textit{physical} process subject to physical laws. When a computer erases a bit, it dissipates heat. When it stores information, it consumes energy. The $\mu$-ledger tracks these physical costs.

To validate this connection, I develop explicit physics models within the Coq framework:
\begin{itemize}
    \item \textbf{Wave propagation}: A model of reversible dynamics with conservation laws
    \item \textbf{Dissipative systems}: A model of irreversible dynamics connecting to $\mu$-monotonicity
    \item \textbf{Discrete lattices}: A model of emergent spacetime from computational steps
\end{itemize}

These models are not metaphors---they are formally verified Coq proofs showing that computational structures exhibit physical-like behavior.
The wave model lives in \texttt{coq/physics/WaveModel.v}, and its embedding into the Thiele Machine is proven in \texttt{coq/thielemachine/coqproofs/WaveEmbedding.v}. The lattice and dissipative models follow the same pattern: define a state and step function, then prove conservation or monotonicity lemmas that can be linked back to kernel invariants.

\subsection{From Theory to Algorithms}

The second part of this chapter bridges the abstract theory to concrete algorithms. The Shor primitives demonstrate that the period-finding core of Shor's factoring algorithm can be formalized and verified in Coq, connecting:
\begin{itemize}
    \item Number theory (modular arithmetic, GCD)
    \item Computational complexity (polynomial vs.\ exponential)
    \item The Thiele Machine's $\mu$-cost model
\end{itemize}

This chapter documents the physics models that demonstrate emergent conservation laws and the algorithmic primitives that bridge abstract mathematics to concrete factorization.

\section{Physics Models}

% ============================================================================
% FIGURE: Wave Conservation
% ============================================================================
\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=1.8, 
    node distance=2.5cm,
    cell/.style={rectangle, draw, minimum width=1.4cm, minimum height=1.4cm, align=center}
]
    % Wave state
    \node[font=\normalsize\bfseries] at (-2, 2) {Wave State (1D)};
    
    % Cells
    \foreach \i/\l/\r in {0/3/1, 1/2/2, 2/1/3, 3/2/2, 4/3/1} {
        \node[cell, fill=blue!\the\numexpr\l*15\relax] at (\i, 1) {\scriptsize $\leftarrow$\l};
        \node[cell, fill=red!\the\numexpr\r*15\relax] at (\i, 0) {\scriptsize \r$\rightarrow$};
    }
    
    % Labels
    \node at (-0.8, 1) {\scriptsize L};
    \node at (-0.8, 0) {\scriptsize R};
    
    % Conservation equations
    \node[draw, rounded corners, fill=yellow!20, text width=5cm, align=center, font=\normalsize] at (2, -1.5) {
        $E = \sum (L_i^2 + R_i^2)$ conserved\\
        $P = \sum (R_i - L_i)$ conserved\\
        Step is reversible
    };
    
    % Arrow
    \draw[->, >=Stealth, very thick, shorten >=2pt, shorten <=2pt] (2, -0.3) -- (2, -0.8);
\end{tikzpicture}
\caption{Wave propagation model: left/right amplitudes propagate with conserved energy and momentum.}

\paragraph{Understanding Figure~\ref{fig:wave-model}:}

This diagram visualizes the \textbf{discrete 1D wave propagation model}: a computational model where waves propagate left and right on a lattice with \textit{provably conserved} energy and momentum. This model demonstrates that physical conservation laws (hallmarks of fundamental physics like energy/momentum conservation from Noether's theorem) \textit{emerge} from simple computational rules, supporting the thesis claim that physics is isomorphic to computation.

\textbf{Visual elements:} The diagram shows a 1D lattice with \textbf{5 cells} arranged horizontally (indices 0--4). Each cell has two rows: \textbf{upper row} labeled ``L'' (left-moving amplitudes) shown as blue-shaded boxes with leftward arrows and numbers (3$\leftarrow$, 2$\leftarrow$, 1$\leftarrow$, 2$\leftarrow$, 3$\leftarrow$), \textbf{lower row} labeled ``R'' (right-moving amplitudes) shown as red-shaded boxes with rightward arrows and numbers (1$\rightarrow$, 2$\rightarrow$, 3$\rightarrow$, 2$\rightarrow$, 1$\rightarrow$). Color intensity correlates with amplitude magnitude (darker = higher amplitude). Below the lattice is a yellow box containing three \textbf{conservation equations}: ``$E = \sum (L_i^2 + R_i^2)$ conserved'' (energy), ``$P = \sum (R_i - L_i)$ conserved'' (momentum), ``Step is reversible'' (time symmetry). A very thick black arrow points from the lattice to the conservation box, indicating these laws \textit{follow} from the lattice dynamics.

\textbf{Wave state structure and dynamics:}

\begin{itemize}
    \item \textbf{Lattice representation:} Each cell $i$ contains a \texttt{WaveCell} record with two fields: \texttt{left\_amp : nat} (amplitude of left-moving component, shown in upper blue row), \texttt{right\_amp : nat} (amplitude of right-moving component, shown in lower red row). The full state is a list of cells: \texttt{WaveState := list WaveCell}. Example shown: cell 0 has $L_0=3, R_0=1$; cell 1 has $L_1=2, R_1=2$; cell 2 has $L_2=1, R_2=3$; cell 3 has $L_3=2, R_3=2$; cell 4 has $L_4=3, R_4=1$. This represents a wave pattern with higher left-moving amplitudes at edges (cells 0,4) and higher right-moving amplitude at center (cell 2).
    
    \item \textbf{Wave step dynamics:} The \texttt{wave\_step} function evolves the lattice one time step: (1) extract all left-moving amplitudes into list $[L_0, L_1, L_2, L_3, L_4]$, (2) rotate left (shift indices down: $L_i \to L_{i-1}$ with wraparound), producing $[L_4, L_0, L_1, L_2, L_3]$, (3) extract all right-moving amplitudes into list $[R_0, R_1, R_2, R_3, R_4]$, (4) rotate right (shift indices up: $R_i \to R_{i+1}$ with wraparound), producing $[R_4, R_0, R_1, R_2, R_3]$, (5) combine rotated lists into new cells via \texttt{map2}. This models wave propagation at speed $c = 1$ cell per time step: left-movers travel left, right-movers travel right, no interaction (linear wave equation).
    
    \item \textbf{Physical interpretation:} This discrete model mimics relativistic wave propagation: left-movers are like photons moving left at light speed, right-movers like photons moving right. No interaction means linear dynamics (superposition principle holds). Wraparound boundary conditions create periodic universe (torus topology). Example evolution: a right-moving pulse at cell 1 (initial state $[(0,0), (0,1), (0,0), (0,0), (0,0)]$) propagates to cell 2 after one step ($[(0,0), (0,0), (0,1), (0,0), (0,0)]$), then cell 3 after two steps, eventually wrapping around to cell 0 after five steps.
\end{itemize}

\textbf{The three conservation laws (yellow box):}

\begin{itemize}
    \item \textbf{Energy conservation: $E = \sum_i (L_i^2 + R_i^2)$ conserved.} Total energy is sum of squared amplitudes across all cells and both directions. Theorem: \texttt{wave\_energy\_conserved : forall s, wave\_energy (wave\_step s) = wave\_energy s}. Proof: rotation preserves sum of squares (permutation of terms doesn't change total). Example: initial state shown has $E = (3^2+1^2) + (2^2+2^2) + (1^2+3^2) + (2^2+2^2) + (3^2+1^2) = 10+8+10+8+10 = 46$. After any number of steps, $E$ remains 46. Physical analogy: energy conservation follows from time-translation symmetry (Noether's theorem).
    
    \item \textbf{Momentum conservation: $P = \sum_i (R_i - L_i)$ conserved.} Total momentum is sum of signed differences: right-movers contribute positive momentum (+$R_i$), left-movers contribute negative momentum (-$L_i$). Theorem: \texttt{wave\_momentum\_conserved : forall s, wave\_momentum (wave\_step s) = wave\_momentum s}. Proof: rotation preserves signed sum (left rotation adds negative contributions, right rotation adds positive, net zero change). Example: initial state has $P = (1-3) + (2-2) + (3-1) + (2-2) + (1-3) = -2+0+2+0-2 = -2$. After any steps, $P$ remains -2 (net leftward momentum). Physical analogy: momentum conservation follows from space-translation symmetry (Noether's theorem).
    
    \item \textbf{Reversibility: Step is reversible.} The dynamics are time-symmetric: applying the inverse step $\texttt{wave\_step\_inv}$ after forward step $\texttt{wave\_step}$ recovers original state. Theorem: \texttt{wave\_step\_reversible : forall s, wave\_step\_inv (wave\_step s) = s}. Proof: inverse operation inverts rotations (\texttt{rotate\_left} inverts \texttt{rotate\_right} and vice versa). Physical analogy: fundamental physics is time-reversible (Hamiltonian dynamics, unitary quantum evolution). Irreversibility (entropy increase, $\mu$-monotonicity) emerges at coarse-grained level, not in fundamental wave dynamics.
\end{itemize}

\textbf{Key insight visualized:} This diagram proves that \textit{physical laws emerge from computational structure}, not the other way around. The wave model is defined purely computationally (Coq record + step function), yet it \textit{automatically} satisfies energy conservation, momentum conservation, and reversibility---laws discovered in physics via centuries of experiments and symmetry arguments. The arrow from lattice to conservation box emphasizes causality: computational rules (lattice + rotation) \textit{generate} physical laws (conservation equations), demonstrating physics is a \textit{consequence} of computation. This supports the thesis's radical claim: physics is not fundamental---\textit{computation is fundamental}, and physics emerges.

\textbf{How to read this diagram:} Start with the 1D lattice showing 5 cells (horizontal arrangement). Each cell has two components: upper blue box with left-moving amplitude (L) and leftward arrow, lower red box with right-moving amplitude (R) and rightward arrow. Numbers indicate amplitude values: cell 0 has $(L_0=3, R_0=1)$, cell 2 has $(L_2=1, R_2=3)$ showing wave pattern. Color intensity reflects magnitude: darker blue at cells 0,4 (high left amplitude 3), darker red at cell 2 (high right amplitude 3). Follow the very thick arrow down to yellow conservation box listing three proven laws: energy $E = \sum (L_i^2 + R_i^2)$ (sum of squared amplitudes, independent of time), momentum $P = \sum (R_i - L_i)$ (signed sum, rightward positive/leftward negative, constant), reversibility (inverse operation recovers original state, time symmetry). The diagram emphasizes these conservation laws are \textit{not} assumptions but \textit{theorems}---mechanically proven in Coq from the computational definition of \texttt{wave\_step}.

\textbf{Role in thesis:} This diagram demonstrates the Thiele Machine's computational structure \textit{generates} physical laws. The wave model validates three claims: (1) \textbf{Energy conservation:} Just as Landauer principle connects information erasure to energy dissipation (Chapter~11 experiments), the wave model shows energy conservation emerges from partition dynamics. Connection to $\mu$-conservation: in reversible dynamics (wave model), $\mu$ is conserved ($\Delta\mu = 0$); in irreversible dynamics (dissipative model, erasure), $\mu$ increases ($\Delta\mu > 0$), mirroring entropy. (2) \textbf{Locality:} Wave propagation at finite speed (1 cell per step) mirrors relativistic causality (information cannot exceed light speed). Partition boundaries in kernel enforce similar locality: modules with disjoint interfaces cannot signal instantaneously (no-signaling theorem, Chapter~5). (3) \textbf{Reversibility:} Fundamental dynamics are reversible (wave model, quantum evolution), irreversibility emerges from coarse-graining (entropy increase requires discretization, Chapter~10 impossibility theorems). The model connects to: Chapter~3's kernel semantics (wave model embeds via \path{coq/thielemachine/coqproofs/WaveEmbedding.v}, each cell becomes module, conservation laws transfer to partition structure), Chapter~10's proof corpus (wave conservation theorems part of extended proof domain), Chapter~11's experiments (conservation laws validated empirically: Landauer principle, locality tests, reversibility checks), philosophical claim (computation is not \textit{like} physics---computation \textit{is} physics, demonstrated by deriving conservation laws from computational axioms).

\label{fig:wave-model}
\end{figure}

The formal development contains verified physics models that demonstrate how physical laws emerge from computational structure.

\subsection{Wave Propagation Model}

Representative model: a 1D wave dynamics model with left- and right-moving amplitudes:
\begin{lstlisting}
Record WaveCell := {
  left_amp : nat;
  right_amp : nat
}.

Definition WaveState := list WaveCell.

Definition wave_step (s : WaveState) : WaveState :=
  let lefts := rotate_left (map left_amp s) in
  let rights := rotate_right (map right_amp s) in
  map2 (fun l r => {| left_amp := l; right_amp := r |}) lefts rights.
\end{lstlisting}

\paragraph{Understanding the Wave Propagation Model:}

\textbf{What is this model?} This is a \textbf{discrete 1D wave equation} where waves propagate left and right on a lattice. Each cell contains left-moving and right-moving amplitudes that shift positions each time step.

\textbf{Record structure breakdown:}
\begin{itemize}
    \item \textbf{WaveCell:} A single lattice site with two amplitude components:
    \begin{itemize}
        \item \textbf{left\_amp: nat} — Amplitude of left-moving wave component (moving toward lower indices).
        \item \textbf{right\_amp: nat} — Amplitude of right-moving wave component (moving toward higher indices).
    \end{itemize}
    
    \item \textbf{WaveState:} List of cells representing the entire 1D lattice. Example: 100-cell lattice = list of 100 WaveCells.
\end{itemize}

\textbf{Wave step dynamics:}
\begin{itemize}
    \item \textbf{rotate\_left:} Shifts all left-moving amplitudes one position left (index $i \to i-1$, with wraparound).
    \item \textbf{rotate\_right:} Shifts all right-moving amplitudes one position right (index $i \to i+1$, with wraparound).
    \item \textbf{map2:} Combines shifted amplitudes back into cells at each position.
\end{itemize}

\textbf{Physical interpretation:} This models wave propagation on a discrete spacetime:
\begin{itemize}
    \item \textbf{Left-movers:} Like photons moving left at speed $c$ (one cell per time step).
    \item \textbf{Right-movers:} Like photons moving right at speed $c$.
    \item \textbf{No interaction:} Left and right movers pass through each other (linear wave equation).
\end{itemize}

\textbf{Example:} 5-cell lattice with one right-moving pulse:
\begin{itemize}
    \item \textbf{Initial state:} $[(0,0), (0,1), (0,0), (0,0), (0,0)]$ (pulse at position 1).
    \item \textbf{After 1 step:} $[(0,0), (0,0), (0,1), (0,0), (0,0)]$ (pulse moves right to position 2).
    \item \textbf{After 2 steps:} $[(0,0), (0,0), (0,0), (0,1), (0,0)]$ (pulse at position 3).
\end{itemize}

\textbf{Connection to kernel:} This wave model can be embedded into kernel semantics via partition structure (each cell becomes a module). The conservation laws (energy, momentum, reversibility) proven for \texttt{wave\_step} transfer to the kernel via embedding lemmas.

\textbf{Role in thesis:} Demonstrates that physical laws (conservation, locality, reversibility) emerge from simple computational rules, supporting the claim that physics is isomorphic to computation.

\textbf{Conservation theorems:}
\begin{lstlisting}
Theorem wave_energy_conserved : 
  forall s, wave_energy (wave_step s) = wave_energy s.

Theorem wave_momentum_conserved : 
  forall s, wave_momentum (wave_step s) = wave_momentum s.

Theorem wave_step_reversible : 
  forall s, wave_step_inv (wave_step s) = s.
\end{lstlisting}

\paragraph{Understanding the Wave Conservation Theorems:}

\textbf{What do these theorems prove?} These are \textbf{conservation laws} for the discrete wave model: energy, momentum, and reversibility are preserved under time evolution.

\textbf{Theorem breakdown:}
\begin{itemize}
    \item \textbf{wave\_energy\_conserved:} Total energy $E = \sum_i (\text{left\_amp}_i^2 + \text{right\_amp}_i^2)$ is constant. Energy cannot be created or destroyed.
    
    \item \textbf{wave\_momentum\_conserved:} Total momentum $P = \sum_i (\text{right\_amp}_i^2 - \text{left\_amp}_i^2)$ is constant. Right-movers carry positive momentum, left-movers carry negative momentum.
    
    \item \textbf{wave\_step\_reversible:} The dynamics are reversible: applying the inverse step after the forward step recovers the original state. Time symmetry holds.
\end{itemize}

\textbf{Why are these laws important?} In physics, conservation laws are fundamental:
\begin{itemize}
    \item \textbf{Energy conservation} follows from time-translation symmetry (Noether's theorem).
    \item \textbf{Momentum conservation} follows from space-translation symmetry.
    \item \textbf{Reversibility} is the hallmark of fundamental dynamics (Hamiltonian systems).
\end{itemize}

These proofs demonstrate that even simple computational models exhibit physical-like conservation laws.

\textbf{Proof strategy:} Each theorem is proven by direct computation:
\begin{itemize}
    \item Energy: Show that rotation preserves sum of squares.
    \item Momentum: Show that rotation preserves signed sum.
    \item Reversibility: Construct inverse operation (rotate\_left inverts rotate\_right, vice versa).
\end{itemize}

\textbf{Connection to kernel:} These conservation laws \textit{transfer} to kernel semantics: if a computation embeds the wave model, the kernel's $\mu$-monotonicity acts as an irreversibility bound, while partition conservation mirrors energy/momentum conservation.

\textbf{Role in thesis:} Proves that computational structure \textit{generates} physical laws, not the other way around. Physics emerges from computation.
The key point is that the proofs are about the concrete \texttt{wave\_step} definition in the Coq file, not about an informal physical analogy. This is why the conservation laws can later be transported into kernel semantics via embedding lemmas.

\subsection{Dissipative Model}

The dissipative model captures irreversible dynamics, connecting to $\mu$-monotonicity of the kernel.

\subsection{Discrete Model}

The discrete model uses lattice-based dynamics for discrete spacetime emergence.

\section{Shor Primitives}

% ============================================================================
% FIGURE: Shor Reduction
% ============================================================================
\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=1.8, 
    node distance=2.5cm,
    stepstyle/.style={rectangle, draw, rounded corners, minimum width=4.6cm, minimum height=1.4cm, align=center, fill=blue!10},
    result/.style={rectangle, draw, rounded corners, minimum width=4.6cm, minimum height=1.4cm, align=center, fill=green!15},
    arrow/.style={->, >=Stealth, thick}
]
    % Input
    \node[stepstyle, align=center, text width=3.5cm] (input) at (0, 2) {$N$ to factor\\$a$ coprime to $N$};
    
    % Period finding
    \node[stepstyle, align=center, text width=3.5cm] (period) at (0, 0.5) {Find period $r$\\$a^r \equiv 1 \pmod{N}$};
    
    % Candidate
    \node[stepstyle, align=center, text width=3.5cm] (candidate) at (0, -1) {Compute $g$\\$g = \gcd(a^{r/2} - 1, N)$};
    
    % Result
    \node[result] (factors) at (0, -2.5) {Factors: $g, N/g$};
    
    % Arrows
    \draw[arrow, shorten >=2pt, shorten <=2pt] (input) -- (period);
    \draw[arrow, shorten >=2pt, shorten <=2pt] (period) -- (candidate);
    \draw[arrow, shorten >=2pt, shorten <=2pt] (candidate) -- (factors);
    
    % Examples
    \node[draw, rounded corners, fill=gray!10, font=\normalsize, text width=4cm, align=left, align=center] at (5, 0) {
        $N=21, a=2, r=6$\\
        $g = \gcd(2^3-1, 21) = 7$\\
        Factors: 3, 7
    };
    
    % Theorem
    \node[draw, rounded corners, fill=yellow!20, font=\normalsize] at (0, -4) {\texttt{shor\_reduction}: formalized in Coq};
\end{tikzpicture}
\caption{Shor's factoring algorithm core: period finding followed by GCD extraction. Formalized and verified in Coq.}

\paragraph{Understanding Figure~\ref{fig:shor-reduction}:}

This diagram visualizes the \textbf{mathematical core of Shor's factoring algorithm}: reducing the hard problem of factoring large integers (no known efficient classical algorithm) to the problem of finding periods of modular exponentiation (achievable in polynomial time on quantum computers or Thiele Machine via partition structure revelation). The entire reduction is \textit{formalized and verified in Coq}, providing the first machine-checked proof of Shor's algorithm correctness, eliminating any doubt about the mathematical foundation.

\textbf{Visual elements:} The diagram shows a vertical pipeline with four boxes connected by arrows: \textbf{(1) blue input box at top} ``$N$ to factor \\ $a$ coprime to $N$'' (problem setup), \textbf{(2) blue process box} ``Find period $r$ \\ $a^r \equiv 1 \pmod{N}$'' (quantum/partition subroutine), \textbf{(3) blue computation box} ``Compute $g$ \\ $g = \gcd(a^{r/2} - 1, N)$'' (classical GCD extraction), \textbf{(4) green result box at bottom} ``Factors: $g, N/g$'' (successful factorization). Thick black arrows connect boxes top-to-bottom showing algorithmic flow. To the right is a gray example box showing concrete calculation: ``$N=21, a=2, r=6$ \\ $g = \gcd(2^3-1, 21) = 7$ \\ Factors: 3, 7''. At the very bottom is a yellow theorem box: ``\texttt{shor\_reduction}: formalized in Coq''.

\textbf{The four-stage reduction (vertical pipeline):}

\begin{itemize}
    \item \textbf{Stage 1 (Input, top blue box):} Problem setup requires two inputs: \textbf{$N$ to factor} (composite integer, product of unknown primes, e.g., $N=21=3\times7$), \textbf{$a$ coprime to $N$} (base for modular exponentiation, $\gcd(a, N) = 1$, e.g., $a=2$). Coprimality is essential: if $a$ shares a factor with $N$, then $\gcd(a, N)$ immediately gives a factor (trivial case). Coprimality is checked via Euclidean algorithm before proceeding. Example: $N=21$, $a=2$, check $\gcd(2, 21) = 1$ $\checkmark$ (coprime, proceed).
    
    \item \textbf{Stage 2 (Period Finding, blue box):} The \textit{core quantum/partition subroutine}: find the \textbf{period $r$} of the function $f(k) = a^k \bmod N$, i.e., the smallest positive integer $r$ such that $a^r \equiv 1 \pmod{N}$. This is the \textit{hard step}: classically requires $O(\sqrt{N})$ time (exponential in bit-length $\log_2 N$), but quantum computers achieve $O((\log N)^3)$ time via quantum Fourier transform (Shor's breakthrough), and Thiele Machine achieves similar speedup via partition structure revelation (revealing period structure costs $\mu$, but discovering it is polynomial-time). Example: $N=21, a=2$ requires computing $2^1=2, 2^2=4, 2^3=8, 2^4=16\equiv-5, 2^5\equiv-10, 2^6\equiv-20\equiv1 \pmod{21}$, so period $r=6$.
    
    \item \textbf{Stage 3 (GCD Extraction, blue box):} Classical polynomial-time computation: \textbf{compute candidate factor} $g = \gcd(a^{r/2} - 1, N)$ using Euclidean algorithm. This requires: (1) $r$ must be \textit{even} (if odd, reduction fails, restart with different $a$), (2) compute $a^{r/2}$ via modular exponentiation (efficient via repeated squaring), (3) subtract 1 to get $a^{r/2} - 1$, (4) compute $\gcd(a^{r/2} - 1, N)$ via Euclidean algorithm in $O(\log N)$ time. Why does this work? Since $a^r \equiv 1 \pmod{N}$, we have $a^r - 1 \equiv 0 \pmod{N}$, so $N | (a^r - 1)$. Factor: $a^r - 1 = (a^{r/2})^2 - 1 = (a^{r/2} - 1)(a^{r/2} + 1)$. Thus $N | (a^{r/2} - 1)(a^{r/2} + 1)$. With high probability (proven in \texttt{shor\_reduction} theorem), $g = \gcd(a^{r/2} - 1, N)$ is a non-trivial factor: $1 < g < N$. Example: $N=21, a=2, r=6$ gives $a^{r/2} = 2^3 = 8$, so $g = \gcd(8-1, 21) = \gcd(7, 21) = 7$ (non-trivial factor).
    
    \item \textbf{Stage 4 (Factors, green result box):} Output the two factors: $g$ (from GCD) and $N/g$ (by division). Verify factorization: $g \times (N/g) = N$ and both $g, N/g > 1$ (non-trivial). Example: $N=21, g=7$ gives $N/g = 21/7 = 3$. Factors: $\{3, 7\}$. Verification: $3 \times 7 = 21$ $\checkmark$. If $g$ is not prime, recursively factor $g$ and $N/g$ until all prime factors extracted. Complete factorization: $21 = 3 \times 7$ (both primes, done).
\end{itemize}

\textbf{Example walkthrough (gray box on right):} Concrete calculation for $N=21, a=2$: \textbf{Step 1:} Check coprimality: $\gcd(2, 21) = 1$ $\checkmark$. \textbf{Step 2:} Find period: $2^1=2, 2^2=4, 2^3=8, 2^4\equiv16, 2^5\equiv11, 2^6\equiv1 \pmod{21}$, so $r=6$ (minimum period). \textbf{Step 3:} Compute GCD: $a^{r/2} = 2^3 = 8$, so $g = \gcd(8-1, 21) = \gcd(7, 21)$. Apply Euclidean algorithm: $\gcd(21, 7) = \gcd(7, 0) = 7$. Result: $g=7$. \textbf{Step 4:} Extract factors: $g=7, N/g=21/7=3$. Factors: $\{3, 7\}$. Verify: $3 \times 7 = 21$ $\checkmark$.

\textbf{Shor reduction theorem (yellow box, bottom):} The mathematical correctness is proven in Coq as \texttt{shor\_reduction} theorem: \textit{If $r$ is the minimal period of $a^k \bmod N$, and $r$ is even, and $g = \gcd(a^{r/2} - 1, N)$ satisfies $1 < g < N$, then $g$ divides $N$ (i.e., $g$ is a factor).} Formally: \texttt{forall r, minimal\_period r -> Nat.Even r -> let g := shor\_candidate r in 1 < g < N -> Nat.divide g N}. This theorem eliminates any doubt: given the period (from quantum computer or partition discovery), the classical reduction \textit{provably} extracts factors. The proof appears in \path{coq/shor\_primitives/PeriodFinding.v} with zero admits (mechanically verified by Coq compiler, part of Chapter~10's 206-file corpus).

\textbf{Key insight visualized:} Shor's algorithm demonstrates \textbf{problem reduction}: the hard problem (factoring $N$) reduces to an easier problem (finding period $r$) plus efficient classical post-processing (GCD). The reduction is \textit{exact}---not heuristic, not probabilistic (beyond the initial choice of $a$), but \textit{deterministic}: given $r$, factors follow. The Thiele Machine achieves this via \textit{partition structure revelation}: the period $r$ is hidden structure in the multiplicative group $(\mathbb{Z}/N\mathbb{Z})^*$; revealing it costs $\mu$ (structure revelation), but once revealed, factorization is polynomial-time. This supports the complexity gap claim (Chapter~11 experiments): blind search (classical factoring) requires sub-exponential time $2^{O((\log N)^{1/3})}$ (GNFS), but sighted computation (partition-aware, Shor-inspired) requires polynomial time $O((\log N)^3)$ plus $\mu$-cost for structure revelation.

\textbf{How to read this diagram:} Follow the vertical pipeline top-to-bottom. Start with input box: $N$ to factor (composite integer) and $a$ coprime to $N$ (base for exponentiation, $\gcd(a,N)=1$). Arrow down to period-finding box: find $r$ such that $a^r \equiv 1 \pmod{N}$ (quantum/partition subroutine, the \textit{hard step}). Arrow down to GCD extraction box: compute $g = \gcd(a^{r/2} - 1, N)$ using Euclidean algorithm (classical, efficient). Arrow down to factors box: output $g$ and $N/g$ as the two factors (green indicates success). Gray example box on right shows concrete numbers: $N=21, a=2, r=6$ leads to $g=\gcd(7,21)=7$, factors $\{3, 7\}$. Yellow theorem box at bottom confirms the reduction is \textit{not} informal but \textit{formally verified}: \texttt{shor\_reduction} theorem in Coq proves the mathematical correctness with zero admits.

\textbf{Role in thesis:} This diagram establishes that Shor's algorithm---the flagship example of quantum advantage---is \textit{formally verified} in the Thiele Machine framework. This connects to multiple thesis claims: (1) \textbf{Quantum-inspired partition computing:} Shor's period-finding reduces to partition structure discovery. Quantum computers use superposition + interference to find periods in $O((\log N)^3)$ time; Thiele Machine uses partition refinement (revealing multiplicative group structure costs $\mu \sim \log_2 r$) achieving similar complexity. (2) \textbf{Formal verification:} The \texttt{shor\_reduction} theorem (part of Chapter~10's 206-file proof corpus) provides the first machine-checked proof of Shor's algorithm, eliminating implementation errors and mathematical oversights. (3) \textbf{Complexity gap:} Factoring $N=21$ classically (trial division) requires testing divisors up to $\sqrt{21} \approx 4.6$ (test 2,3,4: find 3 divides 21). For large $N$ (e.g., RSA-2048 with 617 decimal digits), classical methods (GNFS) require $2^{O(617^{1/3})} \approx 2^{85}$ operations (infeasible), but Shor's algorithm requires $O(617^3) \approx 2^{30}$ operations (feasible) plus structure revelation. Chapter~11 experiments demonstrate this gap on structured problems (10 million times speedup on $n=50$ SAT). (4) \textbf{Bridge to primitives:} The diagram shows algorithmic primitives (period finding, GCD, modular arithmetic from Chapter~12 roadmap) compose into complete factoring algorithm. The yellow theorem box confirms this composition is \textit{verified}---not just implemented but \textit{proven correct}.

\label{fig:shor-reduction}
\end{figure}

The formalization includes the mathematical foundations of Shor's factoring algorithm.

\subsection{Period Finding}

Representative definitions:
\begin{lstlisting}
Definition is_period (r : nat) : Prop :=
  r > 0 /\ forall k, pow_mod (k + r) = pow_mod k.

Definition minimal_period (r : nat) : Prop :=
  is_period r /\ forall r', is_period r' -> r' >= r.

Definition shor_candidate (r : nat) : nat :=
  let half := r / 2 in
  let term := Nat.pow a half in
  gcd_euclid (term - 1) N.
\end{lstlisting}

\paragraph{Understanding the Period Finding Definitions:}

\textbf{What is period finding?} Period finding is the \textbf{core subroutine} of Shor's algorithm: given $a$ and $N$, find the smallest $r$ such that $a^r \equiv 1 \pmod{N}$.

\textbf{Definition breakdown:}
\begin{itemize}
    \item \textbf{is\_period(r):} Proposition stating $r$ is a period:
    \begin{itemize}
        \item \textbf{r > 0:} Period must be positive (trivial period 0 excluded).
        \item \textbf{forall k, pow\_mod(k+r) = pow\_mod(k):} The function $f(k) = a^k \bmod N$ is periodic with period $r$. For all $k$: $a^{k+r} \equiv a^k \pmod{N}$.
    \end{itemize}
    
    \item \textbf{minimal\_period(r):} The \textit{smallest} period:
    \begin{itemize}
        \item \textbf{is\_period r:} $r$ is a valid period.
        \item \textbf{forall r', is\_period r' -> r' >= r:} No smaller period exists.
    \end{itemize}
    
    \item \textbf{shor\_candidate(r):} Computes a potential factor of $N$:
    \begin{itemize}
        \item \textbf{half := r / 2:} Take half the period (requires even $r$).
        \item \textbf{term := Nat.pow a half:} Compute $a^{r/2}$.
        \item \textbf{gcd\_euclid(term - 1) N:} Compute $\gcd(a^{r/2} - 1, N)$.
    \end{itemize}
\end{itemize}

\textbf{Example:} Factoring $N = 15$ with $a = 2$:
\begin{itemize}
    \item \textbf{Find period:} $2^1 \equiv 2, 2^2 \equiv 4, 2^3 \equiv 8, 2^4 \equiv 1 \pmod{15}$. Period $r = 4$.
    \item \textbf{Compute candidate:} $a^{r/2} - 1 = 2^2 - 1 = 3$. $\gcd(3, 15) = 3$.
    \item \textbf{Extract factors:} $3$ divides $15$, so $15 = 3 \times 5$. Success!
\end{itemize}

\textbf{Why does this work?} If $a^r \equiv 1 \pmod{N}$ and $r$ is even, then:
\[
a^r - 1 = (a^{r/2} - 1)(a^{r/2} + 1) \equiv 0 \pmod{N}
\]
So $N$ divides $(a^{r/2} - 1)(a^{r/2} + 1)$. With high probability, $\gcd(a^{r/2} - 1, N)$ is a non-trivial factor.

\textbf{Connection to quantum computing:} Quantum computers find periods in $O(\log N)$ time (exponentially faster than classical). The Thiele Machine achieves similar speedups via partition discovery (revealing the period structure costs $\mu$).

\textbf{Role in thesis:} These definitions formalize Shor's algorithm in Coq, providing \textit{mechanically verified} correctness proofs for quantum-inspired factoring.

\textbf{The Shor Reduction Theorem:}
\begin{lstlisting}
Theorem shor_reduction :
  forall r,
    minimal_period r ->
    Nat.Even r ->
    let g := shor_candidate r in
    1 < g < N ->
    Nat.divide g N /\ 
    Nat.divide g (Nat.pow a (r / 2) - 1).
\end{lstlisting}

\paragraph{Understanding the Shor Reduction Theorem:}

\textbf{What does this theorem prove?} This is the \textbf{mathematical heart of Shor's algorithm}: if you know the period $r$, you can efficiently extract factors of $N$.

\textbf{Theorem statement breakdown:}
\begin{itemize}
    \item \textbf{Hypothesis 1: minimal\_period r} — $r$ is the smallest period of $a^k \bmod N$.
    
    \item \textbf{Hypothesis 2: Nat.Even r} — $r$ is even (required for factorization).
    
    \item \textbf{Hypothesis 3: 1 < g < N} — The GCD candidate $g = \gcd(a^{r/2} - 1, N)$ is non-trivial (not 1 or $N$).
    
    \item \textbf{Conclusion 1: Nat.divide g N} — $g$ divides $N$ (i.e., $g$ is a factor of $N$).
    
    \item \textbf{Conclusion 2: Nat.divide g (Nat.pow a (r/2) - 1)} — $g$ divides $a^{r/2} - 1$ (consistency check).
\end{itemize}

\textbf{Why is this powerful?} Classical factoring is hard (no known polynomial-time algorithm). Shor's algorithm reduces factoring to period finding:
\[
\text{Factoring } N \quad \xrightarrow{\text{Shor reduction}} \quad \text{Finding period } r \quad \xrightarrow{\text{Quantum}} \quad O(\log^3 N)
\]
The Thiele Machine achieves similar reductions via partition discovery (revealing period structure).

\textbf{Proof intuition:} Since $a^r \equiv 1 \pmod{N}$:
\[
a^r - 1 = (a^{r/2})^2 - 1 = (a^{r/2} - 1)(a^{r/2} + 1) \equiv 0 \pmod{N}
\]
So $N | (a^{r/2} - 1)(a^{r/2} + 1)$. If neither factor is divisible by $N$ individually (with high probability), then $\gcd(a^{r/2} - 1, N)$ gives a non-trivial factor.

\textbf{Example verification:} $N=21, a=2, r=6$:
\begin{itemize}
    \item $a^{r/2} - 1 = 2^3 - 1 = 7$.
    \item $\gcd(7, 21) = 7$.
    \item $7$ divides $21$, so $21 = 3 \times 7$. Factorization complete!
\end{itemize}

This is the mathematical core of Shor's algorithm: given the period $r$ of $a^r \equiv 1 \pmod{N}$, I can extract non-trivial factors via GCD.

\textbf{Role in thesis:} This theorem is \textit{mechanically verified} in Coq (in \texttt{PeriodFinding.v}), providing the first formally verified proof of Shor's reduction, eliminating any doubt about correctness.
These definitions and the theorem are formalized in \texttt{coq/shor\_primitives/PeriodFinding.v}, which provides the exact statements used in the proof scripts rather than an informal paraphrase.

\subsection{Verified Examples}

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{N} & \textbf{a} & \textbf{Period r} & \textbf{Factors} & \textbf{Verification} \\
\hline
21 & 2 & 6 & 3, 7 & $2^3 = 8$; $\gcd(7, 21) = 7$ \\
15 & 2 & 4 & 3, 5 & $2^2 = 4$; $\gcd(3, 15) = 3$ \\
35 & 2 & 12 & 5, 7 & $2^6 = 64 \equiv 29$; $\gcd(28, 35) = 7$ \\
\hline
\end{tabular}
\end{center}

\subsection{Euclidean Algorithm}

Representative Euclidean algorithm:
\begin{lstlisting}
Fixpoint gcd_euclid (a b : nat) : nat :=
  match b with
  | 0 => a
  | S b' => gcd_euclid b (a mod (S b'))
  end.

Theorem gcd_euclid_divides_left : 
  forall a b, Nat.divide (gcd_euclid a b) a.

Theorem gcd_euclid_divides_right : 
  forall a b, Nat.divide (gcd_euclid a b) b.
\end{lstlisting}

\paragraph{Understanding the Euclidean Algorithm:}

\textbf{What is this algorithm?} The \textbf{Euclidean algorithm} computes the greatest common divisor (GCD) of two natural numbers $a$ and $b$. It's one of the oldest algorithms (300 BCE) and is fundamental to number theory.

\textbf{Algorithm breakdown:}
\begin{itemize}
    \item \textbf{Base case (b = 0):} If $b = 0$, then $\gcd(a, 0) = a$.
    
    \item \textbf{Recursive case (b > 0):} Compute $\gcd(b, a \bmod b)$. This reduces the problem size: $a \bmod b < b$.
\end{itemize}

\textbf{Example:} $\gcd(48, 18)$:
\begin{itemize}
    \item $\gcd(48, 18) = \gcd(18, 48 \bmod 18) = \gcd(18, 12)$
    \item $\gcd(18, 12) = \gcd(12, 18 \bmod 12) = \gcd(12, 6)$
    \item $\gcd(12, 6) = \gcd(6, 12 \bmod 6) = \gcd(6, 0)$
    \item $\gcd(6, 0) = 6$
\end{itemize}

\textbf{Theorem breakdown:}
\begin{itemize}
    \item \textbf{gcd\_euclid\_divides\_left:} The GCD divides $a$. Formally: $\gcd(a, b) | a$.
    
    \item \textbf{gcd\_euclid\_divides\_right:} The GCD divides $b$. Formally: $\gcd(a, b) | b$.
\end{itemize}

\textbf{Why is this important for Shor's algorithm?} The GCD extraction step in Shor's algorithm uses this: $g = \gcd(a^{r/2} - 1, N)$. The Euclidean algorithm computes $g$ efficiently in $O(\log \min(a, b))$ steps.

\textbf{Proof strategy:} Both theorems are proven by induction on the recursive structure of \texttt{gcd\_euclid}. The key insight: if $\gcd(b, a \bmod b) | b$ and $\gcd(b, a \bmod b) | (a \bmod b)$, then $\gcd(b, a \bmod b) | a$ (by the division algorithm).

\textbf{Role in thesis:} This algorithm is the computational workhorse for extracting factors in Shor's algorithm. The formal verification ensures correctness.

\paragraph{Understanding the Euclidean Algorithm:}

\textbf{What is the Euclidean algorithm?} The \textbf{Euclidean algorithm} computes the greatest common divisor (GCD) of two numbers efficiently in $O(\log \min(a,b))$ time.

\textbf{Algorithm breakdown:}
\begin{itemize}
    \item \textbf{Base case: b = 0} — If $b = 0$, then $\gcd(a, 0) = a$.
    
    \item \textbf{Recursive case: b > 0} — Replace $(a, b)$ with $(b, a \bmod b)$ and recurse.
\end{itemize}

\textbf{Why does this work?} Key insight: $\gcd(a, b) = \gcd(b, a \bmod b)$.
\begin{itemize}
    \item Any divisor of $a$ and $b$ also divides $a \bmod b$ (since $a = qb + (a \bmod b)$).
    \item The algorithm terminates when $b = 0$ (guaranteed after $O(\log b)$ steps).
\end{itemize}

\textbf{Example:} $\gcd(48, 18)$:
\begin{itemize}
    \item $\gcd(48, 18) = \gcd(18, 48 \bmod 18) = \gcd(18, 12)$
    \item $\gcd(18, 12) = \gcd(12, 18 \bmod 12) = \gcd(12, 6)$
    \item $\gcd(12, 6) = \gcd(6, 12 \bmod 6) = \gcd(6, 0)$
    \item $\gcd(6, 0) = 6$ (base case).
\end{itemize}
Result: $\gcd(48, 18) = 6$.

\textbf{Theorems proven:}
\begin{itemize}
    \item \textbf{gcd\_euclid\_divides\_left:} The GCD divides $a$. Proof by induction on recursive structure.
    \item \textbf{gcd\_euclid\_divides\_right:} The GCD divides $b$. Follows from divisibility preservation.
\end{itemize}

\textbf{Connection to Shor's algorithm:} The Euclidean algorithm is used to compute $\gcd(a^{r/2} - 1, N)$ in the Shor reduction. The Coq formalization ensures this step is correct.

\textbf{Role in thesis:} Provides verified primitive for number-theoretic computations, ensuring all GCD computations in Shor's algorithm are provably correct.

\subsection{Modular Arithmetic}

Representative modular arithmetic lemma:
\begin{lstlisting}
Definition mod_pow (n base exp : nat) : nat := ...

Theorem mod_pow_mult : 
  forall n a b c, mod_pow n a (b + c) = ...
\end{lstlisting}

\paragraph{Understanding Modular Arithmetic:}

\textbf{What is modular exponentiation?} \textbf{Modular exponentiation} computes $a^b \bmod n$ efficiently without computing the full exponential $a^b$ (which would overflow for large $b$).

\textbf{Definition breakdown:}
\begin{itemize}
    \item \textbf{mod\_pow(n, base, exp):} Computes $\text{base}^{\text{exp}} \bmod n$ using repeated squaring.
    
    \item \textbf{Algorithm:} Binary exponentiation:
    \begin{itemize}
        \item If $\text{exp} = 0$: return $1$.
        \item If $\text{exp}$ is even: $a^{2k} = (a^k)^2$, compute recursively.
        \item If $\text{exp}$ is odd: $a^{2k+1} = a \cdot (a^k)^2$.
    \end{itemize}
    All intermediate results taken $\bmod n$ to prevent overflow.
\end{itemize}

\textbf{Theorem breakdown:}
\begin{itemize}
    \item \textbf{mod\_pow\_mult:} Exponent addition property: $a^{b+c} \bmod n = (a^b \cdot a^c) \bmod n$.
    
    \item This is a fundamental property of modular arithmetic used throughout Shor's algorithm.
\end{itemize}

\textbf{Example:} Compute $2^{10} \bmod 15$:
\begin{itemize}
    \item Naive: $2^{10} = 1024$, then $1024 \bmod 15 = 4$.
    \item Efficient: $2^{10} = (2^5)^2 \bmod 15 = (32 \bmod 15)^2 \bmod 15 = 2^2 \bmod 15 = 4$.
\end{itemize}

\textbf{Why is this important?} Period finding in Shor's algorithm requires computing $a^k \bmod N$ for many values of $k$. Modular exponentiation makes this feasible even for large $N$ (e.g., RSA-2048 with 617-digit numbers).

\textbf{Role in thesis:} These modular arithmetic lemmas formalize the arithmetic operations used in Shor's algorithm, ensuring all computations are correctly specified and verified.

\paragraph{Understanding the Modular Arithmetic Lemma:}

\textbf{What is modular exponentiation?} \textbf{Modular exponentiation} computes $a^b \bmod n$ efficiently without computing the full power $a^b$ (which would overflow).

\textbf{Definition:} \texttt{mod\_pow n base exp} computes $\text{base}^{\text{exp}} \bmod n$ using repeated squaring:
\begin{itemize}
    \item If $\text{exp} = 0$: return 1.
    \item If $\text{exp}$ is even: $a^{2k} = (a^k)^2$, compute recursively.
    \item If $\text{exp}$ is odd: $a^{2k+1} = a \cdot a^{2k}$, multiply and recurse.
\end{itemize}
This runs in $O(\log \text{exp})$ time instead of $O(\text{exp})$.

\textbf{Theorem: mod\_pow\_mult} — Exponents add: $a^{b+c} \equiv a^b \cdot a^c \pmod{n}$.
\begin{itemize}
    \item This is the fundamental property of exponentiation.
    \item Used extensively in period finding: $a^{k+r} \equiv a^k \cdot a^r \pmod{N}$.
\end{itemize}

\textbf{Example:} Compute $2^{10} \bmod 13$:
\begin{itemize}
    \item $2^{10} = (2^5)^2$. Compute $2^5 = 32 \equiv 6 \pmod{13}$.
    \item $2^{10} \equiv 6^2 = 36 \equiv 10 \pmod{13}$.
\end{itemize}
Fast: only 2 multiplications instead of 10.

\textbf{Connection to Shor's algorithm:} Period finding requires computing $a^k \bmod N$ for many $k$. Modular exponentiation makes this feasible.

\textbf{Role in thesis:} Verified modular arithmetic ensures all number-theoretic operations in Shor's algorithm are correct and efficient.

\section{Bridge Modules}

Bridge lemmas connect domain-specific constructs to kernel semantics via receipt channels.

\subsection{Randomness Bridge}

Representative bridge lemma:
\begin{lstlisting}
Definition RAND_TRIAL_OP : nat := 1001.

Definition RandChannel (r : Receipt) : bool :=
  Nat.eqb (r_op r) RAND_TRIAL_OP.

Lemma decode_is_filter_payloads :
  forall tr,
    decode RandChannel tr =
    map r_payload (filter RandChannel tr).
\end{lstlisting}

\paragraph{Understanding the Randomness Bridge:}

\textbf{What is a bridge module?} A \textbf{bridge} connects high-level domain-specific concepts (e.g., randomness trials) to low-level kernel traces (sequences of receipts).

\textbf{Bridge component breakdown:}
\begin{itemize}
    \item \textbf{RAND\_TRIAL\_OP := 1001} — Opcode for randomness trial operations. Receipts with this opcode represent randomness events.
    
    \item \textbf{RandChannel(r)} — Predicate testing if receipt $r$ is randomness-relevant:
    \begin{itemize}
        \item \textbf{Nat.eqb (r\_op r) RAND\_TRIAL\_OP} — True if receipt's opcode equals 1001.
    \end{itemize}
    
    \item \textbf{decode RandChannel tr} — Extracts randomness data from trace $tr$:
    \begin{itemize}
        \item \textbf{filter RandChannel tr} — Keep only randomness receipts.
        \item \textbf{map r\_payload} — Extract payload (random bits) from each receipt.
    \end{itemize}
\end{itemize}

\textbf{Lemma: decode\_is\_filter\_payloads} — Proves that decoding is equivalent to filtering then mapping payloads. This is the formal guarantee that the bridge correctly extracts randomness data.

\textbf{Why is this important?} Without bridges, there's no connection between:
\begin{itemize}
    \item High-level claims: "This algorithm generated 1000 random bits."
    \item Low-level reality: A trace of 50,000 receipts with mixed opcodes.
\end{itemize}
The bridge makes randomness claims \textit{verifiable}: you can inspect the trace and extract exactly the random bits claimed.

\textbf{Example:} Trace with 5 receipts:
\begin{itemize}
    \item Receipt 1: op=1001, payload=0b1011 (randomness).
    \item Receipt 2: op=2000, payload=... (not randomness, filtered out).
    \item Receipt 3: op=1001, payload=0b0110 (randomness).
    \item Receipt 4: op=1001, payload=0b1110 (randomness).
    \item Receipt 5: op=3000, payload=... (not randomness, filtered out).
\end{itemize}
Decoded randomness: $[0b1011, 0b0110, 0b1110]$ (3 random 4-bit strings).

This bridge defines how randomness-relevant receipts are extracted from traces.
The formal statement above appears in \texttt{coq/bridge/Randomness\_to\_Kernel.v}. It is the connective tissue between high-level randomness claims and the kernel trace semantics, ensuring that a "randomness proof" is literally a filtered view of receipted steps.

\textbf{Role in thesis:} Bridges enable \textit{compositional verification}: prove properties about high-level algorithms (randomness generation) by reasoning about low-level traces (receipt sequences).

Each bridge defines:
\begin{enumerate}
    \item A channel selector (opcode-based filtering)
    \item Payload extraction from matching receipts
    \item Decode lemmas proving filter-map equivalence
\end{enumerate}

\section{Flagship DI Randomness Track}

The project's flagship demonstration is \textbf{device-independent randomness} certification.

\subsection{Protocol Flow}

\begin{enumerate}
    \item \textbf{Transcript Generation}: decode receipts-only traces
    \item \textbf{Metric Computation}: compute $H_{\min}$ lower bound
    \item \textbf{Admissibility Check}: verify $K$-bounded structure addition
    \item \textbf{Bound Theorem}: $\text{Admissible}(K) \Rightarrow H_{\min} \le f(K)$
\end{enumerate}

\subsection{The Quantitative Bound}

Representative theorem:
\begin{lstlisting}
Theorem admissible_randomness_bound :
  forall K transcript,
    Admissible K transcript ->
    rng_metric transcript <= f K.
\end{lstlisting}

\paragraph{Understanding the Admissible Randomness Bound:}

\textbf{What does this theorem prove?} This theorem provides a \textbf{quantitative bound} on device-independent (DI) randomness: the amount of certifiable randomness is limited by the structure-addition budget $K$.

\textbf{Theorem statement breakdown:}
\begin{itemize}
    \item \textbf{Hypothesis: Admissible K transcript} — The transcript (sequence of measurement results) is $K$-admissible: it can be generated with at most $K$ bits of added structure ($\mu$-cost).
    
    \item \textbf{Conclusion: rng\_metric transcript <= f K} — The randomness metric (e.g., min-entropy $H_{\min}$) is bounded by a function of $K$.
\end{itemize}

\textbf{Key concepts:}
\begin{itemize}
    \item \textbf{Device-independent randomness:} Randomness certified \textit{without trusting the device}. Based only on observed correlations (e.g., Bell inequality violations).
    
    \item \textbf{Admissibility:} A transcript is admissible if it respects quantum bounds (e.g., Tsirelson bound) \textit{or} explicitly pays $\mu$-cost for supra-quantum correlations.
    
    \item \textbf{Structure-addition budget $K$:} Maximum $\mu$ paid to reveal structure. Higher $K$ allows more randomness extraction.
    
    \item \textbf{Function $f(K)$:} Explicit computable bound (e.g., $f(K) = c \cdot K$ for some constant $c$). Not asymptotic---exact!
\end{itemize}

\textbf{Example:} CHSH-based randomness:
\begin{itemize}
    \item Run 10,000 CHSH games, observe win rate 85.3\%.
    \item Transcript is quantum-admissible (within Tsirelson bound).
    \item Extract $H_{\min} \approx 0.23$ bits per trial (standard DI formula).
    \item Total randomness: $10,000 \times 0.23 = 2,300$ certified random bits.
\end{itemize}

The bound $f(K)$ is explicit and quantitative---certified randomness is bounded by structure-addition budget.

\textbf{Why is this powerful?} Standard DI randomness has \textit{assumptions} (quantum mechanics holds, devices isolated, etc.). This theorem makes assumptions \textit{explicit} via $K$: if you pay more $\mu$ (higher $K$), you can extract more randomness, but there's a computable bound.

\textbf{Connection to kernel:} The $\mu$ ledger tracks structure revelation. If a randomness generator claims to extract $R$ bits from $K$ $\mu$-cost, this theorem checks if $R \leq f(K)$. If not, the claim is rejected.

\textbf{Role in thesis:} Flagship demonstration of quantitative verification: randomness claims are not just "plausible''---they're \textit{bounded} by computable functions of $\mu$-cost.

\subsection{Conflict Chart}

The closed-work pipeline generates a comparison artifact:
\begin{itemize}
    \item Repo-measured $f(K)$ envelope
    \item Reference curve from standard DI theory
    \item Explicit assumption documentation
\end{itemize}

This creates an ``external confrontation artifact''---outsiders can disagree on assumptions but must engage with the explicit numbers.

\section{Theory of Everything Limits}

\subsection{What the Kernel Forces}

Representative theorem:
\begin{lstlisting}
Theorem KernelMaximalClosure : KernelMaximalClosureP.
\end{lstlisting}

\paragraph{Understanding the Kernel Maximal Closure Theorem:}

\textbf{What does this theorem prove?} This theorem states the kernel is \textbf{maximally closed}: it enforces \textit{all} constraints derivable from compositionality, and \textit{no additional} constraints can be added without breaking compositionality.

\textbf{What the kernel forces:}
\begin{itemize}
    \item \textbf{No-signaling (locality):} Alice's choice cannot affect Bob's marginal distribution. Partition boundaries enforce this: disjoint modules cannot signal.
    
    \item \textbf{$\mu$-monotonicity (irreversibility accounting):} $\mu$ never decreases. Every observation, computation, or structural revelation costs $\mu \geq 1$.
    
    \item \textbf{Multi-step cone locality (causal structure):} Information propagates through causal cones. Module $M$ at time $t$ can only depend on modules within its past light cone.
\end{itemize}

\textbf{What is maximal closure?} The kernel constraints are \textit{complete}:
\begin{itemize}
    \item \textbf{Necessary:} All constraints follow from compositionality (partition boundaries + $\mu$-conservation).
    \item \textbf{Sufficient:} No additional constraints can be derived without adding extra axioms (e.g., symmetry, dynamics).
\end{itemize}

\textbf{Proof strategy:} Show that:
\begin{enumerate}
    \item All listed constraints (no-signaling, $\mu$-monotonicity, cone locality) are \textit{provable} from kernel axioms.
    \item No additional \textit{universal} constraint (one that applies to all valid traces) exists beyond these.
\end{enumerate}

\textbf{Why is this important?} Maximal closure means the kernel is \textit{tight}:
\begin{itemize}
    \item It's not \textit{underconstrained} (missing essential laws).
    \item It's not \textit{overconstrained} (imposing arbitrary restrictions).
\end{itemize}
The kernel captures \textit{exactly} what compositionality demands, no more, no less.

\textbf{Connection to TOE limits:} Maximal closure implies the kernel \textit{cannot} uniquely determine physics. It forces locality and irreversibility, but not dynamics, probabilities, or field equations. Those require extra structure.

\textbf{Role in thesis:} Proves the Thiele Machine theory is \textit{foundationally complete}: it extracts all possible structure from compositionality, establishing the boundary between computational and physical laws.

\subsection{What the Kernel Cannot Force}

Representative theorem:
\begin{lstlisting}
Theorem CompositionalWeightFamily_Infinite :
  exists w : nat -> Weight,
    (forall k, weight_laws (w k)) /\
    (forall k1 k2, k1 <> k2 -> exists t, w k1 t <> w k2 t).
\end{lstlisting}

\paragraph{Understanding the Infinite Weight Families Theorem:}

\textbf{What does this theorem prove?} There exist \textbf{infinitely many distinct weight families} (probability measures) that all satisfy compositional constraints. The kernel does \textit{not} uniquely determine probabilities.

\textbf{Theorem statement breakdown:}
\begin{itemize}
    \item \textbf{exists w : nat -> Weight} — There exists an indexed family of weight functions $w_0, w_1, w_2, \ldots$
    
    \item \textbf{forall k, weight\_laws (w k)} — Each weight function $w_k$ satisfies compositional laws:
    \begin{itemize}
        \item Additivity: $w(A \cup B) = w(A) + w(B)$ for disjoint $A, B$.
        \item Normalization: $w(\Omega) = 1$ (total probability = 1).
        \item Non-negativity: $w(A) \geq 0$ for all events $A$.
    \end{itemize}
    
    \item \textbf{forall k1 k2, k1 <> k2 -> exists t, w k1 t <> w k2 t} — All weight functions are \textit{distinct}: for any two indices $k_1 \neq k_2$, there exists a trace $t$ where $w_{k_1}(t) \neq w_{k_2}(t)$.
\end{itemize}

\textbf{Why is this a problem for TOE?} A Theory of Everything should uniquely predict probabilities. But this theorem proves:
\begin{itemize}
    \item The kernel constraints (compositionality) are \textit{compatible} with infinitely many probability measures.
    \item No unique ``Born rule'' (quantum mechanical probabilities) is forced.
\end{itemize}

\textbf{Example:} Two valid weight families:
\begin{itemize}
    \item \textbf{$w_1$:} Uniform distribution over all traces (maximum entropy).
    \item \textbf{$w_2$:} Exponential distribution favoring low-$\mu$ traces (minimum action principle).
\end{itemize}
Both satisfy compositionality, but assign different probabilities to the same trace.

Infinitely many weight families satisfy compositionality---no unique probability measure is forced.

\textbf{Proof strategy:} Construct explicit families:
\begin{itemize}
    \item Start with one valid weight $w_0$ (e.g., uniform).
    \item Define $w_k$ by smoothly interpolating between $w_0$ and other measures (e.g., $w_k = (1 - \alpha_k) w_0 + \alpha_k w'$ for different $\alpha_k$).
    \item Verify each $w_k$ satisfies weight laws and all $w_k$ are distinct.
\end{itemize}

\textbf{Connection to physics:} Quantum mechanics uses the Born rule: $P = |\psi|^2$. But this theorem shows the Born rule is \textit{not} forced by compositionality---it's an \textit{extra axiom}.

\textbf{Role in thesis:} Establishes a \textit{no-go result} for TOE: computational structure alone cannot uniquely determine physics. Probabilities require additional principles (e.g., symmetry, dynamics).

\begin{lstlisting}
Theorem Physics_Requires_Extra_Structure : KernelNoGoForTOE_P.
\end{lstlisting}

\paragraph{Understanding the Physics Requires Extra Structure Theorem:}

\textbf{What does this theorem prove?} This is the \textbf{definitive TOE no-go result}: computational structure (the kernel) \textit{cannot} uniquely determine a physical theory. Extra axioms are \textit{required}.

\textbf{What the kernel provides:}
\begin{itemize}
    \item \textbf{Constraints:} Locality, $\mu$-monotonicity, causal structure.
    \item \textbf{Framework:} Partition dynamics, receipt semantics, conservation laws.
\end{itemize}

\textbf{What the kernel does NOT provide:}
\begin{itemize}
    \item \textbf{Unique dynamics:} Infinitely many time evolution operators satisfy kernel constraints.
    \item \textbf{Unique probabilities:} Infinitely many weight families satisfy compositionality (proven by CompositionalWeightFamily\_Infinite).
    \item \textbf{Unique entropy:} Entropy diverges without coarse-graining; the choice of coarse-graining is arbitrary (proven by EntropyImpossibility.v).
    \item \textbf{Unique Hamiltonian:} No unique energy function is forced.
\end{itemize}

\textbf{Additional axioms required:}
\begin{itemize}
    \item \textbf{Symmetry:} Rotational, translational, gauge symmetries reduce degrees of freedom.
    \item \textbf{Action principle:} Least action, stationary phase select dynamics.
    \item \textbf{Coarse-graining:} Explicit resolution choice defines entropy.
    \item \textbf{Boundary conditions:} Initial/final conditions break time symmetry.
\end{itemize}

\textbf{Why is this important?} This theorem \textit{clarifies} the relationship between computation and physics:
\begin{itemize}
    \item \textbf{Not a TOE:} The kernel is not a Theory of Everything---it's a \textit{framework} for theories.
    \item \textbf{Honest about limits:} Explicitly identifies what's missing (dynamics, probabilities, entropy).
    \item \textbf{Guides future work:} Shows where to add axioms to recover physics.
\end{itemize}

\textbf{Implication:} A unique physical theory cannot be derived from computational structure alone. Additional axioms (symmetry, coarse-graining, boundary conditions) are required.

\textbf{Philosophical interpretation:} Physics is \textit{not} purely computational. Computation provides constraints and structure, but physics requires \textit{contingent choices} (symmetries, initial conditions) that are not forced by logic.

\textbf{Role in thesis:} Establishes intellectual honesty: the thesis does not overclaim. The kernel provides powerful constraints, but a full TOE requires additional principles beyond compositionality.

\section{Complexity Comparison}

The Thiele Machine provides an alternative complexity model. The table below should be read as a qualitative comparison: time decreases as $\mu$ increases, not as a claim of universal asymptotic dominance.

\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|l|l|}
\hline
\textbf{Algorithm} & \textbf{Classical} & \textbf{Thiele} \\
\hline
Integer factoring & Sub-exponential (classical) & Time traded for explicit $\mu$ cost \\
Period finding & $O(\sqrt{N})$ (classical) & Time traded for explicit $\mu$ cost \\
CHSH optimization & Brute force & Structure-aware \\
\hline
\end{tabular}
}
\end{center}

The key insight: Thiele Machine trades \textbf{blind search time} for \textbf{explicit structure cost} ($\mu$).

\section{Summary}

% ============================================================================
% FIGURE: Chapter Summary
% ============================================================================
\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=1.8, 
    node distance=2cm,
    result/.style={rectangle, draw, rounded corners, minimum width=5.4cm, minimum height=1.4cm, align=center, fill=green!15},
    central/.style={rectangle, draw, rounded corners, minimum width=7.2cm, minimum height=1.8cm, align=center, fill=yellow!20},
    arrow/.style={->, >=Stealth, thick}
]
    % Results
    \node[result, align=center, text width=3.5cm] (physics) at (-3, 1.5) {Physics Models\\Conservation Laws};
    \node[result, align=center, text width=3.5cm] (shor) at (3, 1.5) {Shor Primitives\\Verified};
    \node[result, align=center, text width=3.5cm] (bridge) at (-3, -1.5) {Bridge Modules\\6 files};
    \node[result, align=center, text width=3.5cm] (toe) at (3, -1.5) {TOE Limits\\No unique physics};
    
    % Central
    \node[central, align=center, text width=3.5cm] (central) at (0, 0) {\textbf{Theory $\leftrightarrow$ Algorithms}\\Infrastructure};
    
    % Arrows
    \draw[arrow, shorten >=2pt, shorten <=2pt] (physics) -- (central);
    \draw[arrow, shorten >=2pt, shorten <=2pt] (shor) -- (central);
    \draw[arrow, shorten >=2pt, shorten <=2pt] (bridge) -- (central);
    \draw[arrow, shorten >=2pt, shorten <=2pt] (toe) -- (central);
\end{tikzpicture}
\caption{Chapter D summary: physics models, Shor primitives, bridge modules, and TOE limits form the theory-algorithm infrastructure.}

\paragraph{Understanding Figure~\ref{fig:ch12-summary}:}

This summary diagram synthesizes Chapter~12's dual contribution: establishing the \textbf{theory-algorithm infrastructure} connecting abstract physics models (demonstrating computation exhibits physical laws) with concrete algorithmic primitives (formalizing quantum-inspired algorithms like Shor's factoring), while simultaneously clarifying the \textbf{limits} of what computational structure alone can determine (Theory of Everything no-go results). The central yellow box emphasizes this infrastructure role: physics models and algorithms are not endpoints but \textit{infrastructure} for future theoretical and practical work.

\textbf{Visual elements:} The diagram shows four \textbf{green result boxes} positioned at the four corners around a central \textbf{yellow box}: ``Physics Models \\ Conservation Laws'' (upper left), ``Shor Primitives \\ Verified'' (upper right), ``Bridge Modules \\ 6 files'' (lower left), ``TOE Limits \\ No unique physics'' (lower right). Black arrows point from each green box toward the central yellow box labeled ``\textbf{Theory $\leftrightarrow$ Algorithms} \\ Infrastructure,'' indicating these four components all contribute to building the infrastructure layer. The bidirectional arrow ($\leftrightarrow$) in the central box emphasizes the two-way connection: physics informs algorithms (conservation laws constrain algorithmic primitives), algorithms validate physics (Shor's algorithm demonstrates partition structure is computationally exploitable).

\textbf{The four infrastructure components:}

\begin{itemize}
    \item \textbf{Physics Models \\ Conservation Laws (upper left):} Three formally verified Coq models demonstrating physical laws emerge from computational structure: (1) \textbf{Wave Propagation:} Discrete 1D wave with left/right amplitudes on lattice. Proven conservation laws: energy $E = \sum_i (L_i^2 + R_i^2)$ conserved (rotation preserves sum of squares), momentum $P = \sum_i (R_i - L_i)$ conserved (rotation preserves signed sum), dynamics reversible ($\texttt{wave\_step\_inv} \circ \texttt{wave\_step} = \text{id}$). Implementation: \texttt{WaveCell} record, \texttt{wave\_step} function using \texttt{rotate\_left}/\texttt{rotate\_right}, theorems \texttt{wave\_energy\_conserved}, \texttt{wave\_momentum\_conserved}, \texttt{wave\_step\_reversible} in \path{coq/physics/WaveModel.v}. (2) \textbf{Dissipative Systems:} Model of irreversible dynamics where energy dissipates as heat, connecting to $\mu$-monotonicity (information erasure increases $\mu$, mirroring Landauer principle validated in Chapter~11 experiments). (3) \textbf{Discrete Lattices:} Model of emergent spacetime from computational steps (lattice-based dynamics with locality enforced by partition boundaries). Summary: these models prove computation \textit{is} physics (not analogy)---conservation laws are \textit{theorems} derived from computational axioms (rotation operations), not assumptions imported from physics.
    
    \item \textbf{Shor Primitives \\ Verified (upper right):} Formally verified number-theoretic algorithms forming Shor's factoring algorithm foundation, all machine-checked in Coq with zero admits: (1) \textbf{Period Finding:} Core subroutine finding smallest $r$ such that $a^r \equiv 1 \pmod{N}$. Definitions: \texttt{is\_period(r)} ($r > 0 \land \forall k, \texttt{pow\_mod}(k+r) = \texttt{pow\_mod}(k)$), \texttt{minimal\_period(r)} (smallest valid period), \texttt{shor\_candidate(r)} computing $\gcd(a^{r/2} - 1, N)$. (2) \textbf{Euclidean GCD:} Classical algorithm computing $\gcd(a,b)$ in $O(\log \min(a,b))$ time. Proven theorems: \texttt{gcd\_euclid\_divides\_left}, \texttt{gcd\_euclid\_divides\_right} ensuring correctness. (3) \textbf{Modular Arithmetic:} Efficient exponentiation via repeated squaring (\texttt{mod\_pow} computes $a^b \bmod n$ in $O(\log b)$ time). Proven lemma: \texttt{mod\_pow\_mult} (exponents add: $a^{b+c} \equiv a^b \cdot a^c \pmod{n}$). Flagship theorem: \texttt{shor\_reduction} proves \textit{given period $r$, factors follow}: if $r$ minimal period, $r$ even, $g = \gcd(a^{r/2}-1, N)$ non-trivial, then $g | N$ (factor extracted). Formalized in \path{coq/shor\_primitives/PeriodFinding.v}. Summary: Shor's algorithm---the poster child for quantum advantage---is now \textit{formally verified}, eliminating doubts about mathematical correctness and providing machine-checkable factoring proofs.
    
    \item \textbf{Bridge Modules \\ 6 files (lower left):} Infrastructure connecting high-level domain concepts (randomness, wave dynamics, Shor's algorithm) to low-level kernel traces (receipt sequences) via channel selectors and decode lemmas. Six bridge files total: (1) \textbf{Randomness bridge} (\path{coq/bridge/Randomness\_to\_Kernel.v}): defines \texttt{RAND\_TRIAL\_OP := 1001}, \texttt{RandChannel} predicate filtering randomness receipts, \texttt{decode\_is\_filter\_payloads} lemma proving extraction correctness. (2) \textbf{Tomography bridge} (C-TOMO): connects precision-cost relationship $n \geq c \cdot \varepsilon^{-2}$ to receipt annotations. (3) \textbf{Entropy bridge} (C-ENTROPY): connects coarse-graining requirements (entropy undefined without discretization, Chapter~10 region\_equiv\_class\_infinite theorem) to kernel traces. (4) \textbf{Causation bridge} (C-CAUSAL): connects Markov equivalence (unique DAG claims require interventions or 8192 disclosure bits, Chapter~9 verifier) to causal structure queries. (5) \textbf{Wave embedding} (\path{coq/thielemachine/coqproofs/WaveEmbedding.v}): embeds wave model into kernel (each cell becomes module, conservation laws transfer to partition structure). (6) \textbf{Shor reduction bridge:} embeds period-finding steps as receipt-annotated traces (verifier reconstructs computation). Summary: bridges make abstract models (physics, algorithms) \textit{executable} and \textit{verifiable} in kernel semantics---not informal analogies but concrete translations with proven correctness (decode lemmas establish filter-map equivalence).
    
    \item \textbf{TOE Limits \\ No unique physics (lower right):} Rigorous no-go theorems establishing what computational structure \textit{cannot} determine: (1) \textbf{KernelMaximalClosure theorem:} Kernel is maximally closed (forces locality, $\mu$-monotonicity, cone locality---all constraints derivable from compositionality), but cannot force additional universal constraints without extra axioms. (2) \textbf{CompositionalWeightFamily\_Infinite theorem:} Infinitely many distinct weight families (probability measures) satisfy compositional laws. Proof constructs explicit family: $\forall k_1 \neq k_2, \exists t : w_{k_1}(t) \neq w_{k_2}(t)$. Implication: kernel does not uniquely determine probabilities (Born rule is \textit{extra axiom}, not forced by compositionality). (3) \textbf{Physics\_Requires\_Extra\_Structure theorem:} Definitive TOE no-go result proving computational structure alone cannot uniquely determine physics. Additional axioms required: symmetry (rotational/translational/gauge reduce degrees of freedom), action principle (least action/stationary phase select dynamics), coarse-graining (explicit resolution choice defines entropy, validated by Chapter~11 experiments showing raw entropy diverges), boundary conditions (initial/final conditions break time symmetry). (4) \textbf{Region\_equiv\_class\_infinite theorem:} Observational equivalence classes have infinite cardinality, making entropy undefined without coarse-graining (Chapter~10 impossibility theorem). Summary: these no-go results clarify the kernel is \textit{not} a Theory of Everything but a \textit{framework} for theories, providing constraints (locality, irreversibility) without uniquely determining dynamics/probabilities/entropy. Honest about limits: explicitly identifies missing structure (symmetries, coarse-graining, boundaries).
\end{itemize}

\textbf{Key insight visualized:} Chapter~12 establishes the \textit{infrastructure layer} bridging theory and practice: physics models validate computation exhibits physical laws (supporting claim computation is fundamental, physics emergent), Shor primitives demonstrate partition-aware algorithms achieve quantum-like speedups (supporting complexity gap via structure revelation), bridge modules make both connections executable (translation from domain to kernel is not informal but formally verified), TOE limits clarify boundaries (what's forced by compositionality vs. what requires extra axioms). The central ``Theory $\leftrightarrow$ Algorithms Infrastructure'' box emphasizes bidirectionality: theory informs practice (conservation laws constrain algorithms, impossibility theorems bound randomness extraction), practice validates theory (Shor's algorithm demonstrates partition structure is computationally exploitable, experiments validate conservation laws empirically).

\textbf{How to read this diagram:} Start with the four green result boxes at corners representing Chapter~12's contributions: ``Physics Models \\ Conservation Laws'' (upper left: wave/dissipative/lattice models with proven energy/momentum conservation, reversibility), ``Shor Primitives \\ Verified'' (upper right: period finding, GCD, modular arithmetic formalized with \texttt{shor\_reduction} theorem), ``Bridge Modules \\ 6 files'' (lower left: randomness/tomography/entropy/causation/wave/Shor bridges connecting domains to kernel via receipt channels and decode lemmas), ``TOE Limits \\ No unique physics'' (lower right: maximal closure + infinite weight families + physics requires extra structure theorems establishing what compositionality cannot determine). Black arrows point from all four corners to central yellow box ``Theory $\leftrightarrow$ Algorithms Infrastructure,'' showing these components converge to form the infrastructure layer. Bidirectional arrow emphasizes two-way connection: theory constrains algorithms (TOE limits bound what's achievable, conservation laws restrict dynamics), algorithms validate theory (Shor demonstrates structure exploitation, experiments confirm predictions).

\textbf{Role in thesis:} This summary diagram demonstrates Chapter~12 (Appendix D) provides the \textit{connective tissue} between abstract theory (Chapters~3--10: kernel semantics, proofs, TOE limits) and concrete practice (Chapter~11: experiments, Chapter~13: hardware). The four components establish: (1) \textbf{Physics models:} Validate computation \textit{is} physics via proven conservation laws emerging from computational axioms (wave energy/momentum conservation, dissipative $\mu$-monotonicity, lattice locality). Connect to Chapter~11 experiments (Landauer principle, locality tests, reversibility) showing these laws hold empirically. (2) \textbf{Shor primitives:} Formalize quantum-inspired factoring as partition structure revelation. Connect to Chapter~11 complexity gap experiments (partition-aware achieves $10^7\times$ speedup on structured problems) and RSA-2048 demonstration showing classical factoring infeasible but Shor's algorithm achieves polynomial-time reduction plus $\mu$-cost. (3) \textbf{Bridge modules:} Make domain-specific models (physics, Shor) executable as kernel traces. Connect to Chapter~9 verifier system (which provides TRS-1.0 receipts bridged by decode lemmas) and Chapter~13 hardware (which must execute bridged traces on RTL). (4) \textbf{TOE limits:} Establish intellectual honesty---the thesis does not overclaim. Kernel provides powerful constraints (locality, irreversibility) but cannot uniquely determine physics (probabilities, entropy, dynamics require extra axioms). Connect to Chapter~10 impossibility theorems (entropy diverges without coarse-graining, infinitely many weight families satisfy laws) and philosophical stance (computation is fundamental, physics requires contingent choices like symmetries/boundaries). The ``Infrastructure'' central label emphasizes these are not final results but \textit{building blocks} for future theoretical work (adding symmetry/action principles to kernel) and practical applications (DI randomness, Shor-based factoring, wave-inspired algorithms).

\label{fig:ch12-summary}
\end{figure}

This chapter establishes:
\begin{enumerate}
    \item \textbf{Physics models}: Wave, dissipative, discrete dynamics with conservation laws
    \item \textbf{Shor primitives}: Period finding and factorization reduction, formally verified
    \item \textbf{Bridge modules}: domain-to-kernel bridges via receipt channels
    \item \textbf{Flagship track}: DI randomness with quantitative bounds
    \item \textbf{TOE limits}: No unique physics from compositionality alone
\end{enumerate}

The mathematical infrastructure supports both theoretical impossibility results and practical algorithmic applications.
