\section{Physics Models and Algorithmic Primitives}

\begin{quote}
\textit{Author's Note (Devon): This is where things get... weird. And exciting. I'm not a physicist. I sold cars. But the patterns I found in this model---they look like physics. Waves. Conservation laws. Entropy. I didn't put them there on purpose. They just... emerged. Either I accidentally discovered something real, or I'm seeing patterns that aren't there. I genuinely don't know. But I wrote it down, proved what I could, and put it out there for smarter people to judge.}
\end{quote}

\subsection{Computation as Physics}

A central claim of this thesis is that computation is not merely an abstract mathematical process---it is a \textit{physical} process subject to physical laws. When a computer erases a bit, it dissipates heat. When it stores information, it consumes energy. The $\mu$-ledger tracks these physical costs.

To validate this connection, I develop explicit physics models within the Coq framework:
\begin{itemize}
    \item \textbf{Wave propagation}: A model of reversible dynamics with conservation laws
    \item \textbf{Dissipative systems}: A model of irreversible dynamics connecting to $\mu$-monotonicity
    \item \textbf{Discrete lattices}: A model of emergent spacetime from computational steps
\end{itemize}

These models are not metaphors---they are formally verified Coq proofs showing that computational structures exhibit physical-like behavior.
The wave model lives in \texttt{coq/physics/WaveModel.v}, and its embedding into the Thiele Machine is proven in \texttt{coq/thielemachine/coqproofs/WaveEmbedding.v}. The lattice and dissipative models follow the same pattern: define a state and step function, then prove conservation or monotonicity lemmas that can be linked back to kernel invariants.

\subsection{From Theory to Algorithms}

The second part of this chapter bridges the abstract theory to concrete algorithms. The Shor primitives demonstrate that the period-finding core of Shor's factoring algorithm can be formalized and verified in Coq, connecting:
\begin{itemize}
    \item Number theory (modular arithmetic, GCD)
    \item Computational complexity (polynomial vs.\ exponential)
    \item The Thiele Machine's $\mu$-cost model
\end{itemize}

This chapter documents the physics models that demonstrate emergent conservation laws and the algorithmic primitives that bridge abstract mathematics to concrete factorization.

\section{Physics Models}


The formal development contains verified physics models that demonstrate how physical laws emerge from computational structure.

\subsection{Wave Propagation Model}

Representative model: a 1D wave dynamics model with left- and right-moving amplitudes:
\begin{lstlisting}
Record WaveCell := {
  left_amp : nat;
  right_amp : nat
}.

Definition WaveState := list WaveCell.

Definition wave_step (s : WaveState) : WaveState :=
  let lefts := rotate_left (map left_amp s) in
  let rights := rotate_right (map right_amp s) in
  map2 (fun l r => {| left_amp := l; right_amp := r |}) lefts rights.
\end{lstlisting}

\paragraph{Understanding the Wave Propagation Model:}

\textbf{What is this model?} This is a \textbf{discrete 1D wave equation} where waves propagate left and right on a lattice. Each cell contains left-moving and right-moving amplitudes that shift positions each time step.

\textbf{Record structure breakdown:}
\begin{itemize}
    \item \textbf{WaveCell:} A single lattice site with two amplitude components:
    \begin{itemize}
        \item \textbf{left\_amp: nat} — Amplitude of left-moving wave component (moving toward lower indices).
        \item \textbf{right\_amp: nat} — Amplitude of right-moving wave component (moving toward higher indices).
    \end{itemize}
    
    \item \textbf{WaveState:} List of cells representing the entire 1D lattice. Example: 100-cell lattice = list of 100 WaveCells.
\end{itemize}

\textbf{Wave step dynamics:}
\begin{itemize}
    \item \textbf{rotate\_left:} Shifts all left-moving amplitudes one position left (index $i \to i-1$, with wraparound).
    \item \textbf{rotate\_right:} Shifts all right-moving amplitudes one position right (index $i \to i+1$, with wraparound).
    \item \textbf{map2:} Combines shifted amplitudes back into cells at each position.
\end{itemize}

\textbf{Physical interpretation:} This models wave propagation on a discrete spacetime:
\begin{itemize}
    \item \textbf{Left-movers:} Like photons moving left at speed $c$ (one cell per time step).
    \item \textbf{Right-movers:} Like photons moving right at speed $c$.
    \item \textbf{No interaction:} Left and right movers pass through each other (linear wave equation).
\end{itemize}

\textbf{Example:} 5-cell lattice with one right-moving pulse:
\begin{itemize}
    \item \textbf{Initial state:} $[(0,0), (0,1), (0,0), (0,0), (0,0)]$ (pulse at position 1).
    \item \textbf{After 1 step:} $[(0,0), (0,0), (0,1), (0,0), (0,0)]$ (pulse moves right to position 2).
    \item \textbf{After 2 steps:} $[(0,0), (0,0), (0,0), (0,1), (0,0)]$ (pulse at position 3).
\end{itemize}

\textbf{Connection to kernel:} This wave model can be embedded into kernel semantics via partition structure (each cell becomes a module). The conservation laws (energy, momentum, reversibility) proven for \texttt{wave\_step} transfer to the kernel via embedding lemmas.


\textbf{Conservation theorems:}
\begin{lstlisting}
Theorem wave_energy_conserved : 
  forall s, wave_energy (wave_step s) = wave_energy s.

Theorem wave_momentum_conserved : 
  forall s, wave_momentum (wave_step s) = wave_momentum s.

Theorem wave_step_reversible : 
  forall s, wave_step_inv (wave_step s) = s.
\end{lstlisting}

\paragraph{Understanding the Wave Conservation Theorems:}

\textbf{What do these theorems prove?} These are \textbf{conservation laws} for the discrete wave model: energy, momentum, and reversibility are preserved under time evolution.

\textbf{Theorem breakdown:}
\begin{itemize}
    \item \textbf{wave\_energy\_conserved:} Total energy $E = \sum_i (\text{left\_amp}_i^2 + \text{right\_amp}_i^2)$ is constant. Energy cannot be created or destroyed.
    
    \item \textbf{wave\_momentum\_conserved:} Total momentum $P = \sum_i (\text{right\_amp}_i^2 - \text{left\_amp}_i^2)$ is constant. Right-movers carry positive momentum, left-movers carry negative momentum.
    
    \item \textbf{wave\_step\_reversible:} The dynamics are reversible: applying the inverse step after the forward step recovers the original state. Time symmetry holds.
\end{itemize}

\textbf{Why are these laws important?} In physics, conservation laws are fundamental:
\begin{itemize}
    \item \textbf{Energy conservation} follows from time-translation symmetry (Noether's theorem).
    \item \textbf{Momentum conservation} follows from space-translation symmetry.
    \item \textbf{Reversibility} is the hallmark of fundamental dynamics (Hamiltonian systems).
\end{itemize}

These proofs demonstrate that even simple computational models exhibit physical-like conservation laws.

\textbf{Proof strategy:} Each theorem is proven by direct computation:
\begin{itemize}
    \item Energy: Show that rotation preserves sum of squares.
    \item Momentum: Show that rotation preserves signed sum.
    \item Reversibility: Construct inverse operation (rotate\_left inverts rotate\_right, vice versa).
\end{itemize}

\textbf{Connection to kernel:} These conservation laws \textit{transfer} to kernel semantics: if a computation embeds the wave model, the kernel's $\mu$-monotonicity acts as an irreversibility bound, while partition conservation mirrors energy/momentum conservation.


\subsection{Dissipative Model}

The dissipative model captures irreversible dynamics, connecting to $\mu$-monotonicity of the kernel.

\subsection{Discrete Model}

The discrete model uses lattice-based dynamics for discrete spacetime emergence.

\section{Shor Primitives}


The formalization includes the mathematical foundations of Shor's factoring algorithm.

\subsection{Period Finding}

Representative definitions:
\begin{lstlisting}
Definition is_period (r : nat) : Prop :=
  r > 0 /\ forall k, pow_mod (k + r) = pow_mod k.

Definition minimal_period (r : nat) : Prop :=
  is_period r /\ forall r', is_period r' -> r' >= r.

Definition shor_candidate (r : nat) : nat :=
  let half := r / 2 in
  let term := Nat.pow a half in
  gcd_euclid (term - 1) N.
\end{lstlisting}

\paragraph{Understanding the Period Finding Definitions:}

\textbf{What is period finding?} Period finding is the \textbf{core subroutine} of Shor's algorithm: given $a$ and $N$, find the smallest $r$ such that $a^r \equiv 1 \pmod{N}$.

\textbf{Definition breakdown:}
\begin{itemize}
    \item \textbf{is\_period(r):} Proposition stating $r$ is a period:
    \begin{itemize}
        \item \textbf{r > 0:} Period must be positive (trivial period 0 excluded).
        \item \textbf{forall k, pow\_mod(k+r) = pow\_mod(k):} The function $f(k) = a^k \bmod N$ is periodic with period $r$. For all $k$: $a^{k+r} \equiv a^k \pmod{N}$.
    \end{itemize}
    
    \item \textbf{minimal\_period(r):} The \textit{smallest} period:
    \begin{itemize}
        \item \textbf{is\_period r:} $r$ is a valid period.
        \item \textbf{forall r', is\_period r' -> r' >= r:} No smaller period exists.
    \end{itemize}
    
    \item \textbf{shor\_candidate(r):} Computes a potential factor of $N$:
    \begin{itemize}
        \item \textbf{half := r / 2:} Take half the period (requires even $r$).
        \item \textbf{term := Nat.pow a half:} Compute $a^{r/2}$.
        \item \textbf{gcd\_euclid(term - 1) N:} Compute $\gcd(a^{r/2} - 1, N)$.
    \end{itemize}
\end{itemize}

\textbf{Example:} Factoring $N = 15$ with $a = 2$:
\begin{itemize}
    \item \textbf{Find period:} $2^1 \equiv 2, 2^2 \equiv 4, 2^3 \equiv 8, 2^4 \equiv 1 \pmod{15}$. Period $r = 4$.
    \item \textbf{Compute candidate:} $a^{r/2} - 1 = 2^2 - 1 = 3$. $\gcd(3, 15) = 3$.
    \item \textbf{Extract factors:} $3$ divides $15$, so $15 = 3 \times 5$. Success!
\end{itemize}

\textbf{Why does this work?} If $a^r \equiv 1 \pmod{N}$ and $r$ is even, then:
\[
a^r - 1 = (a^{r/2} - 1)(a^{r/2} + 1) \equiv 0 \pmod{N}
\]
So $N$ divides $(a^{r/2} - 1)(a^{r/2} + 1)$. With high probability, $\gcd(a^{r/2} - 1, N)$ is a non-trivial factor.

\textbf{Connection to quantum computing:} Quantum computers find periods in $O((\log N)^3)$ time (exponentially faster than classical $O(\sqrt{N})$ algorithms). \textbf{IMPORTANT:} The Thiele Machine does \textit{not} achieve similar speedups for factorization. The formal development proves the correctness of the mathematical reduction (given period $r$, extract factors) but uses classical $O(\sqrt{N})$ trial division for period finding. Previous claims of polylog speedup were incorrect and have been retracted (see \texttt{PolylogConjecture.v}).


\textbf{The Shor Reduction Theorem:}
\begin{lstlisting}
Theorem shor_reduction :
  forall r,
    minimal_period r ->
    Nat.Even r ->
    let g := shor_candidate r in
    1 < g < N ->
    Nat.divide g N /\ 
    Nat.divide g (Nat.pow a (r / 2) - 1).
\end{lstlisting}

\paragraph{Understanding the Shor Reduction Theorem:}

\textbf{What does this theorem prove?} This is the \textbf{mathematical heart of Shor's algorithm}: if you know the period $r$, you can efficiently extract factors of $N$.

\textbf{Theorem statement breakdown:}
\begin{itemize}
    \item \textbf{Hypothesis 1: minimal\_period r} — $r$ is the smallest period of $a^k \bmod N$.
    
    \item \textbf{Hypothesis 2: Nat.Even r} — $r$ is even (required for factorization).
    
    \item \textbf{Hypothesis 3: 1 < g < N} — The GCD candidate $g = \gcd(a^{r/2} - 1, N)$ is non-trivial (not 1 or $N$).
    
    \item \textbf{Conclusion 1: Nat.divide g N} — $g$ divides $N$ (i.e., $g$ is a factor of $N$).
    
    \item \textbf{Conclusion 2: Nat.divide g (Nat.pow a (r/2) - 1)} — $g$ divides $a^{r/2} - 1$ (consistency check).
\end{itemize}

\textbf{Why is this powerful?} Classical factoring is hard (no known polynomial-time algorithm). Shor's algorithm reduces factoring to period finding:
\[
\text{Factoring } N \quad \xrightarrow{\text{Shor reduction}} \quad \text{Finding period } r \quad \xrightarrow{\text{Quantum}} \quad O(\log^3 N)
\]
The Thiele Machine achieves similar reductions via partition discovery (revealing period structure).

\textbf{Proof intuition:} Since $a^r \equiv 1 \pmod{N}$:
\[
a^r - 1 = (a^{r/2})^2 - 1 = (a^{r/2} - 1)(a^{r/2} + 1) \equiv 0 \pmod{N}
\]
So $N | (a^{r/2} - 1)(a^{r/2} + 1)$. If neither factor is divisible by $N$ individually (with high probability), then $\gcd(a^{r/2} - 1, N)$ gives a non-trivial factor.

\textbf{Example verification:} $N=21, a=2, r=6$:
\begin{itemize}
    \item $a^{r/2} - 1 = 2^3 - 1 = 7$.
    \item $\gcd(7, 21) = 7$.
    \item $7$ divides $21$, so $21 = 3 \times 7$. Factorization complete!
\end{itemize}

This is the mathematical core of Shor's algorithm: given the period $r$ of $a^r \equiv 1 \pmod{N}$, I can extract non-trivial factors via GCD.


\subsection{Verified Examples}

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{N} & \textbf{a} & \textbf{Period r} & \textbf{Factors} & \textbf{Verification} \\
\hline
21 & 2 & 6 & 3, 7 & $2^3 = 8$; $\gcd(7, 21) = 7$ \\
15 & 2 & 4 & 3, 5 & $2^2 = 4$; $\gcd(3, 15) = 3$ \\
35 & 2 & 12 & 5, 7 & $2^6 = 64 \equiv 29$; $\gcd(28, 35) = 7$ \\
\hline
\end{tabular}
\end{center}

\subsection{Euclidean Algorithm}

Representative Euclidean algorithm:
\begin{lstlisting}
Fixpoint gcd_euclid (a b : nat) : nat :=
  match b with
  | 0 => a
  | S b' => gcd_euclid b (a mod (S b'))
  end.

Theorem gcd_euclid_divides_left : 
  forall a b, Nat.divide (gcd_euclid a b) a.

Theorem gcd_euclid_divides_right : 
  forall a b, Nat.divide (gcd_euclid a b) b.
\end{lstlisting}

\paragraph{Understanding the Euclidean Algorithm:}

\textbf{What is this algorithm?} The \textbf{Euclidean algorithm} computes the greatest common divisor (GCD) of two natural numbers $a$ and $b$. It's one of the oldest algorithms (300 BCE) and is fundamental to number theory.

\textbf{Algorithm breakdown:}
\begin{itemize}
    \item \textbf{Base case (b = 0):} If $b = 0$, then $\gcd(a, 0) = a$.
    
    \item \textbf{Recursive case (b > 0):} Compute $\gcd(b, a \bmod b)$. This reduces the problem size: $a \bmod b < b$.
\end{itemize}

\textbf{Example:} $\gcd(48, 18)$:
\begin{itemize}
    \item $\gcd(48, 18) = \gcd(18, 48 \bmod 18) = \gcd(18, 12)$
    \item $\gcd(18, 12) = \gcd(12, 18 \bmod 12) = \gcd(12, 6)$
    \item $\gcd(12, 6) = \gcd(6, 12 \bmod 6) = \gcd(6, 0)$
    \item $\gcd(6, 0) = 6$
\end{itemize}

\textbf{Theorem breakdown:}
\begin{itemize}
    \item \textbf{gcd\_euclid\_divides\_left:} The GCD divides $a$. Formally: $\gcd(a, b) | a$.
    
    \item \textbf{gcd\_euclid\_divides\_right:} The GCD divides $b$. Formally: $\gcd(a, b) | b$.
\end{itemize}

\textbf{Why is this important for Shor's algorithm?} The GCD extraction step in Shor's algorithm uses this: $g = \gcd(a^{r/2} - 1, N)$. The Euclidean algorithm computes $g$ efficiently in $O(\log \min(a, b))$ steps.

\textbf{Proof strategy:} Both theorems are proven by induction on the recursive structure of \texttt{gcd\_euclid}. The key insight: if $\gcd(b, a \bmod b) | b$ and $\gcd(b, a \bmod b) | (a \bmod b)$, then $\gcd(b, a \bmod b) | a$ (by the division algorithm).


\paragraph{Understanding the Euclidean Algorithm:}

\textbf{What is the Euclidean algorithm?} The \textbf{Euclidean algorithm} computes the greatest common divisor (GCD) of two numbers efficiently in $O(\log \min(a,b))$ time.

\textbf{Algorithm breakdown:}
\begin{itemize}
    \item \textbf{Base case: b = 0} — If $b = 0$, then $\gcd(a, 0) = a$.
    
    \item \textbf{Recursive case: b > 0} — Replace $(a, b)$ with $(b, a \bmod b)$ and recurse.
\end{itemize}

\textbf{Why does this work?} Key insight: $\gcd(a, b) = \gcd(b, a \bmod b)$.
\begin{itemize}
    \item Any divisor of $a$ and $b$ also divides $a \bmod b$ (since $a = qb + (a \bmod b)$).
    \item The algorithm terminates when $b = 0$ (guaranteed after $O(\log b)$ steps).
\end{itemize}

\textbf{Example:} $\gcd(48, 18)$:
\begin{itemize}
    \item $\gcd(48, 18) = \gcd(18, 48 \bmod 18) = \gcd(18, 12)$
    \item $\gcd(18, 12) = \gcd(12, 18 \bmod 12) = \gcd(12, 6)$
    \item $\gcd(12, 6) = \gcd(6, 12 \bmod 6) = \gcd(6, 0)$
    \item $\gcd(6, 0) = 6$ (base case).
\end{itemize}
Result: $\gcd(48, 18) = 6$.

\textbf{Theorems proven:}
\begin{itemize}
    \item \textbf{gcd\_euclid\_divides\_left:} The GCD divides $a$. Proof by induction on recursive structure.
    \item \textbf{gcd\_euclid\_divides\_right:} The GCD divides $b$. Follows from divisibility preservation.
\end{itemize}

\textbf{Connection to Shor's algorithm:} The Euclidean algorithm is used to compute $\gcd(a^{r/2} - 1, N)$ in the Shor reduction. The Coq formalization ensures this step is correct.


\subsection{Modular Arithmetic}

Representative modular arithmetic lemma:
\begin{lstlisting}
Definition mod_pow (n base exp : nat) : nat := ...

Theorem mod_pow_mult : 
  forall n a b c, mod_pow n a (b + c) = ...
\end{lstlisting}

\paragraph{Understanding Modular Arithmetic:}

\textbf{What is modular exponentiation?} \textbf{Modular exponentiation} computes $a^b \bmod n$ efficiently without computing the full exponential $a^b$ (which would overflow for large $b$).

\textbf{Definition breakdown:}
\begin{itemize}
    \item \textbf{mod\_pow(n, base, exp):} Computes $\text{base}^{\text{exp}} \bmod n$ using repeated squaring.
    
    \item \textbf{Algorithm:} Binary exponentiation:
    \begin{itemize}
        \item If $\text{exp} = 0$: return $1$.
        \item If $\text{exp}$ is even: $a^{2k} = (a^k)^2$, compute recursively.
        \item If $\text{exp}$ is odd: $a^{2k+1} = a \cdot (a^k)^2$.
    \end{itemize}
    All intermediate results taken $\bmod n$ to prevent overflow.
\end{itemize}

\textbf{Theorem breakdown:}
\begin{itemize}
    \item \textbf{mod\_pow\_mult:} Exponent addition property: $a^{b+c} \bmod n = (a^b \cdot a^c) \bmod n$.
    
    \item This is a fundamental property of modular arithmetic used throughout Shor's algorithm.
\end{itemize}

\textbf{Example:} Compute $2^{10} \bmod 15$:
\begin{itemize}
    \item Naive: $2^{10} = 1024$, then $1024 \bmod 15 = 4$.
    \item Efficient: $2^{10} = (2^5)^2 \bmod 15 = (32 \bmod 15)^2 \bmod 15 = 2^2 \bmod 15 = 4$.
\end{itemize}

\textbf{Why is this important?} Period finding in Shor's algorithm requires computing $a^k \bmod N$ for many values of $k$. Modular exponentiation makes this feasible even for large $N$ (e.g., RSA-2048 with 617-digit numbers).


\paragraph{Understanding the Modular Arithmetic Lemma:}

\textbf{What is modular exponentiation?} \textbf{Modular exponentiation} computes $a^b \bmod n$ efficiently without computing the full power $a^b$ (which would overflow).

\textbf{Definition:} \texttt{mod\_pow n base exp} computes $\text{base}^{\text{exp}} \bmod n$ using repeated squaring:
\begin{itemize}
    \item If $\text{exp} = 0$: return 1.
    \item If $\text{exp}$ is even: $a^{2k} = (a^k)^2$, compute recursively.
    \item If $\text{exp}$ is odd: $a^{2k+1} = a \cdot a^{2k}$, multiply and recurse.
\end{itemize}
This runs in $O(\log \text{exp})$ time instead of $O(\text{exp})$.

\textbf{Theorem: mod\_pow\_mult} — Exponents add: $a^{b+c} \equiv a^b \cdot a^c \pmod{n}$.
\begin{itemize}
    \item This is the fundamental property of exponentiation.
    \item Used extensively in period finding: $a^{k+r} \equiv a^k \cdot a^r \pmod{N}$.
\end{itemize}

\textbf{Example:} Compute $2^{10} \bmod 13$:
\begin{itemize}
    \item $2^{10} = (2^5)^2$. Compute $2^5 = 32 \equiv 6 \pmod{13}$.
    \item $2^{10} \equiv 6^2 = 36 \equiv 10 \pmod{13}$.
\end{itemize}
Fast: only 2 multiplications instead of 10.

\textbf{Connection to Shor's algorithm:} Period finding requires computing $a^k \bmod N$ for many $k$. Modular exponentiation makes this feasible.


\section{Bridge Modules}

Bridge lemmas connect domain-specific constructs to kernel semantics via receipt channels.

\subsection{Randomness Bridge}

Representative bridge lemma:
\begin{lstlisting}
Definition RAND_TRIAL_OP : nat := 1001.

Definition RandChannel (r : Receipt) : bool :=
  Nat.eqb (r_op r) RAND_TRIAL_OP.

Lemma decode_is_filter_payloads :
  forall tr,
    decode RandChannel tr =
    map r_payload (filter RandChannel tr).
\end{lstlisting}

\paragraph{Understanding the Randomness Bridge:}

\textbf{What is a bridge module?} A \textbf{bridge} connects high-level domain-specific concepts (e.g., randomness trials) to low-level kernel traces (sequences of receipts).

\textbf{Bridge component breakdown:}
\begin{itemize}
    \item \textbf{RAND\_TRIAL\_OP := 1001} — Opcode for randomness trial operations. Receipts with this opcode represent randomness events.
    
    \item \textbf{RandChannel(r)} — Predicate testing if receipt $r$ is randomness-relevant:
    \begin{itemize}
        \item \textbf{Nat.eqb (r\_op r) RAND\_TRIAL\_OP} — True if receipt's opcode equals 1001.
    \end{itemize}
    
    \item \textbf{decode RandChannel tr} — Extracts randomness data from trace $tr$:
    \begin{itemize}
        \item \textbf{filter RandChannel tr} — Keep only randomness receipts.
        \item \textbf{map r\_payload} — Extract payload (random bits) from each receipt.
    \end{itemize}
\end{itemize}

\textbf{Lemma: decode\_is\_filter\_payloads} — Proves that decoding is equivalent to filtering then mapping payloads. This is the formal guarantee that the bridge correctly extracts randomness data.

\textbf{Why is this important?} Without bridges, there's no connection between:
\begin{itemize}
    \item High-level claims: "This algorithm generated 1000 random bits."
    \item Low-level reality: A trace of 50,000 receipts with mixed opcodes.
\end{itemize}
The bridge makes randomness claims \textit{verifiable}: you can inspect the trace and extract exactly the random bits claimed.

\textbf{Example:} Trace with 5 receipts:
\begin{itemize}
    \item Receipt 1: op=1001, payload=0b1011 (randomness).
    \item Receipt 2: op=2000, payload=... (not randomness, filtered out).
    \item Receipt 3: op=1001, payload=0b0110 (randomness).
    \item Receipt 4: op=1001, payload=0b1110 (randomness).
    \item Receipt 5: op=3000, payload=... (not randomness, filtered out).
\end{itemize}
Decoded randomness: $[0b1011, 0b0110, 0b1110]$ (3 random 4-bit strings).

This bridge defines how randomness-relevant receipts are extracted from traces.
The formal statement above appears in \texttt{coq/bridge/Randomness\_to\_Kernel.v}. It is the connective tissue between high-level randomness claims and the kernel trace semantics, ensuring that a "randomness proof" is literally a filtered view of receipted steps.


Each bridge defines:
\begin{enumerate}
    \item A channel selector (opcode-based filtering)
    \item Payload extraction from matching receipts
    \item Decode lemmas proving filter-map equivalence
\end{enumerate}

\subsection{BoxWorld Bridge}

The file \path{coq/bridge/BoxWorld_to_Kernel.v} (7KB, 9 theorems) embeds finite box-world predictions into kernel receipts:

\begin{lstlisting}
(** Box-world trial embedding *)
Definition TheoryTrial : Type := KC.Trial.
Definition TheoryProgram : Type := list TheoryTrial.

(** Translation to kernel instructions *)
Definition translate_trial (t : TheoryTrial) : vm_instruction :=
  instr_chsh_trial (trial_x t) (trial_y t) (trial_a t) (trial_b t) 1.

(** Simulation theorem: receipts recover theory trials *)
Theorem trials_preserved :
  forall prog s0 receipts,
    run_program (translate_program prog) s0 = (s', receipts) ->
    decode_trials receipts = prog.
\end{lstlisting}

\textbf{What this proves:} Any finite box-world experiment (a list of CHSH trials with inputs $x,y$ and outputs $a,b$) can be embedded into kernel instructions, and the receipts exactly recover the original trials. This is a \textit{semantics-preserving embedding} of physical observables.

\subsection{FiniteQuantum Bridge}

The file \path{coq/bridge/FiniteQuantum_to_Kernel.v} (8KB, 10 theorems) extends the box-world bridge to quantum-admissible correlations:

\begin{lstlisting}
(** Tsirelson-envelope admissibility *)
Definition quantum_admissible (trials : list Trial) : Prop :=
  chsh_statistic trials <= kernel_tsirelson_bound_q.

(** Concrete finite dataset matching policy threshold *)
Definition policy_threshold_dataset : list Trial := [...].

Lemma dataset_matches_threshold :
  chsh_statistic policy_threshold_dataset = 5657 / 2000.

(** Simulation theorem for quantum-admissible predictions *)
Theorem quantum_trials_preserved :
  forall prog,
    quantum_admissible prog ->
    decode_trials (run_quantum_program prog) = prog.
\end{lstlisting}

\textbf{What this proves:} Quantum-admissible correlations (those satisfying the Tsirelson bound) embed into the kernel with exact receipt recovery. The file also provides a concrete finite dataset achieving the policy threshold $5657/2000 \approx 2.8285 \approx 2\sqrt{2}$, making the quantum bound computationally verifiable.

\textbf{Why two bridge files?} \texttt{BoxWorld\_to\_Kernel.v} handles arbitrary correlations (up to the algebraic maximum of 4). \texttt{FiniteQuantum\_to\_Kernel.v} specializes to quantum-admissible correlations (up to $2\sqrt{2}$) and provides the concrete dataset used by the runtime policy.

\section{Flagship DI Randomness Track}

The project's flagship demonstration is \textbf{device-independent randomness} certification.

\subsection{Protocol Flow}

\begin{enumerate}
    \item \textbf{Transcript Generation}: decode receipts-only traces
    \item \textbf{Metric Computation}: compute $H_{\min}$ lower bound
    \item \textbf{Admissibility Check}: verify $K$-bounded structure addition
    \item \textbf{Bound Theorem}: $\text{Admissible}(K) \Rightarrow H_{\min} \le f(K)$
\end{enumerate}

\subsection{The Quantitative Bound}

Representative theorem:
\begin{lstlisting}
Theorem admissible_randomness_bound :
  forall K transcript,
    Admissible K transcript ->
    rng_metric transcript <= f K.
\end{lstlisting}

\paragraph{Understanding the Admissible Randomness Bound:}

\textbf{What does this theorem prove?} This theorem provides a \textbf{quantitative bound} on device-independent (DI) randomness: the amount of certifiable randomness is limited by the structure-addition budget $K$.

\textbf{Theorem statement breakdown:}
\begin{itemize}
    \item \textbf{Hypothesis: Admissible K transcript} — The transcript (sequence of measurement results) is $K$-admissible: it can be generated with at most $K$ bits of added structure ($\mu$-cost).
    
    \item \textbf{Conclusion: rng\_metric transcript <= f K} — The randomness metric (e.g., min-entropy $H_{\min}$) is bounded by a function of $K$.
\end{itemize}

\textbf{Key concepts:}
\begin{itemize}
    \item \textbf{Device-independent randomness:} Randomness certified \textit{without trusting the device}. Based only on observed correlations (e.g., Bell inequality violations).
    
    \item \textbf{Admissibility:} A transcript is admissible if it respects quantum bounds (e.g., Tsirelson bound) \textit{or} explicitly pays $\mu$-cost for supra-quantum correlations.
    
    \item \textbf{Structure-addition budget $K$:} Maximum $\mu$ paid to reveal structure. Higher $K$ allows more randomness extraction.
    
    \item \textbf{Function $f(K)$:} Explicit computable bound (e.g., $f(K) = c \cdot K$ for some constant $c$). Not asymptotic---exact!
\end{itemize}

\textbf{Example:} CHSH-based randomness:
\begin{itemize}
    \item Run 10,000 CHSH games, observe win rate 85.3\%.
    \item Transcript is quantum-admissible (within Tsirelson bound).
    \item Extract $H_{\min} \approx 0.23$ bits per trial (standard DI formula).
    \item Total randomness: $10,000 \times 0.23 = 2,300$ certified random bits.
\end{itemize}

The bound $f(K)$ is explicit and quantitative---certified randomness is bounded by structure-addition budget.

\textbf{Why is this powerful?} Standard DI randomness has \textit{assumptions} (quantum mechanics holds, devices isolated, etc.). This theorem makes assumptions \textit{explicit} via $K$: if you pay more $\mu$ (higher $K$), you can extract more randomness, but there's a computable bound.

\textbf{Connection to kernel:} The $\mu$ ledger tracks structure revelation. If a randomness generator claims to extract $R$ bits from $K$ $\mu$-cost, this theorem checks if $R \leq f(K)$. If not, the claim is rejected.


\subsection{Conflict Chart}

The closed-work pipeline generates a comparison artifact:
\begin{itemize}
    \item Repo-measured $f(K)$ envelope
    \item Reference curve from standard DI theory
    \item Explicit assumption documentation
\end{itemize}

This creates an ``external confrontation artifact''---outsiders can disagree on assumptions but must engage with the explicit numbers.

\section{Theory of Everything Limits}

\subsection{What the Kernel Forces}

Representative theorem:
\begin{lstlisting}
Theorem KernelMaximalClosure : KernelMaximalClosureP.
\end{lstlisting}

\paragraph{Understanding the Kernel Maximal Closure Theorem:}

\textbf{What does this theorem prove?} This theorem states the kernel is \textbf{maximally closed}: it enforces \textit{all} constraints derivable from compositionality, and \textit{no additional} constraints can be added without breaking compositionality.

\textbf{What the kernel forces:}
\begin{itemize}
    \item \textbf{No-signaling (locality):} Alice's choice cannot affect Bob's marginal distribution. Partition boundaries enforce this: disjoint modules cannot signal.
    
    \item \textbf{$\mu$-monotonicity (irreversibility accounting):} $\mu$ never decreases. Every observation, computation, or structural revelation costs $\mu \geq 1$.
    
    \item \textbf{Multi-step cone locality (causal structure):} Information propagates through causal cones. Module $M$ at time $t$ can only depend on modules within its past light cone.
\end{itemize}

\textbf{What is maximal closure?} The kernel constraints are \textit{complete}:
\begin{itemize}
    \item \textbf{Necessary:} All constraints follow from compositionality (partition boundaries + $\mu$-conservation).
    \item \textbf{Sufficient:} No additional constraints can be derived without adding extra axioms (e.g., symmetry, dynamics).
\end{itemize}

\textbf{Proof strategy:} Show that:
\begin{enumerate}
    \item All listed constraints (no-signaling, $\mu$-monotonicity, cone locality) are \textit{provable} from kernel axioms.
    \item No additional \textit{universal} constraint (one that applies to all valid traces) exists beyond these.
\end{enumerate}

\textbf{Why is this important?} Maximal closure means the kernel is \textit{tight}:
\begin{itemize}
    \item It's not \textit{underconstrained} (missing essential laws).
    \item It's not \textit{overconstrained} (imposing arbitrary restrictions).
\end{itemize}
The kernel captures \textit{exactly} what compositionality demands, no more, no less.

\textbf{Connection to TOE limits:} Maximal closure implies the kernel \textit{cannot} uniquely determine physics. It forces locality and irreversibility, but not dynamics, probabilities, or field equations. Those require extra structure.


\subsection{What the Kernel Cannot Force}

Representative theorem:
\begin{lstlisting}
Theorem CompositionalWeightFamily_Infinite :
  exists w : nat -> Weight,
    (forall k, weight_laws (w k)) /\
    (forall k1 k2, k1 <> k2 -> exists t, w k1 t <> w k2 t).
\end{lstlisting}

\paragraph{Understanding the Infinite Weight Families Theorem:}

\textbf{What does this theorem prove?} There exist \textbf{infinitely many distinct weight families} (probability measures) that all satisfy compositional constraints. The kernel does \textit{not} uniquely determine probabilities.

\textbf{Theorem statement breakdown:}
\begin{itemize}
    \item \textbf{exists w : nat -> Weight} — There exists an indexed family of weight functions $w_0, w_1, w_2, \ldots$
    
    \item \textbf{forall k, weight\_laws (w k)} — Each weight function $w_k$ satisfies compositional laws:
    \begin{itemize}
        \item Additivity: $w(A \cup B) = w(A) + w(B)$ for disjoint $A, B$.
        \item Normalization: $w(\Omega) = 1$ (total probability = 1).
        \item Non-negativity: $w(A) \geq 0$ for all events $A$.
    \end{itemize}
    
    \item \textbf{forall k1 k2, k1 <> k2 -> exists t, w k1 t <> w k2 t} — All weight functions are \textit{distinct}: for any two indices $k_1 \neq k_2$, there exists a trace $t$ where $w_{k_1}(t) \neq w_{k_2}(t)$.
\end{itemize}

\textbf{Why is this a problem for TOE?} A Theory of Everything should uniquely predict probabilities. But this theorem proves:
\begin{itemize}
    \item The kernel constraints (compositionality) are \textit{compatible} with infinitely many probability measures.
    \item No unique ``Born rule'' (quantum mechanical probabilities) is forced.
\end{itemize}

\textbf{Example:} Two valid weight families:
\begin{itemize}
    \item \textbf{$w_1$:} Uniform distribution over all traces (maximum entropy).
    \item \textbf{$w_2$:} Exponential distribution favoring low-$\mu$ traces (minimum action principle).
\end{itemize}
Both satisfy compositionality, but assign different probabilities to the same trace.

Infinitely many weight families satisfy compositionality---no unique probability measure is forced.

\textbf{Proof strategy:} Construct explicit families:
\begin{itemize}
    \item Start with one valid weight $w_0$ (e.g., uniform).
    \item Define $w_k$ by smoothly interpolating between $w_0$ and other measures (e.g., $w_k = (1 - \alpha_k) w_0 + \alpha_k w'$ for different $\alpha_k$).
    \item Verify each $w_k$ satisfies weight laws and all $w_k$ are distinct.
\end{itemize}

\textbf{Connection to physics:} Quantum mechanics uses the Born rule: $P = |\psi|^2$. But this theorem shows the Born rule is \textit{not} forced by compositionality---it's an \textit{extra axiom}.


\begin{lstlisting}
Theorem Physics_Requires_Extra_Structure : KernelNoGoForTOE_P.
\end{lstlisting}

\paragraph{Understanding the Physics Requires Extra Structure Theorem:}

\textbf{What does this theorem prove?} This is the \textbf{definitive TOE no-go result}: computational structure (the kernel) \textit{cannot} uniquely determine a physical theory. Extra axioms are \textit{required}.

\textbf{What the kernel provides:}
\begin{itemize}
    \item \textbf{Constraints:} Locality, $\mu$-monotonicity, causal structure.
    \item \textbf{Framework:} Partition dynamics, receipt semantics, conservation laws.
\end{itemize}

\textbf{What the kernel does NOT provide:}
\begin{itemize}
    \item \textbf{Unique dynamics:} Infinitely many time evolution operators satisfy kernel constraints.
    \item \textbf{Unique probabilities:} Infinitely many weight families satisfy compositionality (proven by CompositionalWeightFamily\_Infinite).
    \item \textbf{Unique entropy:} Entropy diverges without coarse-graining; the choice of coarse-graining is arbitrary (proven by EntropyImpossibility.v).
    \item \textbf{Unique Hamiltonian:} No unique energy function is forced.
\end{itemize}

\textbf{Additional axioms required:}
\begin{itemize}
    \item \textbf{Symmetry:} Rotational, translational, gauge symmetries reduce degrees of freedom.
    \item \textbf{Action principle:} Least action, stationary phase select dynamics.
    \item \textbf{Coarse-graining:} Explicit resolution choice defines entropy.
    \item \textbf{Boundary conditions:} Initial/final conditions break time symmetry.
\end{itemize}

\textbf{Why is this important?} This theorem \textit{clarifies} the relationship between computation and physics:
\begin{itemize}
    \item \textbf{Not a TOE:} The kernel is not a Theory of Everything---it's a \textit{framework} for theories.
    \item \textbf{Honest about limits:} Explicitly identifies what's missing (dynamics, probabilities, entropy).
    \item \textbf{Guides future work:} Shows where to add axioms to recover physics.
\end{itemize}

\textbf{Implication:} A unique physical theory cannot be derived from computational structure alone. Additional axioms (symmetry, coarse-graining, boundary conditions) are required.

\textbf{Philosophical interpretation:} Physics is \textit{not} purely computational. Computation provides constraints and structure, but physics requires \textit{contingent choices} (symmetries, initial conditions) that are not forced by logic.


\section{Complexity Comparison}

The Thiele Machine provides an alternative complexity model. The table below should be read as a qualitative comparison: time decreases as $\mu$ increases, not as a claim of universal asymptotic dominance.

\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|l|l|}
\hline
\textbf{Algorithm} & \textbf{Classical} & \textbf{Thiele} \\
\hline
Integer factoring & Sub-exponential (classical) & Time traded for explicit $\mu$ cost \\
Period finding & $O(\sqrt{N})$ (classical) & Time traded for explicit $\mu$ cost \\
CHSH optimization & Brute force & Structure-aware \\
\hline
\end{tabular}
}
\end{center}

The key insight: Thiele Machine trades \textbf{blind search time} for \textbf{explicit structure cost} ($\mu$).

\section{Summary}


This chapter establishes:
\begin{enumerate}
    \item \textbf{Physics models}: Wave, dissipative, discrete dynamics with conservation laws
    \item \textbf{Shor primitives}: Period finding and factorization reduction, formally verified
    \item \textbf{Bridge modules}: domain-to-kernel bridges via receipt channels
    \item \textbf{Flagship track}: DI randomness with quantitative bounds
    \item \textbf{TOE limits}: No unique physics from compositionality alone
\end{enumerate}

The mathematical infrastructure supports both theoretical impossibility results and practical algorithmic applications.
