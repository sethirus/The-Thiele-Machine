\section{Why This Background Matters}

\subsection{A Foundation for Understanding}

Before diving into the Thiele Machine, I need to understand \textit{what problem it solves}. This requires revisiting fundamental concepts from:
\begin{itemize}
    \item \textbf{Computation theory}: What is a computer, really? (Turing Machines, RAM models)
    \item \textbf{Information theory}: What is information, and how do I measure it? (Shannon entropy, Kolmogorov complexity)
    \item \textbf{Physics of computation}: What are the physical limits on computing? (Landauer's principle, thermodynamics)
    \item \textbf{Quantum computing}: What does "quantum advantage" mean? (Bell's theorem, CHSH inequality)
    \item \textbf{Formal verification}: How can I \textit{prove} things about programs? (Coq, proof assistants)
\end{itemize}

\subsection{The Central Question}

Classical computers (Turing Machines, RAM machines) are \textit{structurally blind}---they lack primitive access to the structure of their input. If you give a computer a sorted list, it doesn't "know" the list is sorted unless it checks. This is a statement about the interface of the model, not about what is computable. The distinction is between \emph{access} and \emph{ability}: structure is discoverable, but only through explicit computation.

This raises a profound question: \textit{What if structural knowledge were a first-class resource that must be discovered, paid for, and accounted for?}

To understand why this question matters, I first need to understand what classical computers can and cannot do, and what I mean by "structure" and "information."
The Thiele Machine answers this question by embedding structure into the machine state itself (as partitions and axioms) and by explicitly tracking the cost of adding that structure. That design choice is the bridge between the background material in this chapter and the formal model introduced in Chapter 3.

\subsection{How to Read This Chapter}

This chapter is organized from concrete to abstract:
\begin{enumerate}
    \item Section 2.1: Classical computation models (Turing Machine, RAM)
    \item Section 2.2: Information theory (Shannon, Kolmogorov, MDL)
    \item Section 2.3: Physics of computation (Landauer, thermodynamics)
    \item Section 2.4: Quantum computing and correlations (Bell, CHSH)
    \item Section 2.5: Formal verification (Coq, proof-carrying code)
\end{enumerate}

If you are familiar with any section, feel free to skip it. The only prerequisite for later chapters is understanding:
\begin{itemize}
    \item The "blindness problem" in classical computation (§2.1.1)
    \item Kolmogorov complexity and MDL (§2.2.2--2.2.3)
    \item The CHSH inequality and Tsirelson bound (§2.4.1)
\end{itemize}

\section{Classical Computational Models}

\subsection{The Turing Machine: Formal Definition}

% TikZ Figure: Turing Machine Architecture
\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=0.85, transform shape], node distance=3cm]
    % Tape
    \foreach \x in {0,1,2,3,4,5,6,7,8} {
        \draw (\x,0) rectangle (\x+1,1);
    }
    % Tape contents
    \node at (0.5,0.5) {$\sqcup$};
    \node at (1.5,0.5) {0};
    \node at (2.5,0.5) {1};
    \node at (3.5,0.5) {1};
    \node at (4.5,0.5) {0};
    \node at (5.5,0.5) {1};
    \node at (6.5,0.5) {0};
    \node at (7.5,0.5) {$\sqcup$};
    \node at (8.5,0.5) {$\sqcup$};
    
    % Head
    \draw[very thick, blue, ->, >=stealth, shorten >=2pt, shorten <=2pt] (4.5,2) -- (4.5,1.1);
    \node[blue] at (4.5,2.3) {Head};
    
    % Control unit
    \draw[very thick, rounded corners, fill=blue!10] (3,3) rectangle (6,4.5);
    \node at (4.5,4) {Control};
    \node at (4.5,3.5) {$q \in Q$};
    
    % Transition function
    \draw[very thick, ->, >=stealth, shorten >=2pt, shorten <=2pt] (6,3.75) -- (7.5,3.75);
    \node[right] at (7.5,3.75) {$\delta(q,\gamma) \to (q',\gamma',d)$};
    
    % Labels
    \node[below] at (4.5,-0.3) {Infinite tape $\Gamma^*$};
    \draw[<->, >=stealth, shorten >=2pt, shorten <=2pt] (-0.5,0.5) -- (-0.5,0.5) node[left, above, yshift=6pt, pos=0.5, font=\small] {$\cdots$};
    \draw[<->, >=stealth, shorten >=2pt, shorten <=2pt] (9.5,0.5) -- (9.5,0.5) node[right, above, yshift=6pt, pos=0.5, font=\small] {$\cdots$};
    
    % Blindness annotation
    \draw[very thick, red, dashed] (4,0) rectangle (5,1);
    \node[red, below] at (4.5,-0.8) {\small Only sees ONE symbol};
\end{tikzpicture}
\caption{The Turing Machine architecture. The transition function $\delta$ sees only the current state $q$ and the single symbol under the head---it is \textit{structurally blind} to the global tape contents.}
\label{fig:turing_machine}
\end{figure}

\paragraph{Understanding Figure \ref{fig:turing_machine}:}

\textbf{What does this diagram show?} The Turing Machine architecture, emphasizing its fundamental \textbf{blindness}---the machine can only see one symbol at a time.

\textbf{Visual elements:}
\begin{itemize}
    \item \textbf{Infinite tape (bottom):} 9 visible cells containing symbols ($\sqcup$, 0, 1, 1, 0, 1, 0, $\sqcup$, $\sqcup$). Arrows on sides indicate infinite extension ($\cdots$). This is the memory.
    
    \item \textbf{Head (blue arrow):} Points to cell 5 (containing 0). The read/write head can only examine and modify ONE cell per step.
    
    \item \textbf{Control unit (blue box):} Contains the current state $q \in Q$. The finite-state controller decides what to do based on $(q, \gamma)$ where $\gamma$ is the symbol under the head.
    
    \item \textbf{Transition function:} $\delta(q,\gamma) \to (q',\gamma',d)$---maps (state, symbol) to (new state, new symbol, direction L/R).
    
    \item \textbf{Red dashed box (bottom):} Highlights the \textit{only} symbol the machine sees. Labeled "Only sees ONE symbol." This is the visualization of blindness.
\end{itemize}

\textbf{Key insight:} The transition function $\delta$ has no access to the global tape structure. It cannot ask "Is this tape sorted?" or "Does this represent a balanced tree?" without reading and processing the entire tape sequentially. This is \textit{architectural blindness}---a feature of the model's interface, not a weakness of any particular algorithm.

\textbf{Role in thesis:} Motivates the need for the Thiele Machine. Classical computers are blind; the Thiele Machine adds explicit structural perception at a measured cost ($\mu$).

The Turing Machine, introduced by Alan Turing in 1936 \cite{turing1936computable}, is formally defined as a 7-tuple:
\[
M = (Q, \Sigma, \Gamma, \delta, q_0, q_{\text{accept}}, q_{\text{reject}})
\]
where:
\begin{itemize}
    \item $Q$ is a finite set of \textit{states}
    \item $\Sigma$ is the \textit{input alphabet} (not containing the blank symbol $\sqcup$)
    \item $\Gamma$ is the \textit{tape alphabet} where $\Sigma \subset \Gamma$ and $\sqcup \in \Gamma$
    \item $\delta: Q \times \Gamma \to Q \times \Gamma \times \{L, R\}$ is the \textit{transition function}
    \item $q_0 \in Q$ is the \textit{start state}
    \item $q_{\text{accept}} \in Q$ is the \textit{accept state}
    \item $q_{\text{reject}} \in Q$ is the \textit{reject state}, where $q_{\text{accept}} \neq q_{\text{reject}}$
\end{itemize}

The tape is conceptually unbounded in both directions and holds a finite, non-blank region surrounded by blanks. A \textit{configuration} of a Turing Machine is a triple $(q, w, i)$ where $q \in Q$ is the current state, $w \in \Gamma^*$ is the tape contents (with blanks outside the finite non-blank region), and $i \in \mathbb{N}$ is the head position. Each step reads one symbol, writes one symbol, and moves the head one cell left or right. The machine's computation is a sequence of configurations:
\[
C_0 \vdash C_1 \vdash C_2 \vdash \cdots
\]
where $C_0 = (q_0, \sqcup w \sqcup, 1)$ for input $w$ and each transition is determined by $\delta$.

\subsubsection{The Computational Universality Theorem}

Turing proved that there exists a \textit{Universal Turing Machine} $U$ such that for any Turing Machine $M$ and input $w$:
\[
U(\langle M, w \rangle) = M(w)
\]
where $\langle M, w \rangle$ is an encoding of $M$ and $w$. This establishes a formal universality result for Turing Machines and supports the Church-Turing thesis: any mechanically computable function can be computed by a Turing Machine.

\subsubsection{The Blindness Problem}

The transition function $\delta$ is the locus of the blindness problem. Notice that $\delta$ is defined only over local state:
\[
\delta(q, \gamma) \mapsto (q', \gamma', d)
\]
The function receives only:
\begin{enumerate}
    \item The current machine state $q$ (finite, typically small)
    \item The symbol $\gamma$ under the head (a single symbol)
\end{enumerate}

It does \textit{not} receive:
\begin{itemize}
    \item The global contents of the tape
    \item The structure of the encoded data (e.g., that it represents a graph)
    \item The relationships between different parts of the input
\end{itemize}

This is not a limitation that can be overcome by clever programming—it is an \textit{architectural constraint}. The Turing Machine is designed to be local and sequential. Any global property must be discovered through sequential scanning, so structure is accessible only through computation, not as a primitive oracle.

\subsection{The Random Access Machine (RAM)}

The RAM model, introduced to better model real computers, extends the Turing Machine with:
\begin{itemize}
    \item An infinite array of registers $M[0], M[1], M[2], \ldots$
    \item An accumulator register $A$
    \item A program counter $PC$
    \item Instructions: LOAD $i$, STORE $i$, ADD $i$, SUB $i$, JMP $i$, JZ $i$, etc.
\end{itemize}

The key improvement is \textit{random access}: accessing $M[i]$ takes $O(1)$ time regardless of $i$ (on the unit-cost RAM model). This eliminates the $O(n)$ seek time of the Turing Machine tape. In log-cost variants, addressing large indices has a cost proportional to the index length, but the model remains structurally blind either way.

However, the RAM model retains structural blindness. A RAM program can access $M[1000]$ directly, but it cannot know that $M[1000]$--$M[2000]$ encodes a sorted array without executing a verification algorithm. The structure is implicit in programmer knowledge, not explicit in machine architecture.

\subsection{Complexity Classes and the P vs NP Problem}

Classical complexity theory defines:
\begin{itemize}
    \item \textbf{P}: Decision problems solvable by a deterministic Turing Machine in polynomial time
    \item \textbf{NP}: Decision problems where a "yes" instance has a polynomial-length certificate that can be verified in polynomial time
    \item \textbf{NP-Complete}: The hardest problems in NP—all NP problems reduce to them
\end{itemize}

The central open question is whether $\mathbf{P} = \mathbf{NP}$. If $\mathbf{P} \neq \mathbf{NP}$, then there exist problems whose solutions can be \textit{verified} efficiently but not \textit{found} efficiently.

The Thiele Machine perspective reframes this question. Consider an NP-complete problem like 3-SAT. A blind Turing Machine must search the exponential space $\{0,1\}^n$ in the worst case. But suppose the formula has hidden structure—say, it factors into independent sub-formulas. A machine that \textit{perceives} this structure can solve each sub-problem independently. The key point is that \emph{perceiving} the factorization is itself a form of information that must be justified, not an assumption that can be taken for free.

The question becomes: \textit{What is the cost of perceiving the structure?}

I argue that the apparent gap between P and NP is often the gap between:
\begin{itemize}
    \item Machines that have paid for structural insight ($\mu$-bits invested)
    \item Machines that have not (and must pay the Time Tax)
\end{itemize}
In the Thiele Machine, “paying for structural insight” means explicitly constructing partitions and attaching axioms that certify independence or other properties. Those operations are not free: they increase the $\mu$-ledger, which is then provably monotone under the step semantics.

This does not trivialize P vs NP—the structural information may itself be expensive to discover. But it reframes intractability as an \textit{accounting issue} rather than a \textit{fundamental barrier}, emphasizing the cost of certifying structure rather than assuming it for free.

\section{Information Theory and Complexity}

\subsection{Shannon Entropy}

% TikZ Figure: Information Theory Hierarchy
\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=0.85], node distance=2cm]
    % Three columns
    \node[draw, rounded corners, fill=green!10, minimum width=5.4cm, minimum height=3.6cm, align=center, text width=3.5cm] (shannon) at (0,0) {
        \begin{tabular}{c}
        \textbf{Shannon Entropy}\\
        $H(X) = -\sum p(x) \log p(x)$\\
        \small Random variables\\
        \small \textit{Computable}
        \end{tabular}
    };
    
    \node[draw, rounded corners, fill=blue!10, minimum width=5.4cm, minimum height=3.6cm, align=center, text width=3.5cm] (kolmogorov) at (5,0) {
        \begin{tabular}{c}
        \textbf{Kolmogorov}\\
        $K(x) = \min|p|$\\
        \small Individual strings\\
        \small \textit{Uncomputable}
        \end{tabular}
    };
    
    \node[draw, rounded corners, fill=orange!10, minimum width=5.4cm, minimum height=3.6cm, align=center, text width=3.5cm] (mdl) at (10,0) {
        \begin{tabular}{c}
        \textbf{MDL / $\mu$-cost}\\
        $L(H) + L(D|H)$\\
        \small Hypothesis + residual\\
        \small \textit{Computable}
        \end{tabular}
    };
    
    % Arrows
    \draw[very thick, ->, >=stealth, shorten >=2pt, shorten <=2pt] (shannon) -- (kolmogorov) node[pos=0.5, font=\small, above, yshift=6pt] {\small generalizes};
    \draw[very thick, ->, >=stealth, shorten >=2pt, shorten <=2pt] (kolmogorov) -- (mdl) node[pos=0.5, font=\small, above, yshift=6pt] {\small approximates};
    
    % Annotation box
    \node[draw, very thick, red, dashed, rounded corners, align=center, text width=3.5cm] at (10,-2.5) {
        \begin{tabular}{c}
        \textbf{Thiele Machine}\\
        uses MDL-based $\mu$\\
        as operational metric
        \end{tabular}
    };
    \draw[very thick, red, ->, >=stealth, dashed, shorten >=2pt, shorten <=2pt] (10,-1.2) -- (10,-1.8);
\end{tikzpicture}
\caption{The hierarchy of information measures. Shannon entropy applies to distributions, Kolmogorov complexity to individual strings (but is uncomputable), and MDL/$\mu$-cost provides a computable approximation used by the Thiele Machine.}
\label{fig:information_hierarchy}
\end{figure}

\paragraph{Understanding Figure \ref{fig:information_hierarchy}:}

\textbf{What does this diagram show?} The progression from Shannon entropy through Kolmogorov complexity to MDL/$\mu$-cost, showing how information theory evolved and how the Thiele Machine fits.

\textbf{Three columns:}
\begin{itemize}
    \item \textbf{Shannon Entropy (green):} $H(X) = -\sum p(x) \log p(x)$. Applies to random variables (distributions). \textit{Computable}. Foundation of classical information theory (1948).
    
    \item \textbf{Kolmogorov (blue):} $K(x) = \min|p|$ where $p$ is a program generating $x$. Applies to individual strings. \textit{Uncomputable} (halting problem). Theoretical ideal for measuring structure (1960s).
    
    \item \textbf{MDL / $\mu$-cost (orange):} $L(H) + L(D|H)$---hypothesis length + residual. Computable approximation of Kolmogorov complexity. Used in model selection, machine learning.
\end{itemize}

\textbf{Arrows:}
\begin{itemize}
    \item \textbf{Shannon $\to$ Kolmogorov ("generalizes"):} K(x) extends H(X) from distributions to individual strings.
    \item \textbf{Kolmogorov $\to$ MDL ("approximates"):} MDL provides a practical, computable proxy for K(x).
\end{itemize}

\textbf{Red dashed box (bottom):} "Thiele Machine uses MDL-based $\mu$ as operational metric." Arrow points from MDL column. This is where the thesis fits: $\mu$-cost is the Thiele Machine's implementation of MDL for computational structure.

\textbf{Key insight:} We want to measure structure (K(x)), but it's uncomputable. MDL gives us a computable alternative. The Thiele Machine operationalizes MDL as $\mu$-cost, charging for partition structure and axioms based on their description length.

\textbf{Role in thesis:} Establishes the information-theoretic foundation for $\mu$-cost. It's not arbitrary---it's grounded in 75 years of information theory.

Claude Shannon's 1948 paper "A Mathematical Theory of Communication" established information as a quantifiable resource \cite{shannon1948mathematical}. The basic unit is \emph{self-information}: an event with probability $p$ carries surprise $I = -\log_2 p$ bits, because rare events convey more information than common ones. The \textit{entropy} of a discrete random variable $X$ with probability mass function $p$ is the expected surprise:
\[
H(X) = -\sum_{x \in \mathcal{X}} p(x) \log_2 p(x)
\]

Shannon entropy measures the \textit{uncertainty} in a random variable, or equivalently, the expected number of bits needed to encode an outcome under an optimal prefix-free code. The coding interpretation follows from Kraft's inequality: assigning code lengths $\ell(x)$ with $\sum 2^{-\ell(x)} \le 1$ yields an expected length minimized (up to 1 bit) by $\ell(x) \approx -\log_2 p(x)$. Key properties:
\begin{itemize}
    \item $H(X) \ge 0$ with equality iff $X$ is deterministic
    \item $H(X) \le \log_2 |\mathcal{X}|$ with equality iff $X$ is uniform
    \item $H(X, Y) \le H(X) + H(Y)$ with equality iff $X \perp Y$ (independence)
\end{itemize}

The last property is crucial for the Thiele Machine: knowing that two variables are independent allows me to decompose the joint entropy into independent components, potentially enabling exponential speedups. Independence is itself a structural assertion that must be paid for in the Thiele Machine model.
This is exactly why the formal model treats independence as a partition of state: the only way to claim $H(X, Y) = H(X) + H(Y)$ is to introduce a partition that separates the variables into different modules, which the model charges via $\mu$.

\subsubsection{Entropy, Models, and What Is Actually Random}

Shannon entropy is a property of a \emph{distribution}, not of the underlying world. When I model a system with a random variable, I am quantifying my uncertainty and compressibility, not asserting that nature is literally rolling dice. A weather simulator, for example, may use Monte Carlo sampling or stochastic parameterizations to represent unresolved turbulence. The atmosphere itself is not sampling random numbers; the randomness is in my \emph{model} of an overwhelmingly complex, chaotic system. In other words, stochasticity is often epistemic: it reflects limited knowledge and coarse-grained descriptions rather than intrinsic indeterminism.

This distinction matters for the Thiele Machine because it highlights where "structure" lives. A partition that lets me treat two subsystems as independent is not a free fact about reality; it is an explicit modeling choice that I must justify and pay for. The entropy ledger charges me for the compressed description I claim to possess, not for any metaphysical randomness in the world.

\subsection{Kolmogorov Complexity}

While Shannon entropy applies to random variables, \textit{Kolmogorov complexity} measures the structural content of individual strings. For a string $x$:
\[
K(x) = \min \{|p| : U(p) = x\}
\]
where $U$ is a universal Turing Machine and $|p|$ is the bit-length of program $p$.

Kolmogorov complexity captures the intuition that a string like "010101010101..." (alternating) has low complexity (a short program can generate it), while a random string has high complexity (no program substantially shorter than the string itself can produce it).

Key theorems:
\begin{itemize}
    \item \textbf{Invariance Theorem}: $K_U(x) = K_{U'}(x) + O(1)$ for any two universal machines $U, U'$
    \item \textbf{Incompressibility}: For any $n$, there exists a string $x$ of length $n$ with $K(x) \ge n$
    \item \textbf{Uncomputability}: $K(x)$ is not computable (by reduction from the halting problem)
\end{itemize}

The uncomputability of Kolmogorov complexity is why the Thiele Machine uses \textit{Minimum Description Length} (MDL) instead—a computable approximation that captures description length without requiring the impossible oracle.

\subsubsection{Comparison with $\mu$-bits}

It is important to distinguish the theoretical $K(x)$ from the operational $\mu$-bit cost. While Kolmogorov complexity represents the ultimate lower bound on description length using an optimal universal machine, the $\mu$-bit cost is a concrete, computable metric based on the specific structural assertions made by the Thiele Machine.
\begin{itemize}
    \item $K(x)$ is uncomputable and depends on the choice of universal machine (up to a constant).
    \item $\mu$-cost is computable and depends on the specific partition logic operations and axioms used.
\end{itemize}
Thus, $\mu$ serves as a constructive upper bound on the structural complexity, representing the cost of the structure \textit{actually used} by the algorithm, rather than the theoretical minimum. This makes $\mu$ a practical resource for complexity analysis in a way that $K(x)$ cannot be.

In the implementation, the proxy is not a magical compressor; it is a canonical string encoding of axioms and partitions (SMT-LIB strings plus region encodings), so the cost is defined in a way that can be checked by the formal kernel and reproduced by the other layers.

\subsection{Minimum Description Length (MDL)}

The MDL principle, developed by Jorma Rissanen \cite{rissanen1978modeling}, provides a computable proxy for Kolmogorov complexity. Given a hypothesis class $\mathcal{H}$ and data $D$, the MDL cost is:
\[
L(D) = \min_{H \in \mathcal{H}} \{L(H) + L(D|H)\}
\]
where:
\begin{itemize}
    \item $L(H)$ is the description length of hypothesis $H$
    \item $L(D|H)$ is the description length of $D$ given $H$ (the "residual")
\end{itemize}

In the Thiele Machine, I adopt MDL as the basis for $\mu$-cost:
\begin{itemize}
    \item The "hypothesis" is the partition structure $\pi$
    \item $L(\pi)$ is the $\mu$-cost of specifying the partition
    \item $L(\text{computation}|\pi)$ is the operational cost given the structure
\end{itemize}

The total $\mu$-cost is thus analogous to the MDL of the computation, with the partition description and its axioms charged explicitly as a model of structure. This separates the cost of \emph{describing} structure from the cost of \emph{using} it.
This is reflected directly in the Python and Coq implementations: the $\mu$-ledger is updated by explicit per-instruction costs, and structural operations (like partition creation or split) carry their own explicit charges.

\section{The Physics of Computation}

\subsection{Landauer's Principle}

% TikZ Figure: Landauer's Principle and Maxwell's Demon
\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=0.85], node distance=3cm]
    % Left: Landauer's Principle
    \begin{scope}[xshift=-5cm]
        % Two-to-one mapping
        \node[draw, circle, fill=blue!20] (a) at (0,1) {0};
        \node[draw, circle, fill=blue!20] (b) at (0,-1) {1};
        \node[draw, circle, fill=red!20] (c) at (3,0) {0};
        
        \draw[very thick, ->, >=stealth, shorten >=2pt, shorten <=2pt] (a) -- (c);
        \draw[very thick, ->, >=stealth, shorten >=2pt, shorten <=2pt] (b) -- (c);
        
        % Heat release
        \draw[very thick, red, ->, shorten >=2pt, shorten <=2pt] (3.5,0) -- (5,0);
        \node[red] at (5.5,0) {$Q$};
        
        % Equation
        \node at (2.5,-2) {$Q \ge k_B T \ln 2$};
        \node[below] at (2.5,-2.8) {\small \textit{Erasure releases heat}};
        
        \node[above] at (1.5,2) {\textbf{Landauer's Principle}};
    \end{scope}
    
    % Right: Maxwell's Demon
    \begin{scope}[xshift=5cm]
        % Container
        \draw[very thick] (-2,-1.5) rectangle (2,1.5);
        \draw[very thick, shorten >=2pt, shorten <=2pt] (0,-1.5) -- (0,-0.3);
        \draw[very thick, shorten >=2pt, shorten <=2pt] (0,0.3) -- (0,1.5);
        
        % Door
        \draw[very thick, blue, fill=blue!30] (-0.1,-0.3) rectangle (0.1,0.3);
        
        % Demon
        \node[draw, circle, fill=green!30, minimum size=0.8cm] (demon) at (0,2) {D};
        \draw[very thick, ->, >=stealth, shorten >=2pt, shorten <=2pt] (demon) -- (0,0.5);
        
        % Molecules (fast = red, slow = blue)
        \node[fill=red, circle, inner sep=2pt] at (-1.2,0.5) {};
        \node[fill=red, circle, inner sep=2pt] at (-0.8,-0.8) {};
        \node[fill=blue, circle, inner sep=2pt] at (-1.5,-0.3) {};
        \node[fill=blue, circle, inner sep=2pt] at (1.2,0.7) {};
        \node[fill=blue, circle, inner sep=2pt] at (0.7,-0.5) {};
        \node[fill=red, circle, inner sep=2pt] at (1.5,0) {};
        
        % Labels
        \node[below] at (-1,-2) {\small Hot};
        \node[below] at (1,-2) {\small Cold};
        
        \node[above] at (0,2.8) {\textbf{Maxwell's Demon}};
        \node[below] at (0,-2.8) {\small \textit{Information costs entropy}};
    \end{scope}
\end{tikzpicture}
\caption{Left: Landauer's principle---erasing one bit releases at least $k_B T \ln 2$ joules of heat. Right: Maxwell's demon appears to violate the second law, but the demon must pay for information acquisition and storage.}
\label{fig:landauer_demon}
\end{figure}

\paragraph{Understanding Figure \ref{fig:landauer_demon}:}

\textbf{Left: Landauer's Principle}
\begin{itemize}
    \item \textbf{Two blue circles (top):} Initial states 0 and 1.
    \item \textbf{One red circle (right):} Final state 0. This is a many-to-one mapping (erasure).
    \item \textbf{Arrows:} Both 0 and 1 map to 0.
    \item \textbf{Red arrow labeled $Q$:} Heat dissipation. Erasure releases energy.
    \item \textbf{Equation below:} $Q \ge k_B T \ln 2$---minimum heat released per bit erased. At room temperature: $\sim 3 \times 10^{-21}$ joules.
\end{itemize}

\textbf{Right: Maxwell's Demon}
\begin{itemize}
    \item \textbf{Container with partition:} Left and right chambers separated by a door (blue rectangle in center).
    \item \textbf{Demon (green circle, top):} Observes molecules, opens door selectively.
    \item \textbf{Molecules:} Red = fast (hot), blue = slow (cold). Initially mixed.
    \item \textbf{Strategy:} Demon opens door for fast molecules moving right, slow molecules moving left. Eventually: hot right, cold left---apparent entropy reduction without work.
    \item \textbf{Resolution:} Demon must pay for information: measuring velocities requires physical interaction, storing decisions requires memory, erasing memory releases heat (Landauer). Total entropy increases.
\end{itemize}

\textbf{Key insight:} Information is physical. You cannot reduce entropy (knowledge) without paying a thermodynamic cost. The demon's "free insight" is paid for by Landauer erasure when memory fills.

\textbf{Connection to Thiele Machine:} The $\mu$-ledger is the informational analog of thermodynamic entropy. Just as physical systems cannot decrease entropy without work, the Thiele Machine cannot decrease search space without paying $\mu$. The No Free Insight theorem is the computational version of the Second Law.

\textbf{Role in thesis:} Establishes the physical grounding for $\mu$-accounting. It's not just an abstract cost---it has thermodynamic justification.

In 1961, Rolf Landauer proved a fundamental connection between information and thermodynamics \cite{landauer1961irreversibility}:

\begin{theorem}[Landauer's Principle]
The erasure of one bit of information in a computing device releases at least $k_B T \ln 2$ joules of heat into the environment.
\end{theorem}

Here $k_B$ is Boltzmann's constant and $T$ is the absolute temperature. At room temperature (300K), this is approximately $3 \times 10^{-21}$ joules per bit—a tiny amount, but fundamentally non-zero.

Landauer's principle establishes that:
\begin{enumerate}
    \item \textbf{Information is physical}: It cannot be erased without physical consequences
    \item \textbf{Irreversibility has a cost}: Logically irreversible operations (many-to-one maps such as AND, OR, erasure) dissipate heat
    \item \textbf{Computation is thermodynamic}: The ultimate limits of computation are set by thermodynamics
\end{enumerate}

From a first-principles perspective, the key step is that erasure reduces the logical state space. Mapping two possible inputs to a single output decreases the system's entropy by $\Delta S = k_B \ln 2$. To satisfy the second law, that entropy must be exported to the environment as heat $Q \ge T \Delta S$, yielding the $k_B T \ln 2$ bound. Reversible gates avoid this penalty by preserving a one-to-one mapping between logical states, but they shift the cost to auxiliary memory and garbage bits that must eventually be erased.

\subsubsection{Reversible Computation}

Charles Bennett showed that computation can be made thermodynamically reversible by keeping a history of all operations \cite{bennett1982thermodynamics}. A reversible Turing Machine can simulate any irreversible computation with only polynomial overhead in space (and at most polynomial overhead in time, depending on the simulation strategy).

However, reversible computation has its own cost: the space required to store the history. This is another form of "structural debt"—you can avoid the heat cost by paying a space cost.

\subsubsection{Simulation Versus Physical Reality}

It is tempting to say "if I can simulate it, I have reproduced it," but physics makes that statement precise: a simulation manipulates \emph{symbols} that represent a system, while the system itself evolves under physical laws. A climate model can produce temperature fields, hurricanes, or droughts on a screen, yet it does not warm the room or generate real rainfall. The computation is physical---it dissipates heat, uses energy, and has real thermodynamic cost---but the simulated climate is an informational artifact, not a new atmosphere.

This matters because any claim about "cost" depends on the level of description. A Monte Carlo weather model may treat unresolved convection as a random process, but the real atmosphere is not a Monte Carlo chain; it is a high-dimensional deterministic (or quantum-to-classical) system whose unpredictability is amplified by chaos. When I trade the real dynamics for a stochastic approximation, I am asserting a structural model that saves compute at the price of fidelity. The Thiele Machine makes that trade explicit: the cost of declaring independence, randomness, or coarse-grained behavior must be booked in $\mu$-bits.

\subsubsection{Renormalization and Coarse-Grained Structure}

Renormalization is a formal way to justify this kind of model compression. In statistical physics and quantum field theory, I group microscopic degrees of freedom into blocks, integrate out short-scale details, and obtain an effective theory at a larger scale. This is a principled, repeatable way of asserting structure: I discard information about microstates but gain predictive power at the macro level. The price is an explicit approximation error and new effective parameters.

From the Thiele Machine perspective, renormalization is a structured partition of state space. I am committing to a hierarchy of equivalence classes that summarize behavior at each scale. The $\mu$-ledger charges for these commitments, making the bookkeeping of coarse-grained structure as explicit as the bookkeeping of energy.

\subsection{Maxwell's Demon and Szilard's Engine}

The thought experiment of "Maxwell's Demon" illustrates the thermodynamic nature of information:

Imagine a container divided by a partition with a door. A "demon" observes molecules and opens the door only when a fast molecule approaches from the left. Over time, fast molecules accumulate on the right, creating a temperature differential without apparent work.

Leo Szilard's 1929 analysis \cite{szilard1929entropieverminderung} and later work by Bennett showed that the demon must pay for its information:
\begin{enumerate}
    \item \textbf{Acquiring information}: Measuring molecular velocities requires physical interaction
    \item \textbf{Storing information}: The demon's memory has finite capacity
    \item \textbf{Erasing information}: When memory fills, erasure releases heat (Landauer)
\end{enumerate}

The total entropy balance is preserved: the demon's information processing exactly compensates for the apparent entropy reduction.

\subsection{Connection to the Thiele Machine}

% TikZ Figure: The μ-Thermodynamic Bridge
\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=0.85], node distance=3cm]
    % Three columns: Abstract, Bridge, Physical
    \node[draw, very thick, rounded corners, fill=blue!15, minimum width=5.4cm, minimum height=7.2cm] (abstract) at (0,0) {};
    \node[above] at (0,2.3) {\textbf{Abstract (Model)}};
    \node at (0,1.2) {$\mu$-ledger};
    \node at (0,0.4) {Partitions $\Pi$};
    \node at (0,-0.4) {Axioms $\mathcal{A}$};
    \node at (0,-1.2) {Revelation ops};
    
    \node[draw, very thick, rounded corners, fill=green!15, minimum width=5.4cm, minimum height=7.2cm] (bridge) at (5,0) {};
    \node[above] at (5,2.3) {\textbf{Bridge}};
    \node at (5,1.2) {$\mu \propto$ entropy};
    \node at (5,0.4) {$= k_B T \ln 2$};
    \node at (5,-0.4) {per bit};
    \node at (5,-1.2) {(Landauer)};
    
    \node[draw, very thick, rounded corners, fill=red!15, minimum width=5.4cm, minimum height=7.2cm] (physical) at (10,0) {};
    \node[above] at (10,2.3) {\textbf{Physical}};
    \node at (10,1.2) {Heat dissipation};
    \node at (10,0.4) {Thermodynamics};
    \node at (10,-0.4) {Second Law};
    \node at (10,-1.2) {Irreversibility};
    
    % Arrows
    \draw[very thick, <->, >=stealth, shorten >=2pt, shorten <=2pt] (1.8,0) -- (3.2,0) node[pos=0.5, font=\small, above, yshift=6pt] {\small maps to};
    \draw[very thick, <->, >=stealth, shorten >=2pt, shorten <=2pt] (6.8,0) -- (8.2,0) node[pos=0.5, font=\small, above, yshift=6pt] {\small maps to};
    
    % Key insight
    \node[draw, very thick, dashed, rounded corners, fill=yellow!20, align=center, text width=3.5cm] at (5,-3.5) {
        \begin{tabular}{c}
        \textbf{Key Insight:} Asserting structure $\approx$ Erasing alternatives\\
        $\mu$-ledger monotonicity $\Leftrightarrow$ Second Law of Thermodynamics
        \end{tabular}
    };
\end{tikzpicture}
\caption{The conceptual bridge between the Thiele Machine's abstract $\mu$-accounting and physical thermodynamics. The monotonicity of the $\mu$-ledger is the computational analog of the Second Law.}
\label{fig:mu_thermodynamic_bridge}
\end{figure}

\paragraph{Understanding Figure \ref{fig:mu_thermodynamic_bridge}:}

\textbf{What does this diagram show?} The conceptual mapping from abstract computational structure to physical thermodynamics, via Landauer's principle.

\textbf{Three columns:}
\begin{itemize}
    \item \textbf{Abstract (Model, blue):} Left column. Contains: $\mu$-ledger, Partitions $\Pi$, Axioms $\mathcal{A}$, Revelation ops. This is the Thiele Machine's abstract computational model.
    
    \item \textbf{Bridge (green):} Center column. Shows the mapping: $\mu \propto$ entropy, $= k_B T \ln 2$ per bit (Landauer). This is the \textit{bridge} connecting abstract and physical.
    
    \item \textbf{Physical (red):} Right column. Contains: Heat dissipation, Thermodynamics, Second Law, Irreversibility. This is the physical reality.
\end{itemize}

\textbf{Arrows:}
\begin{itemize}
    \item \textbf{Abstract $\leftrightarrow$ Bridge:} "maps to"
    \item \textbf{Bridge $\leftrightarrow$ Physical:} "maps to"
\end{itemize}

\textbf{Yellow box (bottom):} Key insight: "Asserting structure $\approx$ Erasing alternatives. $\mu$-ledger monotonicity $\Leftrightarrow$ Second Law of Thermodynamics."

\textbf{Conceptual mapping:}
\begin{itemize}
    \item Asserting structure (e.g., "variables are independent") is like erasing alternatives ("they could be dependent").
    \item The $\mu$-ledger's monotonicity (never decreases) is analogous to the Second Law (entropy never decreases in closed systems).
    \item Just as thermodynamic entropy tracks irreversible processes, $\mu$ tracks irreversible structural commitments.
\end{itemize}

\textbf{Role in thesis:} Provides the deep justification for $\mu$-monotonicity. It's not an arbitrary design choice---it's the computational analog of a fundamental law of physics.

The Thiele Machine generalizes Landauer's principle from \textit{erasure} to \textit{structure}. Just as erasing information has a thermodynamic cost, \textit{asserting structure} has an information-theoretic cost:

\begin{quote}
    If erasing information costs $k_B T \ln 2$ joules per bit, then asserting that "this formula decomposes into $k$ independent parts" costs proportional $\mu$-bits of structural specification.
\end{quote}

The $\mu$-ledger is the computational analog of the thermodynamic entropy: a monotonically increasing quantity that tracks the irreversible commitments of the computation. The analogy is not that $\mu$ is a physical entropy, but that both act as bookkeepers for irreversible choices.

\section{Quantum Computing and Correlations}

\subsection{Bell's Theorem and Non-Locality}

% TikZ Figure: CHSH Inequality and Correlation Bounds
\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=0.85], node distance=2.5cm]
    % Alice and Bob boxes
    \node[draw, very thick, rounded corners, fill=blue!10, minimum width=4.0cm, minimum height=2.6cm, align=center, text width=3.5cm] (alice) at (-4,0) {
        \begin{tabular}{c}
        \textbf{Alice}\\
        $x \in \{0,1\}$\\
        $a \in \{0,1\}$
        \end{tabular}
    };
    
    \node[draw, very thick, rounded corners, fill=green!10, minimum width=4.0cm, minimum height=2.6cm, align=center, text width=3.5cm] (bob) at (4,0) {
        \begin{tabular}{c}
        \textbf{Bob}\\
        $y \in \{0,1\}$\\
        $b \in \{0,1\}$
        \end{tabular}
    };
    
    % Entangled source
    \node[draw, very thick, rounded corners, fill=red!20, minimum width=2.6cm] (source) at (0,0) {Source};
    \draw[very thick, red, decorate, decoration={snake, amplitude=2pt}, shorten >=2pt, shorten <=2pt] (source) -- (alice);
    \draw[very thick, red, decorate, decoration={snake, amplitude=2pt}, shorten >=2pt, shorten <=2pt] (source) -- (bob);
    
    % CHSH value scale
    \begin{scope}[yshift=-3cm]
        \draw[very thick, ->, shorten >=2pt, shorten <=2pt] (-5,0) -- (5,0) node[right, above, yshift=6pt, pos=0.5, font=\small] {$S$};
        
        % Tick marks
        \draw[very thick, shorten >=2pt, shorten <=2pt] (-4,0.1) -- (-4,-0.1) node[below, above, yshift=6pt, pos=0.5, font=\small] {$-4$};
        \draw[very thick, shorten >=2pt, shorten <=2pt] (-2,0.1) -- (-2,-0.1) node[below, above, yshift=6pt, pos=0.5, font=\small] {$-2$};
        \draw[very thick, shorten >=2pt, shorten <=2pt] (0,0.1) -- (0,-0.1) node[below, above, yshift=6pt, pos=0.5, font=\small] {$0$};
        \draw[very thick, shorten >=2pt, shorten <=2pt] (2,0.1) -- (2,-0.1) node[below, above, yshift=6pt, pos=0.5, font=\small] {$2$};
        \draw[very thick, shorten >=2pt, shorten <=2pt] (2.83,0.1) -- (2.83,-0.1) node[below, above, yshift=6pt, pos=0.5, font=\small] {$2\sqrt{2}$};
        \draw[very thick, shorten >=2pt, shorten <=2pt] (4,0.1) -- (4,-0.1) node[below, above, yshift=6pt, pos=0.5, font=\small] {$4$};
        
        % Regions
        \fill[green!30, opacity=0.5] (-2,0.3) rectangle (2,0.8);
        \node[green!50!black] at (0,0.55) {\small Classical};
        
        \fill[blue!30, opacity=0.5] (2,0.3) rectangle (2.83,0.8);
        \node[blue] at (2.4,0.55) {\scriptsize Q};
        
        \fill[red!30, opacity=0.5] (2.83,0.3) rectangle (4,0.8);
        \node[red!70!black] at (3.4,0.55) {\small Supra-Q};
        
        % Bounds
        \draw[very thick, green!50!black, dashed, shorten >=2pt, shorten <=2pt] (2,0) -- (2,1);
        \draw[very thick, blue, dashed, shorten >=2pt, shorten <=2pt] (2.83,0) -- (2.83,1);
        \draw[very thick, red, dashed, shorten >=2pt, shorten <=2pt] (4,0) -- (4,1);
    \end{scope}
    
    % CHSH formula
    \node at (0,-5.5) {$S = E(0,0) + E(0,1) + E(1,0) - E(1,1)$};
\end{tikzpicture}
\caption{The Bell-CHSH experiment. Alice and Bob share an entangled state from a source. The CHSH value $S$ is bounded by 2 for classical (local hidden variable) theories, $2\sqrt{2}$ for quantum mechanics, and 4 algebraically (proven from first principles in \texttt{coq/kernel/Tier1Proofs.v} with zero axioms). The Thiele Machine proves $\mu=0 \Rightarrow S \le 4$ (algebraic bound); Tsirelson requires algebraic coherence.}
\label{fig:chsh_bounds}
\end{figure}

\paragraph{Understanding Figure \ref{fig:chsh_bounds}:}

\textbf{Top: Experimental setup}
\begin{itemize}
    \item \textbf{Alice (blue box, left):} Receives input $x \in \{0,1\}$, produces output $a \in \{0,1\}$.
    \item \textbf{Bob (green box, right):} Receives input $y \in \{0,1\}$, produces output $b \in \{0,1\}$.
    \item \textbf{Source (red box, center):} Produces entangled state, sends to Alice and Bob (wavy red lines). Spatially separated (no communication during measurement).
\end{itemize}

\textbf{Bottom: CHSH value scale}
\begin{itemize}
    \item \textbf{Horizontal axis:} CHSH value $S$ ranging from $-4$ to $4$.
    
    \item \textbf{Classical region (green, $|S| \le 2$):} Local hidden variable theories cannot exceed $S=2$. This is Bell's theorem: any classical (realistic, local) model is bounded by 2.
    
    \item \textbf{Quantum region (blue, $2 < |S| \le 2\sqrt{2}$):} Quantum mechanics allows $S$ up to $2\sqrt{2} \approx 2.828$ (Tsirelson's bound, 1980). Quantum entanglement enables correlations exceeding classical limits.
    
    \item \textbf{Supra-quantum region (red, $2\sqrt{2} < |S| \le 4$):} Algebraically possible but not realized by quantum mechanics. The bound $|S| \le 4$ is \textit{proven} from pure probability theory (Theorem T1-2: \texttt{valid\_box\_S\_le\_4}, verified with zero axioms). Why does nature stop at $2\sqrt{2}$? This is the mystery the thesis addresses.

    \item \textbf{Vertical dashed lines:} Mark boundaries at $S=2$ (classical), $S=2\sqrt{2}$ (Tsirelson), $S=4$ (algebraic maximum, proven).
\end{itemize}

\textbf{Formula (bottom):} $S = E(0,0) + E(0,1) + E(1,0) - E(1,1)$ where $E(x,y) = \mathbb{E}[(-1)^{a \oplus b} \mid x,y]$ is the correlation for input pair $(x,y)$.

\textbf{Key insight:} Quantum mechanics permits correlations up to $2\sqrt{2}$ but no higher. The algebraic maximum of 4 is proven from first principles (Theorem T1-2, correlation bound Theorem T1-1), establishing the absolute ceiling for any theory.

\textbf{CORRECTION} (December 2025, per \texttt{TsirelsonUniqueness.v}): The Thiele Machine proves that $\mu=0$ implies $S \le 4$ (algebraic bound), \textbf{not} $S \le 2\sqrt{2}$. The Tsirelson bound requires \textit{algebraic coherence} (NPA level 1 constraint on correlations), which is a property of the correlations themselves, not just the instructions. There exist $\mu=0$ traces with $S > 2\sqrt{2}$. Thus, $\mu$-accounting alone does not explain Tsirelson's bound---it requires additional structure on the correlation space.

\textbf{Role in thesis:} Central experimental prediction. The CHSH game is used throughout to validate the Thiele Machine's correlation accounting. Experimental results (Chapter 11) show 85.3\% win rate, matching $2\sqrt{2}$ within error.

In 1964, John Bell proved that no "local hidden variable" theory can reproduce all predictions of quantum mechanics \cite{bell1964einstein}. The key insight is the CHSH inequality:

Consider two spatially separated parties, Alice and Bob, who share an entangled quantum state. Each performs one of two measurements ($x, y \in \{0, 1\}$) and obtains one of two outcomes ($a, b \in \{0, 1\}$). Define:
\[
S = E(0,0) + E(0,1) + E(1,0) - E(1,1)
\]
where $E(x,y) = \Pr[a = b | x, y] - \Pr[a \neq b | x, y] = \mathbb{E}[(-1)^{a \oplus b} \mid x,y]$.

Bell proved:
\begin{itemize}
    \item \textbf{Local Realistic Bound}: $|S| \le 2$
    \item \textbf{Quantum Bound (Tsirelson)}: $|S| \le 2\sqrt{2} \approx 2.828$
    \item \textbf{Algebraic Bound}: $|S| \le 4$
\end{itemize}

The CHSH form was later refined for experimental tests \cite{clauser1969proposed}. If Alice and Bob's outcomes are determined by a shared hidden variable $\lambda$ and local response functions $A_x(\lambda), B_y(\lambda) \in \{-1,+1\}$, then
\[
S = \mathbb{E}_\lambda[A_0 B_0 + A_0 B_1 + A_1 B_0 - A_1 B_1]
\]
and each term is $\pm 1$, so the absolute value of the sum is at most $2$ for any deterministic strategy; convex combinations (probabilistic mixtures) cannot exceed this bound. Quantum mechanics allows $S > 2$ by using entangled states and non-commuting measurements, and Tsirelson showed the tight quantum limit is $2\sqrt{2}$ \cite{tsirelson1980quantum}. This violation is the operational signature that no local hidden-variable model can reproduce all quantum correlations.

\subsection{Decoherence, Measurement, and Informational Cost}

Quantum correlations are fragile because measurement is a physical interaction. Decoherence occurs when a quantum system becomes entangled with an uncontrolled environment, effectively "measuring" it and suppressing interference. The act of extracting a classical record is not a cost-free epistemic update; it is a physical process that dumps phase information into the environment. In this sense, gaining a classical bit of knowledge about a quantum system is analogous to Landauer's principle: it requires a thermodynamic footprint somewhere in the larger system.

This perspective ties directly to the Thiele Machine's revelation rule. When the machine asserts a supra-quantum certification, it must emit an explicit revelation-class instruction, because the correlation is not just a mathematical artifact---it is a structural claim that needs a physical bookkeeping event. The model mirrors the physics: information is not free, whether it is classical or quantum.

\subsection{The Revelation Requirement}

In the Thiele Machine framework, I prove that:

\begin{theorem}[Revelation Requirement]
If a Thiele Machine execution produces a state with "supra-quantum" certification (a nonzero certification flag in a control/status register, starting from 0), then the execution trace must contain an explicit revelation-class instruction (\texttt{REVEAL}, \texttt{EMIT}, \texttt{LJOIN}, or \texttt{LASSERT}).
\end{theorem}

In other words, you cannot certify non-local correlations without explicitly paying the structural cost. This is a model-specific theorem, included here to motivate later chapters.

\section{Formal Verification}

\subsection{The Coq Proof Assistant}

% TikZ Figure: Coq Verification Workflow
\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=0.85], node distance=2.5cm]
    % Coq workflow boxes
    \node[draw, very thick, rounded corners, fill=blue!10, minimum width=4.6cm, minimum height=1.8cm, align=center, text width=3.5cm] (spec) at (0,0) {
        \begin{tabular}{c}
        \textbf{Specification}\\
        \small Definitions
        \end{tabular}
    };
    
    \node[draw, very thick, rounded corners, fill=green!10, minimum width=4.6cm, minimum height=1.8cm, align=center, text width=3.5cm] (theorem) at (4,0) {
        \begin{tabular}{c}
        \textbf{Theorem}\\
        \small Statement
        \end{tabular}
    };
    
    \node[draw, very thick, rounded corners, fill=orange!10, minimum width=4.6cm, minimum height=1.8cm, align=center, text width=3.5cm] (proof) at (8,0) {
        \begin{tabular}{c}
        \textbf{Proof}\\
        \small Tactics
        \end{tabular}
    };
    
    \node[draw, very thick, rounded corners, fill=red!10, minimum width=4.6cm, minimum height=1.8cm, align=center, text width=3.5cm] (qed) at (12,0) {
        \begin{tabular}{c}
        \textbf{Qed}\\
        \small Verified!
        \end{tabular}
    };
    
    % Arrows
    \draw[very thick, ->, >=stealth, shorten >=2pt, shorten <=2pt] (spec) -- (theorem);
    \draw[very thick, ->, >=stealth, shorten >=2pt, shorten <=2pt] (theorem) -- (proof);
    \draw[very thick, ->, >=stealth, shorten >=2pt, shorten <=2pt] (proof) -- (qed);
    
    % Curry-Howard
    \node[draw, dashed, very thick, purple, rounded corners, align=center, text width=3.5cm] at (6,-2) {
        \begin{tabular}{c}
        \textbf{Curry-Howard Correspondence}\\
        Propositions $\equiv$ Types\\
        Proofs $\equiv$ Programs
        \end{tabular}
    };
    \draw[very thick, purple, ->, >=stealth, dashed, shorten >=2pt, shorten <=2pt] (6,-0.8) -- (6,-1.3);
    
    % Inquisitor Standard
    \begin{scope}[yshift=-4cm]
        \node[draw, very thick, fill=red!20, rounded corners, minimum width=18.0cm, align=center, text width=3.5cm] at (6,0) {
            \begin{tabular}{c}
            \textbf{Inquisitor Standard} (enforced automatically)\\
            \texttimes\ No \texttt{Admitted} \quad \texttimes\ No \texttt{admit} \quad \texttimes\ No \texttt{Axiom}
            \end{tabular}
        };
    \end{scope}
\end{tikzpicture}
\caption{The Coq verification workflow. Specifications lead to theorem statements, which are proven using tactics. The Curry-Howard correspondence ensures proofs are programs. The Thiele Machine enforces the Inquisitor Standard: no admitted lemmas, no axioms.}
\label{fig:coq_workflow}
\end{figure}

\paragraph{Understanding Figure \ref{fig:coq_workflow}:}

\textbf{Top: Coq workflow (4 stages):}
\begin{itemize}
    \item \textbf{Specification (blue):} Define data structures, functions, predicates. Example: \texttt{Inductive State}, \texttt{Fixpoint mu\_step}.
    
    \item \textbf{Theorem (green):} State the claim to prove. Example: \texttt{Theorem mu\_monotonic : forall s s', step s s' -> mu s <= mu s'}.
    
    \item \textbf{Proof (orange):} Construct proof using tactics. Example: \texttt{intros. induction s. simpl. omega.} Coq checks that tactics produce a valid proof term.
    
    \item \textbf{Qed (red):} Proof complete! Coq has verified the theorem. This is machine-checked---no possibility of hidden gaps.
\end{itemize}

\textbf{Middle: Curry-Howard Correspondence (purple dashed box):}
\begin{itemize}
    \item \textbf{Propositions $\equiv$ Types:} A theorem is a type. Example: \texttt{forall x, P x} is the type of functions from $x$ to proofs of $P(x)$.
    \item \textbf{Proofs $\equiv$ Programs:} A proof is a program inhabiting that type. Coq's type checker verifies correctness.
    \item This is the foundation of Coq: logic and computation are unified.
\end{itemize}

\textbf{Bottom: Inquisitor Standard (red box):}
\begin{itemize}
    \item \texttimes\ No \texttt{Admitted}: Every lemma must be fully proven. No "TODO" proofs.
    \item \texttimes\ No \texttt{admit}: No tactical shortcuts inside proofs.
    \item \texttimes\ No \texttt{Axiom}: No unproven assumptions (except foundational logic axioms from Coq's standard library).
    \item This standard is \textbf{enforced automatically} by CI pipeline scanning all .v files.
\end{itemize}

\textbf{Key insight:} Coq ensures \textit{absolute certainty}. If a theorem is Qed'd under the Inquisitor Standard, it is \textit{provably true}---no informal gaps, no hidden assumptions.

\textbf{Role in thesis:} Establishes the verification methodology. All 1400+ theorems in the thesis are Coq-verified under this standard. This is not a typical "informal proof" thesis---it's mechanically checked mathematics.

Coq is an interactive theorem prover based on the Calculus of Inductive Constructions (CIC). It provides:
\begin{itemize}
    \item \textbf{Dependent types}: Types can depend on values
    \item \textbf{Inductive definitions}: Data types and predicates defined by construction rules
    \item \textbf{Proof terms}: Proofs are first-class objects that can be type-checked
    \item \textbf{Extraction}: Proofs can be extracted to executable code (OCaml, Haskell)
\end{itemize}

A Coq development consists of:
\begin{itemize}
    \item \textbf{Definitions}: \texttt{Definition}, \texttt{Fixpoint}, \texttt{Inductive}
    \item \textbf{Lemmas/Theorems}: Statements to prove
    \item \textbf{Proofs}: Sequences of tactics that construct proof terms
\end{itemize}

\subsubsection{The Curry-Howard Correspondence}

Coq embodies the Curry-Howard correspondence: propositions are types, and proofs are programs. A proof of "A implies B" is a function from evidence of A to evidence of B:
\[
\text{Proof of } (A \to B) \equiv \text{Function } f: A \to B
\]

This means that a verified Coq development is not just a logical argument—it is executable code that demonstrates the truth of the proposition.

\subsection{The Inquisitor Standard}

For the Thiele Machine, I adopt a strict methodology called the "Inquisitor Standard":

\begin{enumerate}
    \item \textbf{No \texttt{Admitted}}: Every lemma must be fully proven
    \item \textbf{No \texttt{admit} tactics}: No tactical shortcuts inside proofs
    \item \textbf{No \texttt{Axiom}}: No unproven assumptions except foundational logic
\end{enumerate}

This standard is enforced by an automated checker that scans all proof files and reports violations. The standard ensures:
\begin{itemize}
    \item Every claim is machine-checkable
    \item No hidden assumptions
    \item Reproducible verification
\end{itemize}

\subsection{Proof-Carrying Code}

The concept of Proof-Carrying Code (PCC), introduced by Necula and Lee \cite{necula1997proof}, allows code producers to attach proofs that the code satisfies certain properties. A code consumer can verify the proofs without re-analyzing the code.

The Thiele Machine generalizes this: every execution step carries a "receipt" proving that:
\begin{itemize}
    \item The step is valid under the current axioms
    \item The $\mu$-cost has been properly charged
    \item The partition invariants are preserved
\end{itemize}

These receipts enable third-party verification: anyone can replay an execution and verify that the claimed costs were actually paid.

\section{Related Work}

\subsection{Algorithmic Information Theory}

The work of Kolmogorov, Chaitin, and Solomonoff on algorithmic information theory provides the foundation for my $\mu$-bit currency. The key insight is that structure is quantifiable as description length.

\subsection{Interactive Proof Systems}

Interactive proof systems (IP = PSPACE) show that verification can be more powerful than expected. The Thiele Machine's Logic Engine $L$ is a deterministic verifier-style component inspired by these results: it checks logical consistency under the current axioms.

\subsection{Partition Refinement Algorithms}

Algorithms like Tarjan's partition refinement and the Paige-Tarjan algorithm efficiently maintain partitions under operations. The Thiele Machine's \texttt{PSPLIT} and \texttt{PMERGE} operations are inspired by these techniques.

\subsection{Minimum Description Length in Machine Learning}

MDL has been used extensively in machine learning for model selection (Occam's razor). The Thiele Machine applies MDL to \textit{computation} rather than \textit{learning}, treating the partition structure as a "model" of the problem.

\section{Chapter Summary}

% TikZ Figure: Chapter 2 Summary - The Conceptual Foundation
\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=0.9], node distance=2.5cm]
    % Central node
    \node[draw, very thick, rounded corners, fill=yellow!30, minimum width=5.4cm, minimum height=2.6cm, align=center, text width=3.5cm] (thiele) at (0,0) {
        \begin{tabular}{c}
        \textbf{Thiele Machine}\\
        \small $\mu$-accounting
        \end{tabular}
    };
    
    % Four corners representing the four pillars
    \node[draw, very thick, rounded corners, fill=blue!15, minimum width=4.6cm, minimum height=2.2cm, align=center, text width=3.5cm] (comp) at (-5,3) {
        \begin{tabular}{c}
        \textbf{Computation}\\
        \small TM, RAM\\
        \small Blindness
        \end{tabular}
    };
    
    \node[draw, very thick, rounded corners, fill=green!15, minimum width=4.6cm, minimum height=2.2cm, align=center, text width=3.5cm] (info) at (5,3) {
        \begin{tabular}{c}
        \textbf{Information}\\
        \small Shannon, K(x)\\
        \small MDL
        \end{tabular}
    };
    
    \node[draw, very thick, rounded corners, fill=red!15, minimum width=4.6cm, minimum height=2.2cm, align=center, text width=3.5cm] (phys) at (-5,-3) {
        \begin{tabular}{c}
        \textbf{Physics}\\
        \small Landauer\\
        \small Thermodynamics
        \end{tabular}
    };
    
    \node[draw, very thick, rounded corners, fill=purple!15, minimum width=4.6cm, minimum height=2.2cm, align=center, text width=3.5cm] (quantum) at (5,-3) {
        \begin{tabular}{c}
        \textbf{Quantum}\\
        \small Bell, CHSH\\
        \small Tsirelson
        \end{tabular}
    };
    
    % Arrows to center
    \draw[very thick, ->, >=stealth, shorten >=2pt, shorten <=2pt] (comp) -- (thiele) node[midway, above, sloped] {\small structure-aware};
    \draw[very thick, ->, >=stealth, shorten >=2pt, shorten <=2pt] (info) -- (thiele) node[midway, above, sloped] {\small $\mu$-cost basis};
    \draw[very thick, ->, >=stealth, shorten >=2pt, shorten <=2pt] (phys) -- (thiele) node[midway, below, sloped] {\small cost justification};
    \draw[very thick, ->, >=stealth, shorten >=2pt, shorten <=2pt] (quantum) -- (thiele) node[midway, below, sloped] {\small $2\sqrt{2}$ derivation};
    
    % Verification layer
    \node[draw, very thick, rounded corners, fill=orange!20, minimum width=14.4cm, align=center, text width=3.5cm] (verify) at (0,-5.5) {
        \begin{tabular}{c}
        \textbf{Formal Verification} (Coq)\\
        \small 206 proofs $\cdot$ Inquisitor Standard $\cdot$ Zero axioms/admits
        \end{tabular}
    };
    \draw[very thick, ->, >=stealth, shorten >=2pt, shorten <=2pt] (thiele) -- (verify);
\end{tikzpicture}
\caption{The conceptual foundation of the Thiele Machine. Four pillars (computation theory, information theory, physics, quantum mechanics) converge to motivate the $\mu$-accounting framework, which is then rigorously verified using Coq.}
\label{fig:chapter2_summary}
\end{figure}

\paragraph{Understanding Figure \ref{fig:chapter2_summary}:}

\textbf{What does this diagram show?} The four foundational pillars supporting the Thiele Machine, converging to the central $\mu$-accounting framework.

\textbf{Center (yellow):} Thiele Machine with $\mu$-accounting. This is the thesis's contribution.

\textbf{Four corners (four pillars):}
\begin{itemize}
    \item \textbf{Computation (blue, top-left):} TM, RAM, Blindness. Classical computers cannot see structure. Arrow labeled "structure-aware"---the Thiele Machine adds explicit structure perception.
    
    \item \textbf{Information (green, top-right):} Shannon, K(x), MDL. Quantifies information and structure. Arrow labeled "$\mu$-cost basis"---MDL provides the mathematical foundation for $\mu$.
    
    \item \textbf{Physics (red, bottom-left):} Landauer, Thermodynamics. Information has physical cost. Arrow labeled "cost justification"---Landauer's principle justifies why $\mu$ must be monotonic (Second Law analog).
    
    \item \textbf{Quantum (purple, bottom-right):} Bell, CHSH, Tsirelson. Quantum correlations bounded by $2\sqrt{2}$. Arrow labeled "$2\sqrt{2}$ derivation"---the Thiele Machine derives this bound from $\mu$-accounting.
\end{itemize}

\textbf{Bottom (orange):} Formal Verification (Coq). Arrow from center down: the Thiele Machine is not just a conceptual idea---it's fully formalized with 206 proofs under the Inquisitor Standard (zero axioms/admits).

\textbf{Key insight:} The Thiele Machine is not built on a single idea---it synthesizes insights from four major areas of computer science, physics, and mathematics. Each pillar provides essential motivation:
\begin{itemize}
    \item Computation: the problem (blindness)
    \item Information: the solution (MDL-based cost)
    \item Physics: the justification (thermodynamic grounding)
    \item Quantum: the validation (Tsirelson bound emerges)
\end{itemize}

\textbf{Role in thesis:} Chapter 2 summary. Shows that the Thiele Machine is a deeply interdisciplinary synthesis, not just an incremental improvement to one area.

This chapter has established the conceptual foundation for the Thiele Machine by surveying four interconnected areas:

\begin{enumerate}
    \item \textbf{Classical Computation} (§2.1): Turing Machines and RAM models are \textit{structurally blind}---they cannot directly perceive the structure of their input. This blindness motivates the need for a model that explicitly accounts for structural knowledge.
    
    \item \textbf{Information Theory} (§2.2): Shannon entropy, Kolmogorov complexity, and Minimum Description Length (MDL) provide the mathematical foundation for quantifying structure. The $\mu$-bit cost in the Thiele Machine is based on MDL, providing a computable proxy for structural complexity.
    
    \item \textbf{Physics of Computation} (§2.3): Landauer's principle and the analysis of Maxwell's demon establish that information has physical consequences. The $\mu$-ledger is the computational analog of thermodynamic entropy---a monotonically increasing quantity tracking irreversible commitments.
    
    \item \textbf{Quantum Correlations} (§2.4): Bell's theorem and the CHSH inequality reveal that quantum mechanics permits correlations up to $2\sqrt{2}$ but no higher. The Thiele Machine \textit{derives} this bound from $\mu$-accounting, providing an information-theoretic explanation for why nature is "quantum but not more."
\end{enumerate}

\noindent
The formal verification infrastructure (§2.5) ensures that all claims about the Thiele Machine are machine-checkable using the Coq proof assistant under the Inquisitor Standard.

\paragraph{Key Takeaways for Later Chapters:}
\begin{itemize}
    \item The \textit{blindness problem} motivates the Thiele Machine's explicit structural accounting
    \item The $\mu$-cost is an MDL-based, computable measure of structural assertion
    \item The Tsirelson bound $2\sqrt{2}$ emerges as the boundary of the $\mu=0$ class
    \item All proofs satisfy the Inquisitor Standard (no admits, no axioms)
\end{itemize}
