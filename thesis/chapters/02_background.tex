\section{Why This Background Matters}

\subsection{What You Need to Know}

Before I dive into the Thiele Machine, you need to understand \textit{what problem it solves}. I didn't start with formal training in any of this---I started with questions I couldn't answer. This chapter covers what I had to learn:
\begin{itemize}
    \item \textbf{Computation theory}: What is a computer, really? (Turing Machines, RAM models)
    \item \textbf{Information theory}: What is information, and how do you measure it? (Shannon entropy, Kolmogorov complexity)
    \item \textbf{Physics of computation}: What are the physical limits on computing? (Landauer's principle, thermodynamics)
    \item \textbf{Quantum computing}: What does "quantum advantage" mean? (Bell's theorem, CHSH inequality)
    \item \textbf{Formal verification}: How can you \textit{prove} things about programs? (Coq, proof assistants)
\end{itemize}

I learned all of this by pulling on threads. If you already know it, skip ahead. If you don't, this is the chapter I wish I had when I started.

\subsection{The Central Question}

Classical computers (Turing Machines, RAM machines) are \textit{structurally blind}---they lack primitive access to the structure of their input. If you give a computer a sorted list, it doesn't "know" the list is sorted unless it checks. This is a statement about the interface of the model, not about what is computable. The distinction is between \emph{access} and \emph{ability}: structure is discoverable, but only through explicit computation.

This raises the question that drove everything: \textit{What if structural knowledge were a first-class resource that must be discovered, paid for, and accounted for?}

That's what this thesis answers.
The Thiele Machine answers this question by embedding structure into the machine state itself (as partitions and axioms) and by explicitly tracking the cost of adding that structure. That design choice is the bridge between the background material in this chapter and the formal model introduced in Chapter 3.

\subsection{How to Read This Chapter}

This chapter is organized from concrete to abstract:
\begin{enumerate}
    \item Section 2.2: Classical computation models (Turing Machine, RAM)
    \item Section 2.3: Information theory (Shannon, Kolmogorov, MDL)
    \item Section 2.4: Physics of computation (Landauer, thermodynamics)
    \item Section 2.5: Quantum computing and correlations (Bell, CHSH)
    \item Section 2.6: Formal verification (Coq, proof-carrying code)
\end{enumerate}

If you are familiar with any section, feel free to skip it. The only prerequisite for later chapters is understanding:
\begin{itemize}
    \item The ``blindness problem'' in classical computation (\S2.2)
    \item Kolmogorov complexity and MDL (\S2.3)
    \item The CHSH inequality and Tsirelson bound (\S2.5)
\end{itemize}

\section{Classical Computational Models}

\subsection{The Turing Machine: Formal Definition}

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    cell/.style={draw, minimum width=0.55cm, minimum height=0.55cm, font=\scriptsize},
    >=Stealth
]
% Tape cells
\foreach \i/\s in {0/0, 1/1, 2/0, 3/1, 4/1, 5/0, 6/1, 7/0} {
    \node[cell] (c\i) at (\i*0.55, 0) {\s};
}
% Ellipses
\node at (-0.5, 0) {\scriptsize$\cdots$};
\node at (4.9, 0) {\scriptsize$\cdots$};

% Head pointing at cell 3
\draw[->, thick, blue!70!black] (c3.south) ++(0, -0.15) -- ++(0, -0.5)
    node[below, font=\scriptsize\bfseries, blue!70!black] {head};

% State bubble
\node[draw, rounded corners=3pt, fill=blue!10, font=\scriptsize, inner sep=2pt]
    (state) at (1.65, -1.2) {sees: $(q, \gamma)$};

% What it cannot see
\node[draw, rounded corners=3pt, fill=red!8, font=\scriptsize, inner sep=2pt, text width=2.8cm, align=center]
    (blind) at (1.65, -2.1) {cannot see:\\global structure,\\relationships, patterns};

% Brace over tape
\draw[decorate, decoration={brace, amplitude=4pt, mirror}]
    (c0.south west) ++(0, -0.05) -- (c7.south east) ++(0, -0.05)
    node[midway, below=5pt, font=\tiny] {entire tape invisible to $\delta$};
\end{tikzpicture}
\caption{The Turing Machine's blindness: $\delta(q, \gamma)$ sees only the current state and one symbol. Global structure is architecturally inaccessible.}
\label{fig:tm-blindness}
\end{figure}

Turing's 1936 machine \cite{turing1936computable} is elegant. It's also the source of everything wrong with how people think about computation. Here's the formal definition---a 7-tuple:
\[
M = (Q, \Sigma, \Gamma, \delta, q_0, q_{\text{accept}}, q_{\text{reject}})
\]
\begin{itemize}
    \item $Q$: finite set of states
    \item $\Sigma$: input alphabet (no blank $\sqcup$)
    \item $\Gamma$: tape alphabet ($\Sigma \subset \Gamma$, $\sqcup \in \Gamma$)
    \item $\delta: Q \times \Gamma \to Q \times \Gamma \times \{L, R\}$: transition function
    \item $q_0, q_{\text{accept}}, q_{\text{reject}}$: start, accept, reject states
\end{itemize}

The tape is unbounded, with a finite non-blank region surrounded by blanks. A \textit{configuration} $(q, w, i)$ is current state, tape contents, and head position. Each step: read one symbol, write one, move left or right. Computation is a sequence $C_0 \vdash C_1 \vdash C_2 \vdash \cdots$ where $C_0 = (q_0, \sqcup w \sqcup, 1)$.

\subsubsection{The Computational Universality Theorem}

Turing proved there exists a \textit{Universal Turing Machine} $U$ such that $U(\langle M, w \rangle) = M(w)$ for any machine $M$ and input $w$. This establishes formal universality and supports the Church-Turing thesis: any mechanically computable function can be computed by a Turing Machine.

\subsubsection{The Blindness Problem}

Here's where the rot lives. Look at the transition function:
\[
\delta(q, \gamma) \mapsto (q', \gamma', d)
\]
It receives only the current state $q$ and the symbol $\gamma$ under the head. It does \textit{not} receive:
\begin{itemize}
    \item Global tape contents
    \item Structure of encoded data (e.g., that it's a graph)
    \item Relationships between input parts
\end{itemize}

This isn't a limitation you can program around---it's \textit{architectural}. The Turing Machine was designed to be local and sequential. Structure is accessible only through computation, not as a primitive. That's the blindness problem, and it's baked into the foundation of computer science.

\subsection{The Random Access Machine (RAM)}

The RAM model is the upgrade everyone thinks solves the problem:
\begin{itemize}
    \item Infinite register array $M[0], M[1], M[2], \ldots$
    \item Accumulator $A$ and program counter $PC$
    \item Instructions: LOAD, STORE, ADD, SUB, JMP, JZ, etc.
\end{itemize}

The improvement: \textit{random access}---accessing $M[i]$ takes $O(1)$ regardless of $i$ (unit-cost model). No more $O(n)$ seek time.

But structural blindness remains. A RAM can access $M[1000]$ directly, but it can't \emph{know} that $M[1000]$--$M[2000]$ encodes a sorted array without checking every element. Structure lives in programmer knowledge, not machine architecture. The problem moved; it didn't get solved.

\subsection{Complexity Classes and the P vs NP Problem}

The million-dollar question. Classical complexity theory defines:
\begin{itemize}
    \item \textbf{P}: Decision problems solvable by a deterministic Turing Machine in polynomial time
    \item \textbf{NP}: Decision problems where a "yes" instance has a polynomial-length certificate that can be verified in polynomial time
    \item \textbf{NP-Complete}: The hardest problems in NP—all NP problems reduce to them
\end{itemize}

The central open question is whether $\mathbf{P} = \mathbf{NP}$. If $\mathbf{P} \neq \mathbf{NP}$, then there exist problems whose solutions can be \textit{verified} efficiently but not \textit{found} efficiently.

The Thiele Machine reframes this entirely. Consider 3-SAT. A blind Turing Machine must search the exponential space $\{0,1\}^n$ in the worst case. But suppose the formula has hidden structure---say, it factors into independent sub-formulas. A machine that \textit{perceives} this structure can solve each sub-problem independently. The catch: \emph{perceiving} the factorization is itself information that must be justified, not assumed for free.

The question becomes: \textit{What does it cost to see the structure?}

The thesis argues that the apparent gap between P and NP is often the gap between:
\begin{itemize}
    \item Machines that have paid for structural insight ($\mu$-bits invested)
    \item Machines that have not (and must pay the Time Tax)
\end{itemize}
In the Thiele Machine, “paying for structural insight” means explicitly constructing partitions and attaching axioms that certify independence or other properties. Those operations are not free: they increase the $\mu$-ledger, which is then provably monotone under the step semantics.

This doesn't trivialize P vs NP---the structural information may itself be expensive to discover. But it reframes intractability as an \textit{accounting problem} rather than a \textit{fundamental barrier}. The cost of certifying structure, not assuming it for free.

\section{Information Theory and Complexity}

\subsection{Shannon Entropy}

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    box/.style={draw, rounded corners=2pt, font=\scriptsize, inner sep=4pt, minimum width=2.2cm, align=center},
    arr/.style={->, >=Stealth, thick, gray!70},
    note/.style={font=\tiny, gray!60!black, align=center}
]
% Boxes - vertical stack
\node[box, fill=blue!8] (sh) at (0, 0) {Shannon Entropy\\$H(X)$};
\node[box, fill=blue!15] (kc) at (0, -1.3) {Kolmogorov\\$K(x)$};
\node[box, fill=blue!25] (mdl) at (0, -2.6) {MDL\\$L(H) + L(D|H)$};
\node[box, fill=blue!40, text=white] (mu) at (0, -3.9) {$\mu$-bits\\$|\text{encode}(\Phi)|$};

% Arrows
\draw[arr] (sh) -- (kc);
\draw[arr] (kc) -- (mdl);
\draw[arr] (mdl) -- (mu);

% Annotations on right
\node[note, anchor=west] at (1.4, 0) {distributions\\computable};
\node[note, anchor=west] at (1.4, -1.3) {individual strings\\\textbf{uncomputable}};
\node[note, anchor=west] at (1.4, -2.6) {computable proxy\\model selection};
\node[note, anchor=west] at (1.4, -3.9) {operational cost\\implementation};

% Left annotations
\node[note, anchor=east, text=red!60!black] at (-1.5, -0.65) {measures\\uncertainty};
\node[note, anchor=east, text=red!60!black] at (-1.5, -1.95) {measures\\structure};
\node[note, anchor=east, text=red!60!black] at (-1.5, -3.25) {charges for\\structure};
\end{tikzpicture}
\caption{From theory to implementation: Shannon entropy measures uncertainty over distributions; Kolmogorov complexity measures individual string structure but is uncomputable; MDL provides a computable proxy; $\mu$-bits operationalize the cost in the Thiele Machine.}
\label{fig:info-hierarchy}
\end{figure}

Shannon's 1948 paper \cite{shannon1948mathematical} made information into something you can measure. The core idea: an event with probability $p$ carries surprise $I = -\log_2 p$ bits. The \textit{entropy} of random variable $X$:
\[
H(X) = -\sum_{x \in \mathcal{X}} p(x) \log_2 p(x)
\]

This measures uncertainty, or equivalently, expected bits needed under optimal prefix-free coding. Key properties:
\begin{itemize}
    \item $H(X) \ge 0$ (equality iff deterministic)
    \item $H(X) \le \log_2 |\mathcal{X}|$ (equality iff uniform)
    \item $H(X, Y) \le H(X) + H(Y)$ (equality iff $X \perp Y$)
\end{itemize}

The last property is the one that matters: knowing two variables are independent lets you decompose joint entropy. That's where exponential speedups hide. But independence is a structural assertion---and in the Thiele Machine, you pay $\mu$ for it.

\subsubsection{Entropy, Models, and What Is Actually Random}

Shannon entropy is a property of a \emph{distribution}, not of the underlying world. When I model a system with a random variable, I am quantifying my uncertainty and compressibility, not asserting that nature is literally rolling dice. A weather simulator, for example, may use Monte Carlo sampling or stochastic parameterizations to represent unresolved turbulence. The atmosphere itself is not sampling random numbers; the randomness is in my \emph{model} of an overwhelmingly complex, chaotic system. In other words, stochasticity is often epistemic: it reflects limited knowledge and coarse-grained descriptions rather than intrinsic indeterminism.

This distinction matters for the Thiele Machine because it highlights where "structure" lives. A partition that lets me treat two subsystems as independent is not a free fact about reality; it is an explicit modeling choice that I must justify and pay for. The entropy ledger charges me for the compressed description I claim to possess, not for any metaphysical randomness in the world.

\subsection{Kolmogorov Complexity}

Shannon entropy applies to random variables. \textit{Kolmogorov complexity} measures the structural content of individual strings---the ultimate compression. For a string $x$:
\[
K(x) = \min \{|p| : U(p) = x\}
\]
where $U$ is a universal Turing Machine and $|p|$ is the bit-length of program $p$.

The intuition: "010101010101..." (alternating) has low complexity---a short program generates it. A random string has high complexity---no program substantially shorter than the string itself can produce it.

Key theorems:
\begin{itemize}
    \item \textbf{Invariance Theorem}: $K_U(x) = K_{U'}(x) + O(1)$ for any two universal machines $U, U'$
    \item \textbf{Incompressibility}: For any $n$, there exists a string $x$ of length $n$ with $K(x) \ge n$
    \item \textbf{Uncomputability}: $K(x)$ is not computable (by reduction from the halting problem)
\end{itemize}

The uncomputability of Kolmogorov complexity is why the Thiele Machine uses \textit{Minimum Description Length} (MDL) instead---a computable approximation that captures description length without requiring an impossible oracle.

\subsubsection{Comparison with $\mu$-bits}

It is important to distinguish the theoretical $K(x)$ from the operational $\mu$-bit cost. While Kolmogorov complexity represents the ultimate lower bound on description length using an optimal universal machine, the $\mu$-bit cost is a concrete, computable metric based on the specific structural assertions made by the Thiele Machine.
\begin{itemize}
    \item $K(x)$ is uncomputable and depends on the choice of universal machine (up to a constant).
    \item $\mu$-cost is computable and depends on the specific partition logic operations and axioms used.
\end{itemize}
Thus, $\mu$ serves as a constructive upper bound on the structural complexity, representing the cost of the structure \textit{actually used} by the algorithm, rather than the theoretical minimum. This makes $\mu$ a practical resource for complexity analysis in a way that $K(x)$ cannot be.

In the implementation, the proxy is not a magical compressor; it is a canonical string encoding of axioms and partitions (SMT-LIB strings plus region encodings), so the cost is defined in a way that can be checked by the formal kernel and reproduced by the other layers.

\subsection{Minimum Description Length (MDL)}

MDL, developed by Jorma Rissanen \cite{rissanen1978modeling}, gives us what Kolmogorov can't: a computable proxy. Given a hypothesis class $\mathcal{H}$ and data $D$:
\[
L(D) = \min_{H \in \mathcal{H}} \{L(H) + L(D|H)\}
\]
where:
\begin{itemize}
    \item $L(H)$ is the description length of hypothesis $H$
    \item $L(D|H)$ is the description length of $D$ given $H$ (the "residual")
\end{itemize}

In the Thiele Machine, MDL serves as the basis for $\mu$-cost:
\begin{itemize}
    \item The "hypothesis" is the partition structure $\pi$
    \item $L(\pi)$ is the $\mu$-cost of specifying the partition
    \item $L(\text{computation}|\pi)$ is the operational cost given the structure
\end{itemize}

The total $\mu$-cost is thus analogous to the MDL of the computation, with the partition description and its axioms charged explicitly as a model of structure. This separates the cost of \emph{describing} structure from the cost of \emph{using} it.
This is reflected directly in the Python and Coq implementations: the $\mu$-ledger is updated by explicit per-instruction costs, and structural operations (like partition creation or split) carry their own explicit charges.

\section{The Physics of Computation}

\subsection{Landauer's Principle}

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    box/.style={draw, rounded corners=2pt, font=\scriptsize, inner sep=3pt, align=center},
    arr/.style={->, >=Stealth, thick},
    heat/.style={->, >=Stealth, thick, red!70!black, decorate, decoration={snake, amplitude=1.5pt, segment length=5pt}},
    note/.style={font=\tiny, gray!50!black}
]
% Left: Bit erasure
\node[box, fill=gray!10, minimum width=1.1cm] (b0) at (0, 0) {0 or 1};
\node[box, fill=gray!10, minimum width=1.1cm] (b1) at (0, -1.2) {0};
\draw[arr] (b0) -- (b1) node[midway, right, font=\tiny] {erase};
\draw[heat] (b1.south) -- ++(0, -0.6) node[below, font=\tiny, red!70!black] {$\ge k_B T \ln 2$};

% Label
\node[font=\scriptsize\bfseries, anchor=south] at (0, 0.6) {Landauer};

% Right: Thiele Machine analog
\node[box, fill=blue!10, minimum width=1.2cm] (s0) at (3.2, 0) {no structure};
\node[box, fill=blue!10, minimum width=1.2cm] (s1) at (3.2, -1.2) {$\Phi$ asserted};
\draw[arr, blue!70!black] (s0) -- (s1) node[midway, right, font=\tiny] {\texttt{LASSERT}};
\draw[arr, blue!70!black] (s1.south) -- ++(0, -0.6) node[below, font=\tiny, blue!70!black] {$\Delta\mu \ge |\phi|_{\text{bits}}$};

% Label
\node[font=\scriptsize\bfseries, anchor=south, blue!70!black] at (3.2, 0.6) {Thiele Machine};

% Connecting idea
\node[draw, dashed, rounded corners=2pt, fill=yellow!10, font=\tiny, inner sep=2pt, text width=2.5cm, align=center]
    at (1.6, -2.6) {Both: irreversible commitment\\costs proportional to information};
\end{tikzpicture}
\caption{Landauer's principle and its computational analog. Bit erasure costs $k_B T \ln 2$ joules; structural assertion costs $\mu$-bits. Both are irreversible, monotonic, proportional to information.}
\label{fig:landauer-thiele}
\end{figure}

In 1961, Rolf Landauer proved something that changes everything \cite{landauer1961irreversibility}:

\begin{theorem}[Landauer's Principle]
The erasure of one bit of information in a computing device releases at least $k_B T \ln 2$ joules of heat into the environment.
\end{theorem}

At room temperature (300K), this is approximately $3 \times 10^{-21}$ joules per bit---tiny, but fundamentally non-zero.

Landauer's principle establishes three facts that underpin this entire thesis:
\begin{enumerate}
    \item \textbf{Information is physical}: You can't erase it without physical consequences
    \item \textbf{Irreversibility costs}: Logically irreversible operations (many-to-one maps like AND, OR, erasure) dissipate heat
    \item \textbf{Computation is thermodynamic}: The ultimate limits are set by physics, not engineering
\end{enumerate}

From a first-principles perspective, the key step is that erasure reduces the logical state space. Mapping two possible inputs to a single output decreases the system's entropy by $\Delta S = k_B \ln 2$. To satisfy the second law, that entropy must be exported to the environment as heat $Q \ge T \Delta S$, yielding the $k_B T \ln 2$ bound. Reversible gates avoid this penalty by preserving a one-to-one mapping between logical states, but they shift the cost to auxiliary memory and garbage bits that must eventually be erased.

\subsubsection{From Landauer to Planck}

Landauer's principle provides more than a thermodynamic bound---it suggests a structural connection between information theory and quantum mechanics. Define the Landauer energy as:
\[
E_{\text{landauer}} = k_B T \ln 2
\]

\textbf{Conjecture:} If computational operations occur at a fundamental time scale $\tau_\mu$, then Planck's constant might be expressible as:
\[
h = 4 E_{\text{landauer}} \cdot \tau_\mu = 4 k_B T \ln 2 \cdot \tau_\mu
\]

\textbf{Important caveat:} This is \textit{not} a derivation of $h$. Rather, defining $\tau_\mu := h / (4 E_{\text{landauer}})$ makes the relationship a tautology. The formal proof in \path{coq/physics_exploration/PlanckDerivation.v} (Chapter 12) establishes algebraic consistency but provides no predictive power without an independent derivation of $\tau_\mu$.

At room temperature ($T = 300$K) with known $h$, the \textit{computed} (not predicted) time scale is:
\[
\tau_\mu = \frac{h}{4 k_B T \ln 2} \approx 5.77 \times 10^{-14} \text{ seconds}
\]

This $\sim$58 femtosecond scale is consistent with fundamental quantum time scales, suggesting that $\mu$-operations occur at the boundary between classical information processing and quantum dynamics.

\subsubsection{Reversible Computation}

Charles Bennett showed you can make computation thermodynamically reversible by keeping a history of all operations \cite{bennett1982thermodynamics}. A reversible Turing Machine can simulate any irreversible computation with only polynomial overhead in space.

But there's a catch: the space required to store the history. This is another form of "structural debt"---you can avoid the heat cost by paying a space cost. The universe doesn't give free lunches.

\subsubsection{Simulation Versus Physical Reality}

"If I can simulate it, I have reproduced it." That's wrong, and physics tells us exactly why.

A simulation manipulates \emph{symbols} that represent a system. The system itself evolves under physical laws. A climate model produces temperature fields, hurricanes, droughts on a screen---but it doesn't warm the room or generate real rainfall. The computation is physical (it dissipates heat, uses energy), but the simulated climate is informational, not atmospheric.

This matters because any claim about "cost" depends on the level of description. A Monte Carlo weather model may treat unresolved convection as a random process, but the real atmosphere is not a Monte Carlo chain; it is a high-dimensional deterministic (or quantum-to-classical) system whose unpredictability is amplified by chaos. When I trade the real dynamics for a stochastic approximation, I am asserting a structural model that saves compute at the price of fidelity. The Thiele Machine makes that trade explicit: the cost of declaring independence, randomness, or coarse-grained behavior must be booked in $\mu$-bits.

\subsubsection{Renormalization and Coarse-Grained Structure}

Renormalization is a formal way to justify this kind of model compression. In statistical physics and quantum field theory, I group microscopic degrees of freedom into blocks, integrate out short-scale details, and obtain an effective theory at a larger scale. This is a principled, repeatable way of asserting structure: I discard information about microstates but gain predictive power at the macro level. The price is an explicit approximation error and new effective parameters.

From the Thiele Machine perspective, renormalization is a structured partition of state space. I am committing to a hierarchy of equivalence classes that summarize behavior at each scale. The $\mu$-ledger charges for these commitments, making the bookkeeping of coarse-grained structure as explicit as the bookkeeping of energy.

\subsection{Maxwell's Demon and Szilard's Engine}

This thought experiment explains why information can't be free:

Imagine a container divided by a partition with a door. A "demon" observes molecules and opens the door only when a fast molecule approaches from the left. Over time, fast molecules accumulate on the right, creating a temperature differential without apparent work.

Leo Szilard's 1929 analysis \cite{szilard1929entropieverminderung} and Bennett's later work showed the demon must pay:
\begin{enumerate}
    \item \textbf{Acquiring information}: Measuring molecular velocities requires physical interaction
    \item \textbf{Storing information}: The demon's memory has finite capacity
    \item \textbf{Erasing information}: When memory fills, erasure releases heat (Landauer)
\end{enumerate}

The entropy balance is preserved. The demon's information processing exactly compensates for the apparent entropy reduction. No cheating.

\subsection{Connection to the Thiele Machine}

The $\mu$-ledger is the computational analog of thermodynamic entropy. Both are monotonically increasing. Both track irreversible commitments. The difference: thermodynamic entropy tracks energy dissipation; the $\mu$-ledger tracks structural information cost. And the initiality theorem (\path{coq/kernel/MuInitiality.v}) proves something thermodynamics can't: $\mu$ is the \emph{unique} monotone cost functional consistent with the instruction set. There is no alternative accounting system.

Landauer's principle says erasing one bit costs $k_B T \ln 2$ joules. The Thiele Machine says asserting one bit of structural constraint costs at least one $\mu$-bit. The parallel is deliberate---both track irreversible information commitments---but honesty requires noting a difference: $\mu$-monotonicity is true \textit{by construction} (the machine is built so that every instruction adds a non-negative cost), whereas the second law of thermodynamics is an empirical fact about the universe. The Coq proofs in \path{coq/kernel/MuLedgerConservation.v} verify that the design achieves monotonicity, not that monotonicity is a discovered physical law.

Every \texttt{LASSERT} (asserting a constraint), every \texttt{PSPLIT} (partitioning state), every \texttt{REVEAL} (disclosing information) increases $\mu$. By design, the $\mu$-ledger never decreases. This is the computational analog of the second law: entropy (here, $\mu$) is monotonic. The analogy is structural, and the engineering is intentional.

Maxwell's Demon thought it could cheat thermodynamics by being clever. It could not---Landauer showed the demon's memory erasure pays exactly the entropy it tried to steal. The Thiele Machine makes the same guarantee: you cannot reduce the search space without paying structural information cost. The No Free Insight theorem (\path{coq/kernel/NoFreeInsight.v}) is, in a sense, the computational Landauer's principle.

\section{Quantum Computing and Correlations}

\subsection{Bell's Theorem and Non-Locality}

This is where physics gets strange---and where the Thiele Machine makes a testable prediction.

In 1964, John Bell \cite{bell1964einstein} asked a simple question: can quantum mechanics be explained by ``local hidden variables''---some underlying classical mechanism where particles carry pre-determined values and nothing travels faster than light?

The answer is no. Bell proved that any local hidden variable theory must satisfy certain inequalities on correlations between distant measurements. Quantum mechanics violates those inequalities. Nature is not locally classical.

The CHSH inequality \cite{clauser1969proposed} makes this experimentally testable. Take two parties---Alice and Bob---each choosing between two measurement settings ($x, y \in \{0, 1\}$) and each getting a binary outcome ($a, b \in \{-1, +1\}$). Define the CHSH parameter:
\[
S = E(0,0) + E(0,1) + E(1,0) - E(1,1)
\]
where $E(x,y)$ is the correlation between Alice and Bob's outcomes for settings $x, y$.

Three bounds matter:
\begin{itemize}
    \item \textbf{Classical bound} ($S \le 2$): If Alice and Bob share only classical correlations---local hidden variables---then $|S| \le 2$. This is Bell's inequality, provable by enumerating all 16 deterministic strategies. Formalized as \texttt{local\_S\_2} in \path{coq/kernel/AssumptionBundle.v} and independently proven via Fine's theorem in \path{coq/kernel/MinorConstraints.v}.
    \item \textbf{Quantum bound} ($S \le 2\sqrt{2} \approx 2.828$): If they share quantum entanglement, the maximum is the Tsirelson bound. This requires Tsirelson's 1980 theorem \cite{tsirelson1980quantum}, formalized as \texttt{tsir\_from\_coh} in AssumptionBundle and independently proven via algebraic coherence constraints ($A^2 = B^2 = I$, which encode that observables are self-adjoint involutions) in \path{coq/kernel/TsirelsonFromAlgebra.v}.
    \item \textbf{Algebraic bound} ($S \le 4$): The absolute maximum from triangle inequality alone, achievable only by hypothetical ``PR-box'' correlations that violate quantum mechanics. Formalized as \texttt{valid\_S\_4}.
\end{itemize}

The gap between 2 and $2\sqrt{2}$ is why quantum computers can do things classical computers cannot. The gap between $2\sqrt{2}$ and 4 is why nature stops at quantum---it does not go all the way to the no-signaling limit.

These three bounds are among the 6 hard mathematical facts formalized in \path{coq/kernel/AssumptionBundle.v} as a \texttt{HardMathFacts} record---bundled in a record type, threaded through proofs as explicit parameters, every dependency transparent.

\path{coq/kernel/HardMathFactsProven.v} constructs a concrete instance of the corrected record (\texttt{HardMathFactsCorrected}) with zero axioms and zero admits---686 lines of machine-checked proof from first principles. All six are proven: \texttt{norm\_E\_bound\_proven} from probability axioms, \texttt{valid\_S\_4\_proven} from the triangle inequality, \texttt{local\_S\_2\_proven} from exhaustive 16-case enumeration, \texttt{pr\_no\_ext\_proven} for PR-box monogamy, \texttt{symm\_coh\_bound\_proven} for symmetric coherence, and \texttt{tsir\_from\_coh\_proven} for Tsirelson's bound via algebraic coherence. The bundle is a theorem, not an assumption. Run \texttt{Print Assumptions} on any downstream result and Coq reports only standard library axioms.

\subsection{Decoherence, Measurement, and Informational Cost}

Quantum correlations are fragile because measurement is a physical interaction. Decoherence occurs when a quantum system becomes entangled with an uncontrolled environment, effectively "measuring" it and suppressing interference.

The key insight: extracting a classical bit from a quantum system isn't a free epistemic update---it's a physical process that dumps phase information into the environment. Gaining classical knowledge has a thermodynamic footprint.

This perspective ties directly to the Thiele Machine's revelation rule. When the machine asserts a supra-quantum certification, it must emit an explicit revelation-class instruction, because the correlation is not just a mathematical artifact---it is a structural claim that needs a physical bookkeeping event. The model mirrors the physics: information is not free, whether it is classical or quantum.

\subsection{The Revelation Requirement}

Here's the theorem that connects quantum correlations to $\mu$-accounting:

\begin{theorem}[Revelation Requirement (\path{coq/kernel/RevelationRequirement.v})]
If a Thiele Machine execution produces a state with ``supra-quantum'' certification (a nonzero certification flag in a control/status register, starting from 0), then the execution trace must contain an explicit revelation-class instruction (\texttt{REVEAL}, \texttt{EMIT}, \texttt{LJOIN}, or \texttt{LASSERT}).
\end{theorem}

Translation: you can't certify non-local correlations without paying the structural cost. No free insight, no free certification.

\section{Formal Verification}

\subsection{The Coq Proof Assistant}

How do you know a proof is correct? You could check it by hand. You could have reviewers check it. Or you could have a machine verify every single step.

Coq is the machine. It implements the Calculus of Inductive Constructions---a type theory where proofs are programs and types are propositions. This is the Curry-Howard correspondence: proving a theorem is the same as writing a program that inhabits a type. When the Coq compiler accepts a proof, the type checker has verified every logical step. No human judgment involved. No ``trust me'' appeals.

The trusted computing base is small---Coq's kernel is roughly 30,000 lines of OCaml. If you trust that kernel (and thousands of mathematicians do, daily), then you trust every proof it accepts. The alternative is trusting me, a car salesman. I know which one I would pick.

For the Thiele Machine, this matters because the claims are not hand-waved. The Coq corpus---286 files, 79,741 lines, 2,329 theorem/lemma declarations---is checked by the compiler. Every property---$\mu$-monotonicity, $\mu$-initiality, No Free Insight, observational no-signaling, oracle impossibility---is established by a proof that the machine accepted, not by an argument I wrote in prose.

\subsection{The Inquisitor Standard}

For the Thiele Machine, the project enforces a strict methodology. No wiggle room:

\begin{enumerate}
    \item \textbf{No \texttt{Admitted}}: Every lemma must be fully proven
    \item \textbf{No \texttt{admit} tactics}: No shortcuts inside proofs
    \item \textbf{Documented assumptions}: External mathematical results (e.g., Tsirelson's theorem, Fine's theorem, Bell's inequality) are formalized as a \texttt{HardMathFacts} record in \path{coq/kernel/AssumptionBundle.v}---6 facts total. These are not Coq \texttt{Axiom} declarations; they are bundled as a record type and threaded through proofs as explicit parameters. All 6 are \textit{mechanically proven from first principles} in \path{coq/kernel/HardMathFactsProven.v}. The assumption surface is zero.
\end{enumerate}

This is enforced by \path{scripts/inquisitor.py}---a static analyzer with 50+ finding codes that scans every Coq file in CI. It checks for \texttt{Admitted}, \texttt{admit}, custom axioms, circular definitions, tautological proofs, smuggled constraints, and more. If a theorem says ``Proven,'' it compiles to \texttt{Qed}.

\subsection{Proof-Carrying Code}

Necula and Lee's Proof-Carrying Code (PCC) \cite{necula1997proof} lets code producers attach proofs that the code satisfies certain properties. A consumer can verify the proofs without re-analyzing the code.

The Thiele Machine generalizes this: every execution step carries a ``receipt'' proving that:
\begin{itemize}
    \item The step is valid under the current axioms
    \item The $\mu$-cost has been properly charged
    \item The partition invariants are preserved
\end{itemize}

These receipts enable third-party verification: anyone can replay an execution and verify that the claimed costs were paid. Trust nothing, verify everything.

\section{Related Work}

This thesis does not claim to have invented these ideas. It claims to have connected them in a new way.

\subsection{Algorithmic Information Theory}

Kolmogorov, Chaitin, and Solomonoff established that structure is quantifiable as description length. That's the foundation of $\mu$-bits.

\subsection{Interactive Proof Systems}

Interactive proof systems (IP = PSPACE) show that verification can be more powerful than expected. The Thiele Machine's Logic Engine $L$ is a deterministic verifier-style component inspired by these results: it checks logical consistency under the current axioms.

\subsection{Partition Refinement Algorithms}

Algorithms like Tarjan's partition refinement and the Paige-Tarjan algorithm efficiently maintain partitions under operations. The Thiele Machine's \texttt{PSPLIT} and \texttt{PMERGE} operations are inspired by these techniques.

\subsection{Minimum Description Length in Machine Learning}

MDL has been used extensively in machine learning for model selection (Occam's razor). The Thiele Machine applies MDL to \textit{computation} rather than \textit{learning}, treating the partition structure as a "model" of the problem.

\section{Chapter Summary}

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    area/.style={draw, rounded corners=3pt, font=\scriptsize, inner sep=4pt, minimum width=2cm, align=center},
    core/.style={draw, rounded corners=3pt, font=\scriptsize\bfseries, inner sep=5pt, fill=blue!30, minimum width=2.2cm, align=center},
    link/.style={->, >=Stealth, thick, gray!60},
    contrib/.style={font=\tiny, gray!50!black, align=center}
]
% Central node
\node[core] (tm) at (0, 0) {Thiele\\Machine};

% Four areas
\node[area, fill=gray!10] (comp) at (-2.2, 1.5) {Classical\\Computation};
\node[area, fill=gray!10] (info) at (2.2, 1.5) {Information\\Theory};
\node[area, fill=gray!10] (phys) at (-2.2, -1.5) {Physics of\\Computation};
\node[area, fill=gray!10] (quant) at (2.2, -1.5) {Quantum\\Correlations};

% Arrows to center
\draw[link] (comp) -- (tm) node[midway, above, contrib, sloped] {blindness};
\draw[link] (info) -- (tm) node[midway, above, contrib, sloped] {$\mu$-bits};
\draw[link] (phys) -- (tm) node[midway, below, contrib, sloped] {$\mu$-ledger};
\draw[link] (quant) -- (tm) node[midway, below, contrib, sloped] {revelation};

% Top: Formal Verification spanning
\node[draw, dashed, rounded corners=2pt, fill=yellow!8, font=\tiny, inner sep=3pt]
    at (0, 2.6) {Formal Verification (Coq): all claims machine-checked};
\end{tikzpicture}
\caption{Conceptual foundation: four areas converge on the Thiele Machine. Classical computation identifies the blindness problem; information theory provides the $\mu$-bit measure; physics grounds irreversibility in the $\mu$-ledger; quantum bounds require the revelation rule. Formal verification ensures all claims are machine-checked.}
\label{fig:ch2-summary}
\end{figure}

This chapter established the foundation. Four interconnected areas:

\begin{enumerate}
    \item \textbf{Classical Computation} (\S2.2): Turing Machines and RAM models are \textit{structurally blind}---they can't directly perceive input structure. This blindness motivates everything that follows.

\item \textbf{Information Theory} (\S2.3): Shannon entropy, Kolmogorov complexity, and MDL provide the math for quantifying structure. The $\mu$-bit cost is based on MDL---a computable proxy for structural complexity.

\item \textbf{Physics of Computation} (\S2.4): Landauer's principle and Maxwell's demon establish that information has physical consequences. The $\mu$-ledger is the computational analog of thermodynamic entropy---monotonically increasing, tracking irreversible commitments.

\item \textbf{Quantum Correlations} (\S2.5): Bell's theorem and the CHSH inequality reveal that quantum mechanics permits correlations up to $2\sqrt{2}$ but no higher. The Thiele Machine connects this bound to $\mu$-accounting---the 6 hard mathematical facts formalized in \path{coq/kernel/AssumptionBundle.v} are all mechanically proven from first principles in \path{coq/kernel/HardMathFactsProven.v}, making the dependency chain fully transparent with zero remaining assumptions.
\end{enumerate}

\noindent
The formal verification infrastructure (\S2.6) ensures all claims are machine-checkable using Coq under the Inquisitor Standard.

\paragraph{Key Takeaways:}
\begin{itemize}
    \item The \textit{blindness problem} motivates explicit structural accounting
    \item The $\mu$-cost is MDL-based and computable
    \item The classical bound ($S \le 2$) characterizes the $\mu=0$ class, while the Tsirelson bound ($2\sqrt{2}$) requires $\mu > 0$ investment
    \item All proofs satisfy the Inquisitor Standard---zero admits, zero custom axioms, all hard mathematical assumptions proven from first principles
\end{itemize}
