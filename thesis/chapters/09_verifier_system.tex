\section{The Verifier System: Receipt-Defined Certification}

\subsection{Why Verification Matters}

Scientific claims require evidence. When a researcher claims ``this algorithm produces truly random numbers'' or ``this drug causes improved outcomes,'' I need a way to verify these claims independently. Traditional verification relies on trust: I trust that the researcher ran the experiments correctly, recorded the data accurately, and analyzed it properly.

The Thiele Machine's verifier system replaces trust with \textit{cryptographic proof}. Every claim must be accompanied by a \textbf{receipt}---a tamper-proof record of the computation that produced the claim. Anyone can verify the receipt independently, without trusting the original claimant.

From first principles, a verifier needs three ingredients:
\begin{enumerate}
    \item \textbf{Trace integrity}: a way to bind a claim to a specific execution history.
    \item \textbf{Semantic checking}: a way to re-interpret that history under the modelâ€™s rules.
    \item \textbf{Cost accounting}: a way to ensure that any strengthened claim paid the required $\mu$-cost.
\end{enumerate}
The verifier system is built to guarantee all three.

This chapter documents the complete verification infrastructure. The system implements four certification modules (C-modules) that enforce the No Free Insight principle across different application domains:
\begin{itemize}
    \item \textbf{C-RAND}: Certified randomness---proving that bits are truly unpredictable
    \item \textbf{C-TOMO}: Certified estimation---proving that measurements are accurate
    \item \textbf{C-ENTROPY}: Certified entropy---proving that disorder is quantified correctly
    \item \textbf{C-CAUSAL}: Certified causation---proving that causes actually produce effects
\end{itemize}

The key insight is that \textit{stronger claims require more evidence}. If you claim high-quality randomness, you must demonstrate the source of that randomness. If you claim precise measurements, you must show enough trials to support that precision. The verifier system makes this relationship explicit and enforceable by turning every claim into a checkable predicate over receipts and by requiring explicit $\mu$-charged disclosures whenever the predicate is strengthened.

\section{Architecture Overview}

\subsection{The Closed Work System}

The verification system is orchestrated through a unified closed-work pipeline that produces verifiable artifacts for each certification module. A ``closed work'' run is one where the verifier only accepts inputs that appear in the receipt manifest; any out-of-band data is ignored.

Each verification includes:
\begin{itemize}
    \item PASS/FAIL/UNCERTIFIED status
    \item Explicit falsifier attempts and outcomes
    \item Declared structure additions (if any)
    \item Complete $\mu$-accounting summary
\end{itemize}

\subsection{The TRS-1.0 Receipt Protocol}

All verification is receipt-defined through the TRS-1.0 (Thiele Receipt Standard) protocol:
\begin{verbatim}
{
    "version": "TRS-1.0",
    "timestamp": "2025-12-17T00:00:00Z",
    "manifest": {
        "claim.json": "sha256:...",
        "samples.csv": "sha256:...",
        "disclosure.json": "sha256:..."
    },
    "signature": "ed25519:..."
}
\end{verbatim}

Key properties:
\begin{itemize}
    \item \textbf{Content-addressed}: All artifacts are identified by SHA-256 hash
    \item \textbf{Signed}: Ed25519 signatures prevent tampering
    \item \textbf{Minimal}: Only receipted artifacts can influence verification
\end{itemize}

This protocol supplies the trace integrity requirement: a verifier can recompute hashes and signatures to confirm that the claim is exactly the one produced by the recorded execution.

\subsection{Non-Negotiable Falsifier Pattern}

Every C-module ships three mandatory falsifier tests. Each test targets a distinct failure mode:
\begin{enumerate}
    \item \textbf{Forge test}: Attempt to manufacture receipts without the canonical channel/opcode.
    \item \textbf{Underpay test}: Attempt to obtain the claim while paying fewer $\mu$/info bits.
    \item \textbf{Bypass test}: Route around the channel and confirm rejection.
\end{enumerate}

\section{C-RAND: Device-Independent Certified Randomness}

\subsection{Claim Structure}

A randomness claim specifies:
\begin{verbatim}
{
    "n_bits": 1024,
    "min_entropy_per_bit": 0.95
}
\end{verbatim}

\subsection{Verification Rules}

The randomness verifier enforces:
\begin{itemize}
    \item Every input must appear in the TRS-1.0 receipt manifest
    \item Min-entropy claims require explicit nonlocality/disclosure evidence
    \item Required disclosure bits: $\lceil 1024 \cdot H_{min} \rceil$
\end{itemize}

Why these rules? Because without a receipt-bound source, the verifier has no basis for trusting the bits, and without disclosure evidence, the claim could be strengthened without paying the structural cost.

\subsection{The Randomness Bound}

Formal bridge lemma (illustrative):
\begin{verbatim}
Definition RandChannel (r : Receipt) : bool :=
  Nat.eqb (r_op r) RAND_TRIAL_OP.

Lemma decode_is_filter_payloads :
  forall tr,
    decode RandChannel tr = map r_payload (filter RandChannel tr).
\end{verbatim}

This ensures that randomness claims are derived only from receipted trial data. In other words, the verifier can only compute a randomness predicate over the receipts it can check.

\subsection{Falsifier Tests}

\begin{itemize}
    \item \textbf{Forge}: Create receipts claiming high entropy without running trials $\rightarrow$ REJECTED
    \item \textbf{Underpay}: Claim $H_{min} = 0.99$ but provide only $H_{min} = 0.5$ disclosure $\rightarrow$ REJECTED
    \item \textbf{Bypass}: Submit raw bits without receipt chain $\rightarrow$ UNCERTIFIED
\end{itemize}

\section{C-TOMO: Tomography as Priced Knowledge}

\subsection{Claim Structure}

A tomography claim specifies an estimate within tolerance:
\begin{verbatim}
{
    "estimate": 0.785,
    "epsilon": 0.01,
    "n_trials": 10000
}
\end{verbatim}

\subsection{Verification Rules}

The tomography verifier enforces:
\begin{itemize}
    \item Trial count must match receipted samples
    \item Tighter $\epsilon$ requires more trials (cost rule)
    \item Statistical consistency checks on estimate derivation
\end{itemize}

These rules embody a first-principles trade-off: precision is information, and information requires evidence. The verifier therefore couples $\epsilon$ to a minimum sample size and rejects claims that underpay the evidence requirement.

\subsection{The Precision-Cost Relationship}

Estimation precision is priced: tighter $\epsilon$ requires proportionally more evidence:
\begin{equation}
    n_{required} \ge c \cdot \epsilon^{-2}
\end{equation}

where $c$ is a domain-specific constant.

\section{C-ENTROPY: Coarse-Graining Made Explicit}

\subsection{The Entropy Underdetermination Problem}

Entropy is ill-defined without specifying a coarse-graining (partition). Two observers with different partitions will compute different entropies for the same physical state. A verifier therefore treats the coarse-graining itself as part of the claim and requires it to be receipted.

\subsection{Claim Structure}

An entropy claim must declare its coarse-graining:
\begin{verbatim}
{
    "h_lower_bound_bits": 3.2,
    "n_samples": 5000,
    "coarse_graining": {
        "type": "histogram",
        "bins": 32
    }
}
\end{verbatim}

\subsection{Verification Rules}

The entropy verifier enforces:
\begin{itemize}
    \item Entropy claims without declared coarse-graining $\rightarrow$ REJECTED
    \item Coarse-graining must be in receipted manifest
    \item Disclosure bits scale with entropy bound: $\lceil 1024 \cdot H \rceil$
\end{itemize}

The rationale is direct: entropy is a function of a partition, and the partition itself is structural information that must be paid for.

\subsection{Coq Formalization}

Formal impossibility lemma (illustrative):
\begin{verbatim}
Theorem region_equiv_class_infinite : forall s,
  exists f : nat -> VMState,
    (forall n, region_equiv s (f n)) /\
    (forall n1 n2, f n1 = f n2 -> n1 = n2).
\end{verbatim}

This proves that observational equivalence classes are infinite, blocking entropy computation without explicit coarse-graining. In practice, the verifier uses this impossibility result to reject entropy claims that omit a receipted partition.

\section{C-CAUSAL: No Free Causal Explanation}

\subsection{The Causal Inference Problem}

Claiming a unique causal DAG from observational data alone is impossible in general (Markov equivalence classes contain multiple DAGs). Stronger-than-observational claims require explicit assumptions or interventional evidence, and those assumptions are themselves structure that must be disclosed and charged.

\subsection{Claim Types}

\begin{itemize}
    \item \texttt{unique\_dag}: Claims a unique causal graph (requires 8192 disclosure bits)
    \item \texttt{ate}: Claims average treatment effect (requires 2048 disclosure bits)
\end{itemize}

\subsection{Verification Rules}

The causal verifier enforces:
\begin{itemize}
    \item \texttt{unique\_dag} claims require \texttt{assumptions.json} or \texttt{interventions.csv}
    \item Intervention count must match receipted data
    \item Pure observational data cannot certify unique DAGs
\end{itemize}

\subsection{Falsifier Tests}

\begin{verbatim}
def test_unique_dag_without_assumptions_rejected():
    # Claim unique DAG from pure observational data
    # Must be rejected: causal claims need extra structure
    result = verify_causal(run_dir, trust_manifest)
    assert result.status == "REJECTED"
\end{verbatim}

\section{Bridge Modules: Kernel Integration}

The verifier system includes bridge lemmas connecting application domains to the kernel. Each bridge supplies:
\begin{itemize}
    \item a channel selector for the opcode class,
    \item a decoding lemma that extracts only receipted payloads,
    \item a proof that domain-specific claims incur the corresponding $\mu$-cost.
\end{itemize}

This is the semantic checking requirement: the verifier can only interpret what the kernel would accept, and any domain-specific claim is reduced to a kernel-level obligation.

Each bridge:
\begin{itemize}
    \item Defines a channel selector for its opcode class
    \item Proves that decoding extracts only receipted payloads
    \item Connects domain-specific claims to kernel $\mu$-accounting
\end{itemize}

\section{The Flagship Divergence Prediction}

\subsection{The "Science Can't Cheat" Theorem}

The flagship prediction derived from the verifier system:

\begin{quote}
\textit{Any pipeline claiming improved predictive power / stronger evaluation / stronger compression must carry an explicit, checkable structure/revelation certificate; otherwise it is vulnerable to undetectable "free insight" failures.}
\end{quote}

\subsection{Implementation}

Representative falsifier test (simplified):
\begin{verbatim}
def test_uncertified_improvement_detected():
    # Attempt to claim better predictions without structure certificate
    result = vm.verify_improvement(baseline, improved, certificate=None)
    assert result.status == "UNCERTIFIED"
    assert "missing revelation" in result.reason
\end{verbatim}

\subsection{Quantitative Bound}

Under admissibility constraint $K$ (bounded $\mu$-information):
\begin{equation}
    \text{certified\_improvement}(\text{transcript}) \le f(K)
\end{equation}

This bound is machine-checked in the formal development and enforced by the verifier. The exact form of $f$ depends on the domain-specific bridge, but the dependency on $K$ is universal: stronger improvements require larger disclosed structure.

\section{Summary}

The verifier system transforms the theoretical No Free Insight principle into practical, falsifiable enforcement:

\begin{enumerate}
    \item \textbf{C-RAND}: You cannot claim certified random bits without paying $\mu$-revelation
    \item \textbf{C-TOMO}: Tighter precision requires proportionally more trials
    \item \textbf{C-ENTROPY}: Entropy is undefined without declared coarse-graining
    \item \textbf{C-CAUSAL}: Unique causal claims require interventions or explicit assumptions
\end{enumerate}

Each module includes forge/underpay/bypass falsifier tests that demonstrate the system correctly rejects attempts to circumvent the No Free Insight principle.

The closed-work system produces cryptographically signed artifacts that enable third-party verification of all claims.
