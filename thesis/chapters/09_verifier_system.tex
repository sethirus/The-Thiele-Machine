\section{The Verifier System: Receipt-Defined Certification}

% ============================================================================
% FIGURE: Chapter Roadmap
% ============================================================================
\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=1.5cm,
    box/.style={rectangle, draw, rounded corners, minimum width=2.5cm, minimum height=0.8cm, align=center, fill=blue!10},
    cmodule/.style={rectangle, draw, rounded corners, minimum width=2cm, minimum height=0.8cm, align=center, fill=green!15},
    arrow/.style={->, >=Stealth, thick}
]
    % Central node
    \node[box, fill=yellow!20, minimum width=4cm] (verifier) at (0,0) {\textbf{Verifier System}\\Receipt-Defined};
    
    % C-modules
    \node[cmodule] (crand) at (-4, 2) {C-RAND\\Randomness};
    \node[cmodule] (ctomo) at (-1.5, 2.5) {C-TOMO\\Tomography};
    \node[cmodule] (centropy) at (1.5, 2.5) {C-ENTROPY\\Entropy};
    \node[cmodule] (ccausal) at (4, 2) {C-CAUSAL\\Causation};
    
    % Ingredients
    \node[box] (trace) at (-4, -1.5) {Trace\\Integrity};
    \node[box] (semantic) at (0, -1.5) {Semantic\\Checking};
    \node[box] (cost) at (4, -1.5) {$\mu$-Cost\\Accounting};
    
    % TRS Protocol
    \node[box, fill=red!15] (trs) at (0, -3) {TRS-1.0\\Receipt Protocol};
    
    % Arrows
    \draw[arrow] (crand) -- (verifier);
    \draw[arrow] (ctomo) -- (verifier);
    \draw[arrow] (centropy) -- (verifier);
    \draw[arrow] (ccausal) -- (verifier);
    
    \draw[arrow] (trace) -- (verifier);
    \draw[arrow] (semantic) -- (verifier);
    \draw[arrow] (cost) -- (verifier);
    
    \draw[arrow] (trs) -- (trace);
    \draw[arrow] (trs) -- (semantic);
    \draw[arrow] (trs) -- (cost);
    
    % Annotations
    \node[font=\scriptsize, text=gray] at (-4, 1.2) {§A.3};
    \node[font=\scriptsize, text=gray] at (-1.5, 1.7) {§A.4};
    \node[font=\scriptsize, text=gray] at (1.5, 1.7) {§A.5};
    \node[font=\scriptsize, text=gray] at (4, 1.2) {§A.6};
\end{tikzpicture}
\caption{Chapter A (Verifier System) roadmap showing the four C-modules and three verification ingredients, all built on the TRS-1.0 receipt protocol.}
\label{fig:ch9-roadmap}
\end{figure}

\subsection{Why Verification Matters}

Scientific claims require evidence. When a researcher claims ``this algorithm produces truly random numbers'' or ``this drug causes improved outcomes,'' I need a way to verify these claims independently. Traditional verification relies on trust: I trust that the researcher ran the experiments correctly, recorded the data accurately, and analyzed it properly.

The Thiele Machine's verifier system replaces trust with \textit{cryptographic proof}. Every claim must be accompanied by a \textbf{receipt}---a tamper-proof record of the computation that produced the claim. Anyone can verify the receipt independently, without trusting the original claimant.

From first principles, a verifier needs three ingredients:
\begin{enumerate}
    \item \textbf{Trace integrity}: a way to bind a claim to a specific execution history.
    \item \textbf{Semantic checking}: a way to re-interpret that history under the model’s rules.
    \item \textbf{Cost accounting}: a way to ensure that any strengthened claim paid the required $\mu$-cost.
\end{enumerate}
The verifier system is built to guarantee all three.
In the codebase, these ingredients are implemented by receipt parsing and signature checks (\path{verifier/receipt_protocol.py}), trace replays in the domain-specific checkers (for example \path{verifier/check_randomness.py}), and explicit $\mu$-cost rules inside the C-modules themselves.

This chapter documents the complete verification infrastructure. The system implements four certification modules (C-modules) that enforce the No Free Insight principle across different application domains:
\begin{itemize}
    \item \textbf{C-RAND}: Certified randomness---proving that bits are truly unpredictable
    \item \textbf{C-TOMO}: Certified estimation---proving that measurements are accurate
    \item \textbf{C-ENTROPY}: Certified entropy---proving that disorder is quantified correctly
    \item \textbf{C-CAUSAL}: Certified causation---proving that causes actually produce effects
\end{itemize}
Each module corresponds to a concrete verifier implementation under \path{verifier/} (for example, \texttt{c\_randomness.py}, \texttt{c\_tomography.py}, \texttt{c\_entropy2.py}, and \texttt{c\_causal.py}). This makes the certification rules auditable and runnable, not just conceptual.

The key insight is that \textit{stronger claims require more evidence}. If you claim high-quality randomness, you must demonstrate the source of that randomness. If you claim precise measurements, you must show enough trials to support that precision. The verifier system makes this relationship explicit and enforceable by turning every claim into a checkable predicate over receipts and by requiring explicit $\mu$-charged disclosures whenever the predicate is strengthened.

\section{Architecture Overview}

% ============================================================================
% FIGURE: TRS-1.0 Receipt Structure
% ============================================================================
\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=0.5cm,
    field/.style={rectangle, draw, minimum width=6cm, minimum height=0.7cm, align=center},
    arrow/.style={->, >=Stealth, thick}
]
    % Receipt structure
    \node[field, fill=blue!10] (version) at (0, 3) {\texttt{version}: "TRS-1.0"};
    \node[field, fill=blue!10] (timestamp) at (0, 2.2) {\texttt{timestamp}: ISO-8601};
    \node[field, fill=green!15, minimum height=1.5cm] (manifest) at (0, 1) {\texttt{manifest}: \{hash → artifact\}};
    \node[field, fill=red!15] (signature) at (0, -0.2) {\texttt{signature}: Ed25519};
    
    % Brace
    \draw[decorate, decoration={brace, amplitude=10pt, mirror}] (3.5, 3.4) -- (3.5, -0.6) node[midway, right=0.5cm, align=left] {Content-addressed\\Signed\\Minimal};
    
    % Properties
    \node[font=\scriptsize] at (-4, 2.5) {Immutable};
    \node[font=\scriptsize] at (-4, 1) {SHA-256};
    \node[font=\scriptsize] at (-4, -0.2) {Tamper-proof};
    
    % Arrows
    \draw[arrow, dashed] (-3.2, 2.5) -- (-3.1, 2.5);
    \draw[arrow, dashed] (-3.2, 1) -- (-3.1, 1);
    \draw[arrow, dashed] (-3.2, -0.2) -- (-3.1, -0.2);
\end{tikzpicture}
\caption{TRS-1.0 Receipt Protocol structure. All artifacts are content-addressed via SHA-256 and signed with Ed25519 for tamper-proof verification.}
\label{fig:trs-receipt}
\end{figure}

\subsection{The Closed Work System}

The verification system is orchestrated through a unified closed-work pipeline that produces verifiable artifacts for each certification module. A ``closed work'' run is one where the verifier only accepts inputs that appear in the receipt manifest; any out-of-band data is ignored.

Each verification includes:
\begin{itemize}
    \item PASS/FAIL/UNCERTIFIED status
    \item Explicit falsifier attempts and outcomes
    \item Declared structure additions (if any)
    \item Complete $\mu$-accounting summary
\end{itemize}

\subsection{The TRS-1.0 Receipt Protocol}

All verification is receipt-defined through the TRS-1.0 (Thiele Receipt Standard) protocol:
\begin{lstlisting}
{
    "version": "TRS-1.0",
    "timestamp": "2025-12-17T00:00:00Z",
    "manifest": {
        "claim.json": "sha256:...",
        "samples.csv": "sha256:...",
        "disclosure.json": "sha256:..."
    },
    "signature": "ed25519:..."
}
\end{lstlisting}

Key properties:
\begin{itemize}
    \item \textbf{Content-addressed}: All artifacts are identified by SHA-256 hash
    \item \textbf{Signed}: Ed25519 signatures prevent tampering
    \item \textbf{Minimal}: Only receipted artifacts can influence verification
\end{itemize}

This protocol supplies the trace integrity requirement: a verifier can recompute hashes and signatures to confirm that the claim is exactly the one produced by the recorded execution.
The full TRS-1.0 specification is in \path{docs/specs/trs-spec-v1.md}, and the reference implementation for verification lives in \path{verifier/receipt_protocol.py} and \path{tools/verify_trs10.py}. This ensures that the protocol described here is backed by a concrete parser and validator.

\subsection{Non-Negotiable Falsifier Pattern}

Every C-module ships three mandatory falsifier tests. Each test targets a distinct failure mode:
\begin{enumerate}
    \item \textbf{Forge test}: Attempt to manufacture receipts without the canonical channel/opcode.
    \item \textbf{Underpay test}: Attempt to obtain the claim while paying fewer $\mu$/info bits.
    \item \textbf{Bypass test}: Route around the channel and confirm rejection.
\end{enumerate}

\section{C-RAND: Certified Randomness}

% ============================================================================
% FIGURE: C-RAND Verification Flow
% ============================================================================
\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=1.5cm,
    box/.style={rectangle, draw, rounded corners, minimum width=2.5cm, minimum height=0.8cm, align=center, fill=blue!10},
    check/.style={diamond, draw, aspect=2, fill=yellow!20, align=center},
    arrow/.style={->, >=Stealth, thick}
]
    % Flow
    \node[box] (input) at (0, 0) {Randomness\\Claim};
    \node[check] (receipt) at (3, 0) {In TRS?};
    \node[check] (entropy) at (6, 0) {$H_{min}$\\evidence?};
    \node[box, fill=green!20] (pass) at (9, 0.8) {PASS};
    \node[box, fill=red!20] (fail) at (9, -0.8) {REJECT};
    
    % Arrows
    \draw[arrow] (input) -- (receipt);
    \draw[arrow] (receipt) -- node[above, font=\scriptsize] {Yes} (entropy);
    \draw[arrow] (receipt.south) -- ++(0, -0.5) -| node[near start, below, font=\scriptsize] {No} (fail);
    \draw[arrow] (entropy) -- node[above, font=\scriptsize] {Yes} (pass);
    \draw[arrow] (entropy.south) -- ++(0, -0.3) -| (fail);
    
    % Cost equation
    \node[draw, rounded corners, fill=gray!10, font=\small] at (4.5, -2) {Required disclosure: $\lceil 1024 \cdot H_{min} \rceil$ bits};
\end{tikzpicture}
\caption{C-RAND verification flow. Claims must be receipt-bound and provide min-entropy evidence proportional to claimed quality.}
\label{fig:crand-flow}
\end{figure}

\subsection{Claim Structure}

A randomness claim specifies:
\begin{lstlisting}
{
    "n_bits": 1024,
    "min_entropy_per_bit": 0.95
}
\end{lstlisting}

\subsection{Verification Rules}

The randomness verifier enforces:
\begin{itemize}
    \item Every input must appear in the TRS-1.0 receipt manifest
    \item Min-entropy claims require explicit nonlocality/disclosure evidence
    \item Required disclosure bits: $\lceil 1024 \cdot H_{min} \rceil$
\end{itemize}

Why these rules? Because without a receipt-bound source, the verifier has no basis for trusting the bits, and without disclosure evidence, the claim could be strengthened without paying the structural cost.

\subsection{The Randomness Bound}

Formal bridge lemma (illustrative):
\begin{lstlisting}
Definition RandChannel (r : Receipt) : bool :=
  Nat.eqb (r_op r) RAND_TRIAL_OP.

Lemma decode_is_filter_payloads :
  forall tr,
    decode RandChannel tr = map r_payload (filter RandChannel tr).
\end{lstlisting}

This ensures that randomness claims are derived only from receipted trial data. In other words, the verifier can only compute a randomness predicate over the receipts it can check.

\subsection{Falsifier Tests}

\begin{itemize}
    \item \textbf{Forge}: Create receipts claiming high entropy without running trials $\rightarrow$ REJECTED
    \item \textbf{Underpay}: Claim $H_{min} = 0.99$ but provide only $H_{min} = 0.5$ disclosure $\rightarrow$ REJECTED
    \item \textbf{Bypass}: Submit raw bits without receipt chain $\rightarrow$ UNCERTIFIED
\end{itemize}

\section{C-TOMO: Tomography as Priced Knowledge}

\subsection{Claim Structure}

A tomography claim specifies an estimate within tolerance:
\begin{lstlisting}
{
    "estimate": 0.785,
    "epsilon": 0.01,
    "n_trials": 10000
}
\end{lstlisting}

\subsection{Verification Rules}

The tomography verifier enforces:
\begin{itemize}
    \item Trial count must match receipted samples
    \item Tighter $\epsilon$ requires more trials (cost rule)
    \item Statistical consistency checks on estimate derivation
\end{itemize}

These rules embody a first-principles trade-off: precision is information, and information requires evidence. The verifier therefore couples $\epsilon$ to a minimum sample size and rejects claims that underpay the evidence requirement.

\subsection{The Precision-Cost Relationship}

Estimation precision is priced: tighter $\epsilon$ requires proportionally more evidence:
\begin{equation}
    n_{required} \ge c \cdot \epsilon^{-2}
\end{equation}

where $c$ is a domain-specific constant.

\section{C-ENTROPY: Coarse-Graining Made Explicit}

% ============================================================================
% FIGURE: Entropy Coarse-Graining
% ============================================================================
\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=1cm,
    state/.style={circle, draw, minimum size=0.3cm, fill=blue!20},
    partition/.style={rectangle, draw, dashed, rounded corners, minimum width=1.5cm, minimum height=1cm}
]
    % Raw states (infinite equivalence class)
    \node[font=\small\bfseries] at (-3, 2) {Raw State Space};
    \foreach \i in {1,...,12} {
        \node[state, fill=blue!\the\numexpr20+\i*5\relax] at ({-4+mod(\i-1,4)*0.6}, {1.5-floor((\i-1)/4)*0.5}) {};
    }
    \node[font=\scriptsize] at (-3, 0) {$|\Omega| = \infty$};
    
    % Arrow
    \draw[->, >=Stealth, thick, decorate, decoration={snake, amplitude=2pt, segment length=10pt}] (-1, 1) -- (1, 1) node[midway, above, font=\scriptsize] {Coarse-grain};
    
    % Partitioned states
    \node[font=\small\bfseries] at (3, 2) {With Partition};
    \node[partition, fill=red!10] (p1) at (2, 1) {};
    \node[partition, fill=green!10] (p2) at (3.5, 1) {};
    \node[partition, fill=blue!10] (p3) at (2.75, 0) {};
    
    \foreach \i in {1,...,3} {
        \node[state, fill=red!40] at ({1.7+(\i-1)*0.3}, 1) {};
    }
    \foreach \i in {1,...,3} {
        \node[state, fill=green!40] at ({3.2+(\i-1)*0.3}, 1) {};
    }
    \foreach \i in {1,...,4} {
        \node[state, fill=blue!40] at ({2.3+(\i-1)*0.3}, 0) {};
    }
    
    \node[font=\scriptsize] at (3, -0.8) {$H = \log_2(|\text{bins}|)$};
    
    % Key insight
    \node[draw, rounded corners, fill=yellow!20, font=\small, text width=5cm, align=center] at (0, -1.8) {Entropy is \textbf{undefined} without declared coarse-graining};
\end{tikzpicture}
\caption{Entropy requires explicit coarse-graining. The infinite raw state space has undefined entropy; only partitioned views have computable entropy.}
\label{fig:entropy-coarse}
\end{figure}

\subsection{The Entropy Underdetermination Problem}

Entropy is ill-defined without specifying a coarse-graining (partition). Two observers with different partitions will compute different entropies for the same physical state. A verifier therefore treats the coarse-graining itself as part of the claim and requires it to be receipted.

\subsection{Claim Structure}

An entropy claim must declare its coarse-graining:
\begin{lstlisting}
{
    "h_lower_bound_bits": 3.2,
    "n_samples": 5000,
    "coarse_graining": {
        "type": "histogram",
        "bins": 32
    }
}
\end{lstlisting}

\subsection{Verification Rules}

The entropy verifier enforces:
\begin{itemize}
    \item Entropy claims without declared coarse-graining $\rightarrow$ REJECTED
    \item Coarse-graining must be in receipted manifest
    \item Disclosure bits scale with entropy bound: $\lceil 1024 \cdot H \rceil$
\end{itemize}

The rationale is direct: entropy is a function of a partition, and the partition itself is structural information that must be paid for.

\subsection{Coq Formalization}

Formal impossibility lemma (illustrative):
\begin{lstlisting}
Theorem region_equiv_class_infinite : forall s,
  exists f : nat -> VMState,
    (forall n, region_equiv s (f n)) /\
    (forall n1 n2, f n1 = f n2 -> n1 = n2).
\end{lstlisting}

This proves that observational equivalence classes are infinite, blocking entropy computation without explicit coarse-graining. In practice, the verifier uses this impossibility result to reject entropy claims that omit a receipted partition.

\section{C-CAUSAL: No Free Causal Explanation}

% ============================================================================
% FIGURE: Causal DAG Markov Equivalence
% ============================================================================
\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=1.5cm,
    var/.style={circle, draw, minimum size=0.8cm, fill=blue!20},
    arrow/.style={->, >=Stealth, thick}
]
    % Equivalence class
    \node[font=\small\bfseries] at (-3, 2) {Markov Equivalence Class};
    
    % DAG 1
    \node[var] (a1) at (-4.5, 0.5) {A};
    \node[var] (b1) at (-3, 0.5) {B};
    \node[var] (c1) at (-3.75, -0.5) {C};
    \draw[arrow] (a1) -- (b1);
    \draw[arrow] (a1) -- (c1);
    \draw[arrow] (b1) -- (c1);
    
    % DAG 2
    \node[var] (a2) at (-1.5, 0.5) {A};
    \node[var] (b2) at (0, 0.5) {B};
    \node[var] (c2) at (-0.75, -0.5) {C};
    \draw[arrow] (b2) -- (a2);
    \draw[arrow] (a2) -- (c2);
    \draw[arrow] (b2) -- (c2);
    
    % DAG 3
    \node[var] (a3) at (1.5, 0.5) {A};
    \node[var] (b3) at (3, 0.5) {B};
    \node[var] (c3) at (2.25, -0.5) {C};
    \draw[arrow] (a3) -- (b3);
    \draw[arrow] (c3) -- (a3);
    \draw[arrow] (c3) -- (b3);
    
    % Equals signs
    \node at (-2.25, 0) {$\equiv$};
    \node at (0.75, 0) {$\equiv$};
    
    % Annotation
    \node[draw, rounded corners, fill=red!15, font=\small, text width=6cm, align=center] at (-0.75, -2) {Observational data \textbf{cannot} distinguish these DAGs\\Unique DAG claim requires 8192 disclosure bits};
\end{tikzpicture}
\caption{Markov equivalence: multiple DAGs produce identical observational distributions. Unique causal claims require interventional evidence or explicit assumptions.}
\label{fig:markov-equiv}
\end{figure}

\subsection{The Causal Inference Problem}

Claiming a unique causal DAG from observational data alone is impossible in general (Markov equivalence classes contain multiple DAGs). Stronger-than-observational claims require explicit assumptions or interventional evidence, and those assumptions are themselves structure that must be disclosed and charged.

\subsection{Claim Types}

\begin{itemize}
    \item \texttt{unique\_dag}: Claims a unique causal graph (requires 8192 disclosure bits)
    \item \texttt{ate}: Claims average treatment effect (requires 2048 disclosure bits)
\end{itemize}

\subsection{Verification Rules}

The causal verifier enforces:
\begin{itemize}
    \item \texttt{unique\_dag} claims require \texttt{assumptions.json} or \texttt{interventions.csv}
    \item Intervention count must match receipted data
    \item Pure observational data cannot certify unique DAGs
\end{itemize}

\subsection{Falsifier Tests}

\begin{lstlisting}
def test_unique_dag_without_assumptions_rejected():
    # Claim unique DAG from pure observational data
    # Must be rejected: causal claims need extra structure
    result = verify_causal(run_dir, trust_manifest)
    assert result.status == "REJECTED"
\end{lstlisting}

\section{Bridge Modules: Kernel Integration}

The verifier system includes bridge lemmas connecting application domains to the kernel. Each bridge supplies:
\begin{itemize}
    \item a channel selector for the opcode class,
    \item a decoding lemma that extracts only receipted payloads,
    \item a proof that domain-specific claims incur the corresponding $\mu$-cost.
\end{itemize}

This is the semantic checking requirement: the verifier can only interpret what the kernel would accept, and any domain-specific claim is reduced to a kernel-level obligation.

Each bridge:
\begin{itemize}
    \item Defines a channel selector for its opcode class
    \item Proves that decoding extracts only receipted payloads
    \item Connects domain-specific claims to kernel $\mu$-accounting
\end{itemize}

\section{The Flagship Divergence Prediction}

\subsection{The "Science Can't Cheat" Theorem}

The flagship prediction derived from the verifier system:

\begin{quote}
\textit{Any pipeline claiming improved predictive power / stronger evaluation / stronger compression must carry an explicit, checkable structure/revelation certificate; otherwise it is vulnerable to undetectable "free insight" failures.}
\end{quote}

\subsection{Implementation}

Representative falsifier test (simplified):
\begin{lstlisting}
def test_uncertified_improvement_detected():
    # Attempt to claim better predictions without structure certificate
    result = vm.verify_improvement(baseline, improved, certificate=None)
    assert result.status == "UNCERTIFIED"
    assert "missing revelation" in result.reason
\end{lstlisting}

\subsection{Quantitative Bound}

Under admissibility constraint $K$ (bounded $\mu$-information):
\begin{equation}
    \text{certified\_improvement}(\text{transcript}) \le f(K)
\end{equation}

This bound is machine-checked in the formal development and enforced by the verifier. The exact form of $f$ depends on the domain-specific bridge, but the dependency on $K$ is universal: stronger improvements require larger disclosed structure.

\section{Summary}

% ============================================================================
% FIGURE: Chapter Summary
% ============================================================================
\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=1.5cm,
    cmodule/.style={rectangle, draw, rounded corners, minimum width=3cm, minimum height=1cm, align=center, fill=green!15},
    principle/.style={rectangle, draw, rounded corners, minimum width=5cm, minimum height=1cm, align=center, fill=yellow!20},
    arrow/.style={->, >=Stealth, thick}
]
    % C-modules
    \node[cmodule] (crand) at (-3, 2) {C-RAND\\$\mu$-revelation for bits};
    \node[cmodule] (ctomo) at (3, 2) {C-TOMO\\$n \propto \epsilon^{-2}$};
    \node[cmodule] (centropy) at (-3, 0) {C-ENTROPY\\Coarse-graining required};
    \node[cmodule] (ccausal) at (3, 0) {C-CAUSAL\\Interventions for DAGs};
    
    % Central principle
    \node[principle] (nfi) at (0, -2) {\textbf{No Free Insight}\\Stronger claims require more evidence};
    
    % Arrows
    \draw[arrow] (crand) -- (nfi);
    \draw[arrow] (ctomo) -- (nfi);
    \draw[arrow] (centropy) -- (nfi);
    \draw[arrow] (ccausal) -- (nfi);
    
    % Falsifier pattern
    \node[draw, rounded corners, fill=gray!10, font=\scriptsize, text width=4cm, align=center] at (0, -4) {Each module includes\\Forge / Underpay / Bypass\\falsifier tests};
\end{tikzpicture}
\caption{Chapter A summary: Four C-modules transform No Free Insight into practical, falsifiable enforcement.}
\label{fig:ch9-summary}
\end{figure}

The verifier system transforms the theoretical No Free Insight principle into practical, falsifiable enforcement:

\begin{enumerate}
    \item \textbf{C-RAND}: Certified random bits require paying $\mu$-revelation
    \item \textbf{C-TOMO}: Tighter precision requires proportionally more trials
    \item \textbf{C-ENTROPY}: Entropy is undefined without declared coarse-graining
    \item \textbf{C-CAUSAL}: Unique causal claims require interventions or explicit assumptions
\end{enumerate}

Each module includes forge/underpay/bypass falsifier tests that demonstrate the system correctly rejects attempts to circumvent the No Free Insight principle.

The closed-work system produces cryptographically signed artifacts that enable third-party verification of all claims.
