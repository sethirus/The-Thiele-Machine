\section{Evaluation Overview}

\subsection{From Theory to Evidence}

The previous chapters established the \textit{theoretical} foundations of the Thiele Machine: definitions, proofs, and implementations. But theoretical correctness is not sufficient---I must also demonstrate that the theory \textit{works in practice}. Evaluation has a different role than proof: it does not establish truth for all inputs, but it validates that implementations faithfully realize the formal semantics and that the predicted invariants hold under realistic workloads.

This chapter presents empirical evaluation addressing three fundamental questions:
\begin{enumerate}
    \item \textbf{Does the 3-layer isomorphism actually hold?} \\
    The theory claims that Coq, Python, and Verilog implementations produce identical results. I test this claim on thousands of instruction sequences, including randomized traces and structured micro-programs designed to stress the ISA.
    
    \item \textbf{Does the revelation requirement actually enforce costs?} \\
    The theory claims that supra-quantum correlations require explicit revelation. I run CHSH experiments to verify this constraint is enforced and that the ledger charges match the structure disclosed.
    
    \item \textbf{Is the implementation practical?} \\
    A beautiful theory that runs too slowly is useless. I benchmark performance and resource utilization to assess practicality, focusing on the overhead of receipts and the hardware cost of the accounting units.
\end{enumerate}

\subsection{Methodology}

All experiments follow scientific best practices:
\begin{itemize}
    \item \textbf{Reproducibility}: Every experiment can be re-run from the published artifacts and trace descriptions
    \item \textbf{Automation}: Tests are automated in a continuous validation pipeline
    \item \textbf{Adversarial testing}: I actively try to break the system, not just confirm it works
\end{itemize}

All experiments use the reference VM with receipt generation enabled. Each run produces receipts and state snapshots so that results can be rechecked independently. The emphasis is on \textit{replayability}: anyone can take the same trace, replay it through each layer, and confirm equality of the observable projection.
The concrete test harnesses live under \texttt{tests/} (for example, \texttt{tests/test\_partition\_isomorphism\_minimal.py} and \texttt{tests/test\_rtl\_compute\_isomorphism.py}), so the evaluation is tied to executable scripts rather than hand-run examples.

\section{3-Layer Isomorphism Verification}

\subsection{Test Architecture}

The isomorphism gate verifies that Python VM, extracted Coq semantics, and RTL simulation produce identical final states for the same instruction traces. The comparison uses suite-specific projections rather than a single fixed snapshot: compute traces compare registers and memory, while partition traces compare canonicalized module regions. The extracted runner emits a superset JSON snapshot (pc, $\mu$, err, regs, mem, CSRs, graph), whereas the RTL testbench emits a smaller JSON object tailored to the gate under test. The purpose of each projection is to compare only the declared observables relevant to that trace type and ignore internal bookkeeping fields.

\subsubsection{Test Implementation}

Representative test (simplified):
\begin{lstlisting}
def test_rtl_python_coq_compute_isomorphism():
    # Small, deterministic compute program.
    # Semantics must match across:
    #   - Python reference VM
    #   - extracted formal semantics runner
    #   - RTL simulation
    
    init_mem[0] = 0x29
    init_mem[1] = 0x12
    init_mem[2] = 0x22
    init_mem[3] = 0x03
    
    program_words = [
        _encode_word(0x0A, 0, 0),  # XOR_LOAD r0 <= mem[0]
        _encode_word(0x0A, 1, 1),  # XOR_LOAD r1 <= mem[1]
        _encode_word(0x0A, 2, 2),  # XOR_LOAD r2 <= mem[2]
        _encode_word(0x0A, 3, 3),  # XOR_LOAD r3 <= mem[3]
        _encode_word(0x0B, 3, 0),  # XOR_ADD r3 ^= r0
        _encode_word(0x0B, 3, 1),  # XOR_ADD r3 ^= r1
        _encode_word(0x0C, 0, 3),  # XOR_SWAP r0 <-> r3
        _encode_word(0x07, 2, 4),  # XFER r4 <- r2
        _encode_word(0x0D, 5, 4),  # XOR_RANK r5 := popcount(r4)
        _encode_word(0xFF, 0, 0),  # HALT
    ]
    
    py_regs, py_mem = _run_python_vm(init_mem, init_regs, program_text)
    coq_regs, coq_mem = _run_extracted(init_mem, init_regs, trace_lines)
    rtl_regs, rtl_mem = _run_rtl(program_words, data_words)
    
    assert py_regs == coq_regs == rtl_regs
    assert py_mem == coq_mem == rtl_mem
\end{lstlisting}

\subsubsection{State Projection}

Final states are projected to canonical form:
\begin{lstlisting}
{
  "pc": <int>,
  "mu": <int>,
  "err": <bool>,
  "regs": [<32 integers>],
  "mem": [<256 integers>],
  "csrs": {"cert_addr": ..., "status": ..., "error": ...},
  "graph": {"modules": [...]}
}
\end{lstlisting}

\subsection{Partition Operation Tests}

Representative test (simplified):
\begin{lstlisting}
def test_pnew_dedup_singletons_isomorphic():
    # Same singleton regions requested multiple times; canonical semantics dedup.
    indices = [0, 1, 2, 0, 1]  # Duplicates
    
    py_regions = _python_regions_after_pnew(indices)
    coq_regions = _coq_regions_after_pnew(indices)
    rtl_regions = _rtl_regions_after_pnew(indices)
    
    assert py_regions == coq_regions == rtl_regions
\end{lstlisting}

This verifies that canonical normalization produces identical results across all layers, which is essential because partitions are represented as lists but compared modulo ordering and duplicates.
In the formal kernel, the normalization function is \texttt{normalize\_region} (based on \texttt{nodup}), so this test is checking that the Python and RTL representations match the Coq canonicalization rather than relying on a coincidental list order.

\subsection{Results Summary}

\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Test Suite} & \textbf{Python} & \textbf{Coq} & \textbf{RTL} \\
\hline
Compute Operations & PASS & PASS & PASS \\
Partition PNEW & PASS & PASS & PASS \\
Partition PSPLIT & PASS & PASS & PASS \\
Partition PMERGE & PASS & PASS & PASS \\
XOR Operations & PASS & PASS & PASS \\
$\mu$-Ledger Updates & PASS & PASS & PASS \\
\hline
\textbf{Total} & 100\% & 100\% & 100\% \\
\hline
\end{tabular}
\end{center}

\section{CHSH Correlation Experiments}

\subsection{Bell Test Protocol}

The CHSH inequality bounds correlations in local realistic theories. For measurement settings $x,y \in \{0,1\}$ and outcomes $a,b \in \{0,1\}$, define
\[
E(x,y) = \Pr[a=b \mid x,y] - \Pr[a \neq b \mid x,y].
\]
Then:
\begin{equation}
    S = |E(a,b) - E(a,b') + E(a',b) + E(a',b')| \le 2
\end{equation}

Quantum mechanics predicts $S_{\max} = 2\sqrt{2} \approx 2.828$ (Tsirelson's bound).

\subsection{Partition-Native CHSH}

The Thiele Machine implements CHSH trials through the \texttt{CHSH\_TRIAL} instruction:
\begin{lstlisting}
instr_chsh_trial (x y a b : nat) (mu_delta : nat)
\end{lstlisting}

Where:
\begin{itemize}
    \item \texttt{x, y}: Input bits (setting choices)
    \item \texttt{a, b}: Output bits (measurement outcomes)
    \item \texttt{mu\_delta}: $\mu$-cost for the trial
\end{itemize}

\subsection{Correlation Bounds}

The implementation enforces a Tsirelson bound:
\begin{lstlisting}
from fractions import Fraction

TSIRELSON_BOUND: Fraction = Fraction(5657, 2000)  # ~2.8285

def is_supra_quantum(*, chsh: Fraction, bound: Fraction = TSIRELSON_BOUND) -> bool:
    return chsh > bound

DEFAULT_ENFORCEMENT_MIN_TRIALS_PER_SETTING = 100
\end{lstlisting}
The implementation uses a conservative rational bound (\texttt{5657/2000}) rather than a floating approximation to make proof and test comparisons exact across layers.

\subsection{Experimental Design}

The CHSH evaluation pipeline:
\begin{enumerate}
    \item Generate CHSH trial sequences
    \item Execute on Python VM with receipt generation
    \item Compute $S$ value from outcome statistics
    \item Verify $\mu$-cost matches declared cost
    \item Verify receipt chain integrity
\end{enumerate}
The pipeline is mirrored in test utilities such as \texttt{tools/finite\_quantum.py} and \texttt{tests/test\_supra\_revelation\_semantics.py}, which compute the same CHSH statistics and check the revelation rule against the formal kernel's expectations.

\subsection{Supra-Quantum Certification}

To certify $S > 2\sqrt{2}$, the trace must include a revelation event:
\begin{lstlisting}
Theorem nonlocal_correlation_requires_revelation :
  forall (trace : Trace) (s_init s_final : VMState) (fuel : nat),
    trace_run fuel trace s_init = Some s_final ->
    s_init.(vm_csrs).(csr_cert_addr) = 0 ->
    has_supra_cert s_final ->
    uses_revelation trace \/ ...
\end{lstlisting}
The theorem shown here is proven in \texttt{coq/kernel/RevelationRequirement.v}. The evaluation checks the operational side of that theorem by building traces that attempt to exceed the bound without \texttt{REVEAL} and confirming that the machine marks them invalid or charges the appropriate $\mu$.

Experimental verification confirms:
\begin{itemize}
    \item Traces with $S \le 2$ do not require revelation
    \item Traces with $2 < S \le 2\sqrt{2}$ may use revelation
    \item Traces claiming $S > 2\sqrt{2}$ \textbf{must} use revelation
\end{itemize}

\subsection{Results}

\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Regime} & \textbf{$S$ Value} & \textbf{Revelation} & \textbf{$\mu$-Cost} \\
\hline
Local Realistic & $\le 2.0$ & Not required & 0 \\
Classical Shared & $\le 2.0$ & Not required & $\mu_{\text{seed}}$ \\
Quantum & $\le 2.828$ & Optional & $\mu_{\text{corr}}$ \\
Supra-Quantum & $> 2.828$ & \textbf{Required} & $\mu_{\text{reveal}}$ \\
\hline
\end{tabular}
\end{center}

\section{$\mu$-Ledger Verification}

\subsection{Monotonicity Tests}

Representative monotonicity check:
\begin{lstlisting}
def test_mu_monotonic_under_any_trace():
    for _ in range(100):
        trace = generate_random_trace(length=50)
        vm = VM(State())
        vm.run(trace)
        
        mu_values = [s.mu for s in vm.trace]
        for i in range(1, len(mu_values)):
            assert mu_values[i] >= mu_values[i-1]
\end{lstlisting}
The monotonicity check mirrors the formal lemma that \texttt{vm\_mu} never decreases under \texttt{vm\_step}. In the Python VM, the ledger is split into \texttt{mu\_discovery} and \texttt{mu\_execution} (see \texttt{MuLedger} in \texttt{thielecpu/state.py}), so the test verifies that their total is non-decreasing step by step.

\subsection{Conservation Tests}

Representative conservation check:
\begin{lstlisting}
def test_mu_conservation():
    program = [
        ("PNEW", "{0,1,2,3}"),
        ("PSPLIT", "1 {0,1} {2,3}"),
        ("PMERGE", "2 3"),
        ("HALT", ""),
    ]
    
    vm = VM(State())
    vm.run(program)
    
    total_declared = sum(instr.cost for instr in program)
    assert vm.state.mu_ledger.total == total_declared
\end{lstlisting}
The conservation test matches the formal definition of \texttt{apply\_cost} in \texttt{coq/kernel/VMStep.v}, which adds the per-instruction \texttt{mu\_delta} to the running ledger. The experiment is therefore a concrete replay of the same rule used in the proofs.

\subsection{Results}

\begin{itemize}
    \item \textbf{Monotonicity}: 100\% of random traces maintain $\mu_{t+1} \ge \mu_t$
    \item \textbf{Conservation}: Declared costs exactly match ledger increments
    \item \textbf{Irreversibility}: Ledger growth bounds irreversible operations
\end{itemize}

\section{Thermodynamic bridge experiment (publishable plan)}

To connect the ledger to a physical observable, I design a narrowly scoped, falsifiable experiment focused on measurement/erasure thermodynamics.

\subsection{Workload construction}
Use the thermodynamic bridge harness to emit four traces that differ only in which singleton module is revealed from a fixed candidate pool: (1) choose 1 of 2 elements, (2) choose 1 of 4, (3) choose 1 of 16, (4) choose 1 of 64. Instruction count, data size, and clocking remain identical so that only the $\Omega \to \Omega'$ reduction changes. The bundle records per-step $\mu$ (raw and normalized), $|\Omega|$, $|\Omega'|$, normalization flags for the formal, reference, and hardware layers, and an `evidence\_strict` bit indicating whether normalization was allowed.

\subsection{Bridge prediction}
By construction $\mu \ge \log_2(|\Omega|/|\Omega'|)$ for each trace. Under the thermodynamic postulate $Q_{\min} = k_B T \ln 2 \cdot \mu$, measured energy/heat must scale with $\mu$ at slope $k_B T \ln 2$ (within an explicit inefficiency factor $\epsilon$). Genesis-only traces remain the lone legitimate zero-$\mu$ run; a zero $\mu$ on any nontrivial trace is treated as a test failure, not “alignment.”

\subsection{Instrumentation and analysis}
Run the three traces on instrumented hardware (or a calibrated switching-energy simulator) at fixed temperature $T$. Record per-run energy and environmental metadata. Fit measured energy against $k_B T \ln 2 \cdot \mu$ and report residuals. A sustained sub-linear slope falsifies the bridge; a super-linear slope quantifies overhead. Publish both ledger outputs and raw measurements so reviewers can recompute the bound.

\subsection{Executed thermodynamic bundle (Dec 2025)}
I executed the four $\Omega \to \Omega'$ traces with the bridge harness, exporting a JSON artifact. The runs charge $\mu$ via partition discovery only (explicit \texttt{MDLACC} omitted to mirror the hardware harness) and capture normalization flags and \texttt{evidence\_strict} for $\mu$ propagation across layers. Each scenario fails fast if the requested region is not representable by the hardware encoding. These runs are intended to validate that the ledger and trace machinery produce consistent, reproducible $\mu$ values that a future physical experiment can bind to energy.

\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
Scenario & $\mu_{\text{python}}$ & $\mu_{\text{raw,extracted}}$ / $\mu_{\text{raw,rtl}}$ & Normalized? & $\log_2(|\Omega|/|\Omega'|)$ & $k_B T \ln 2 \cdot \mu$ (J) & $\mu / \log_2(|\Omega|/|\Omega'|)$ \\
\hline
singleton\_from\_2 & 2 & 2 / 2 & no & 1 & $5.74 \times 10^{-21}$ & 2.0 \\
singleton\_from\_4 & 3 & 3 / 3 & no & 2 & $8.61 \times 10^{-21}$ & 1.5 \\
singleton\_from\_16 & 5 & 5 / 5 & no & 4 & $1.44 \times 10^{-20}$ & 1.25 \\
singleton\_from\_64 & 7 & 7 / 7 & no & 6 & $2.02 \times 10^{-20}$ & 1.167 \\
\hline
\end{tabular}
\end{center}

All four traces satisfy $\mu \ge \log_2(|\Omega|/|\Omega'|)$ and align on regs/mem/$\mu$ without normalization. The harness encodes an explicit $\mu$-delta into the formal trace and hardware instruction word, and the reference VM consumes the same $\mu$-delta (disabling implicit MDLACC) so that $\mu_{\text{raw}}$ matches across layers. With this encoding in place, \texttt{EVIDENCE\_STRICT} runs succeed for these workloads.

\subsection{Structural heat anomaly workload}
To mirror the thesis claim that structured insight carries binding energy, the structural-heat harness executes two erase tasks over the same 1~GiB buffer: \texttt{erase\_random\_noise} (no certificate) and \texttt{erase\_structured\_sorted} (explicit $\log_2(n!)$ certificate for $n=2^{20}$ sorted records). The emitted artifact records $\mu$ totals, Landauer lower bounds, and slack for both workloads. The structured erase pays dramatically larger $\mu$---raising the Landauer floor accordingly---while keeping instruction count and data volume fixed, turning the ``structural heat'' prediction into a measurable differential once hardware power logging is connected.

\subsection{Ledger-constrained time dilation workload}
\label{sec:ledger_time_dilation}
To test the ``speed limit'' claim, the time-dilation harness fixes a $\mu$ budget per global tick (32 $\mu$-bits) and varies only the communication payload queued each tick before local computation is allowed. The emitted JSON artifact reports four scenarios: no communication (32 compute steps/tick), light (28 steps/tick), moderate (20 steps/tick), and heavy (8 steps/tick) while holding the per-tick $\mu$ spend constant at 2048 $\mu$ over 64 ticks. As communication $\mu$ increases, the compute rate monotonically falls, demonstrating that signal propagation consumes the same finite $\mu$ budget that would otherwise power local evolution---a ledger-level analogue of time dilation under a fixed speed-of-ledger constraint. Evidence-strict extensions can wire this harness to EMIT traces and RTL exports to enforce the same trade-off across layers.

\section{Performance Benchmarks}

\subsection{Instruction Throughput}

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Mode} & \textbf{Ops/sec} & \textbf{Overhead} \\
\hline
Raw Python VM & $\sim 10^6$ & Baseline \\
Receipt Generation & $\sim 10^4$ & 100$\times$ \\
Full Tracing & $\sim 10^3$ & 1000$\times$ \\
\hline
\end{tabular}
\end{center}

\subsection{Receipt Chain Overhead}

Each step generates:
\begin{itemize}
    \item Pre-state SHA-256 hash: 32 bytes
    \item Post-state SHA-256 hash: 32 bytes
    \item Instruction encoding: $\sim$50 bytes
    \item Chain link: 32 bytes
\end{itemize}

Total per-step overhead: $\sim$150 bytes

\subsection{Hardware Synthesis Results}

\textbf{YOSYS\_LITE Configuration:}
\begin{lstlisting}
NUM_MODULES = 4
REGION_SIZE = 16
\end{lstlisting}
\begin{itemize}
    \item LUTs: $\sim$2,500
    \item Flip-Flops: $\sim$1,200
    \item Target: Xilinx 7-series
\end{itemize}

\textbf{Full Configuration:}
\begin{lstlisting}
NUM_MODULES = 64
REGION_SIZE = 1024
\end{lstlisting}
\begin{itemize}
    \item LUTs: $\sim$45,000
    \item Flip-Flops: $\sim$35,000
    \item Target: Xilinx UltraScale+
\end{itemize}

\section{Validation Coverage}

\subsection{Test Categories}

The evaluation suite is organized by the kinds of claims it is meant to stress:

\begin{itemize}
    \item \textbf{Isomorphism tests}: cross-layer equality of the observable state projection.
    \item \textbf{Partition operations}: normalization, split/merge preconditions, and canonical region equality.
    \item \textbf{$\mu$-ledger tests}: monotonicity, conservation, and irreversibility lower bounds.
    \item \textbf{CHSH/Bell tests}: enforcement of correlation bounds and revelation requirements.
    \item \textbf{Receipt verification}: signature integrity and step-by-step replay.
    \item \textbf{Adversarial tests}: malformed traces and invalid certificates.
    \item \textbf{Performance benchmarks}: throughput with and without receipts.
\end{itemize}

\subsection{Automation}

The evaluation pipeline is automated: each change is checked against proof compilation, isomorphism gates, and verification policy checks to prevent semantic drift.
The fast local gates are the same ones described in the repository workflow: \texttt{make -C coq core} and the two isomorphism pytest suites. When the full hardware toolchain is present, the synthesis gate (\texttt{scripts/forge\_artifact.sh}) adds a hardware-level check.

\subsection{Execution Gates}

The fast local gates are proof compilation and the two isomorphism tests. The full foundry gate adds synthesis when the hardware toolchain is available.

\section{Reproducibility}

\subsection{Artifact Bundles}

Key artifacts include:
\begin{itemize}
    \item 3-way comparison results
    \item Cross-platform isomorphism summaries
    \item Synthesis reports
    \item Content hashes for artifact bundles
\end{itemize}

\subsection{Container Reproducibility}

Containerized builds are supported to ensure reproducibility across environments.

\section{Summary}

The evaluation demonstrates:
\begin{enumerate}
    \item \textbf{3-Layer Isomorphism}: Python, Coq extraction, and RTL produce identical state projections for all tested instruction sequences
    \item \textbf{CHSH Correctness}: Supra-quantum certification requires revelation as predicted by theory
    \item \textbf{$\mu$-Conservation}: The ledger is monotonic and exactly tracks declared costs
    \item \textbf{Scalability}: Hardware synthesis targets modern FPGAs with reasonable resource utilization
    \item \textbf{Reproducibility}: All results can be reproduced from the published traces and artifact bundles
\end{enumerate}

The empirical results validate the theoretical claims: the Thiele Machine enforces structural accounting as a physical law, not merely as a convention.
