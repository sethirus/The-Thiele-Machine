\section{Evaluation Overview}

\subsection{From Theory to Evidence}

The previous chapters established the \textit{theoretical} foundations of the Thiele Machine: definitions, proofs, and implementations. But theoretical correctness is not sufficient---we must also demonstrate that the theory \textit{works in practice}.

This chapter presents empirical evaluation addressing three fundamental questions:
\begin{enumerate}
    \item \textbf{Does the 3-layer isomorphism actually hold?} \\
    The theory claims that Coq, Python, and Verilog implementations produce identical results. We test this claim on thousands of instruction sequences.
    
    \item \textbf{Does the revelation requirement actually enforce costs?} \\
    The theory claims that supra-quantum correlations require explicit revelation. We run CHSH experiments to verify this constraint is enforced.
    
    \item \textbf{Is the implementation practical?} \\
    A beautiful theory that runs too slowly is useless. We benchmark performance and resource utilization to assess practicality.
\end{enumerate}

\subsection{Methodology}

All experiments follow scientific best practices:
\begin{itemize}
    \item \textbf{Reproducibility}: Every experiment can be re-run from the repository
    \item \textbf{Automation}: Tests are automated in the CI pipeline
    \item \textbf{Adversarial testing}: We actively try to break the system, not just confirm it works
\end{itemize}

All experiments use the Python Reference VM with receipt generation enabled. Results are reproducible via the test suite in \texttt{tests/}.

\section{3-Layer Isomorphism Verification}

\subsection{Test Architecture}

The isomorphism gate verifies that Python VM, extracted Coq semantics, and RTL simulation produce identical final states for the same instruction traces.

\subsubsection{Test Implementation}

From \texttt{tests/test\_rtl\_compute\_isomorphism.py}:
\begin{verbatim}
def test_rtl_python_coq_compute_isomorphism():
    # Small, deterministic compute program.
    # Semantics must match across:
    #   - Python VM (thielecpu/vm.py)
    #   - extracted Coq semantics runner (build/extracted_vm_runner)
    #   - RTL sim (thielecpu/hardware/thiele_cpu.v + thiele_cpu_tb.v)
    
    init_mem[0] = 0x29
    init_mem[1] = 0x12
    init_mem[2] = 0x22
    init_mem[3] = 0x03
    
    program_words = [
        _encode_word(0x0A, 0, 0),  # XOR_LOAD r0 <= mem[0]
        _encode_word(0x0A, 1, 1),  # XOR_LOAD r1 <= mem[1]
        _encode_word(0x0A, 2, 2),  # XOR_LOAD r2 <= mem[2]
        _encode_word(0x0A, 3, 3),  # XOR_LOAD r3 <= mem[3]
        _encode_word(0x0B, 3, 0),  # XOR_ADD r3 ^= r0
        _encode_word(0x0B, 3, 1),  # XOR_ADD r3 ^= r1
        _encode_word(0x0C, 0, 3),  # XOR_SWAP r0 <-> r3
        _encode_word(0x07, 2, 4),  # XFER r4 <- r2
        _encode_word(0x0D, 5, 4),  # XOR_RANK r5 := popcount(r4)
        _encode_word(0xFF, 0, 0),  # HALT
    ]
    
    py_regs, py_mem = _run_python_vm(init_mem, init_regs, program_text)
    coq_regs, coq_mem = _run_extracted(init_mem, init_regs, trace_lines)
    rtl_regs, rtl_mem = _run_rtl(program_words, data_words)
    
    assert py_regs == coq_regs == rtl_regs
    assert py_mem == coq_mem == rtl_mem
\end{verbatim}

\subsubsection{State Projection}

Final states are projected to canonical form:
\begin{verbatim}
{
  "pc": <int>,
  "mu": <int>,
  "err": <bool>,
  "regs": [<32 integers>],
  "mem": [<256 integers>],
  "csrs": {"cert_addr": ..., "status": ..., "error": ...},
  "graph": {"modules": [...]}
}
\end{verbatim}

\subsection{Partition Operation Tests}

From \texttt{tests/test\_partition\_isomorphism\_minimal.py}:
\begin{verbatim}
def test_pnew_dedup_singletons_isomorphic():
    # Same singleton regions requested multiple times; canonical semantics dedup.
    indices = [0, 1, 2, 0, 1]  # Duplicates
    
    py_regions = _python_regions_after_pnew(indices)
    coq_regions = _coq_regions_after_pnew(indices)
    rtl_regions = _rtl_regions_after_pnew(indices)
    
    assert py_regions == coq_regions == rtl_regions
\end{verbatim}

This verifies that the canonical normalization (\texttt{normalize\_region}) produces identical results across all layers.

\subsection{Results Summary}

\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Test Suite} & \textbf{Python} & \textbf{Coq} & \textbf{RTL} \\
\hline
Compute Operations & PASS & PASS & PASS \\
Partition PNEW & PASS & PASS & PASS \\
Partition PSPLIT & PASS & PASS & PASS \\
Partition PMERGE & PASS & PASS & PASS \\
XOR Operations & PASS & PASS & PASS \\
$\mu$-Ledger Updates & PASS & PASS & PASS \\
\hline
\textbf{Total} & 100\% & 100\% & 100\% \\
\hline
\end{tabular}
\end{center}

\section{CHSH Correlation Experiments}

\subsection{Bell Test Protocol}

The CHSH inequality bounds correlations in local realistic theories:
\begin{equation}
    S = |E(a,b) - E(a,b') + E(a',b) + E(a',b')| \le 2
\end{equation}

Quantum mechanics predicts $S_{\max} = 2\sqrt{2} \approx 2.828$ (Tsirelson's bound).

\subsection{Partition-Native CHSH}

The Thiele Machine implements CHSH trials through the \texttt{CHSH\_TRIAL} instruction:
\begin{verbatim}
instr_chsh_trial (x y a b : nat) (mu_delta : nat)
\end{verbatim}

Where:
\begin{itemize}
    \item \texttt{x, y}: Input bits (setting choices)
    \item \texttt{a, b}: Output bits (measurement outcomes)
    \item \texttt{mu\_delta}: $\mu$-cost for the trial
\end{itemize}

\subsection{Correlation Bounds}

From \texttt{thielecpu/bell\_semantics.py}:
\begin{verbatim}
TSIRELSON_BOUND = 2 * math.sqrt(2)  # ~2.828

def is_supra_quantum(S: float) -> bool:
    return S > TSIRELSON_BOUND

DEFAULT_ENFORCEMENT_MIN_TRIALS_PER_SETTING = 100
\end{verbatim}

\subsection{Experimental Design}

Test from \texttt{tests/test\_bell\_artifact\_supra\_quantum\_csv.py}:
\begin{enumerate}
    \item Generate CHSH trial sequences
    \item Execute on Python VM with receipt generation
    \item Compute $S$ value from outcome statistics
    \item Verify $\mu$-cost matches declared cost
    \item Verify receipt chain integrity
\end{enumerate}

\subsection{Supra-Quantum Certification}

To certify $S > 2\sqrt{2}$, the trace must include a revelation event:
\begin{verbatim}
Theorem nonlocal_correlation_requires_revelation :
  forall (trace : Trace) (s_init s_final : VMState) (fuel : nat),
    trace_run fuel trace s_init = Some s_final ->
    s_init.(vm_csrs).(csr_cert_addr) = 0 ->
    has_supra_cert s_final ->
    uses_revelation trace \/ ...
\end{verbatim}

Experimental verification confirms:
\begin{itemize}
    \item Traces with $S \le 2$ do not require revelation
    \item Traces with $2 < S \le 2\sqrt{2}$ may use revelation
    \item Traces claiming $S > 2\sqrt{2}$ \textbf{must} use revelation
\end{itemize}

\subsection{Results}

\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Regime} & \textbf{$S$ Value} & \textbf{Revelation} & \textbf{$\mu$-Cost} \\
\hline
Local Realistic & $\le 2.0$ & Not required & 0 \\
Classical Shared & $\le 2.0$ & Not required & $\mu_{\text{seed}}$ \\
Quantum & $\le 2.828$ & Optional & $\mu_{\text{corr}}$ \\
Supra-Quantum & $> 2.828$ & \textbf{Required} & $\mu_{\text{reveal}}$ \\
\hline
\end{tabular}
\end{center}

\section{$\mu$-Ledger Verification}

\subsection{Monotonicity Tests}

From \texttt{tests/test\_mu\_monotonicity.py}:
\begin{verbatim}
def test_mu_monotonic_under_any_trace():
    for _ in range(100):
        trace = generate_random_trace(length=50)
        vm = VM(State())
        vm.run(trace)
        
        mu_values = [s.mu for s in vm.trace]
        for i in range(1, len(mu_values)):
            assert mu_values[i] >= mu_values[i-1]
\end{verbatim}

\subsection{Conservation Tests}

From \texttt{tests/test\_mu\_costs.py}:
\begin{verbatim}
def test_mu_conservation():
    program = [
        ("PNEW", "{0,1,2,3}"),
        ("PSPLIT", "1 {0,1} {2,3}"),
        ("PMERGE", "2 3"),
        ("HALT", ""),
    ]
    
    vm = VM(State())
    vm.run(program)
    
    total_declared = sum(instr.cost for instr in program)
    assert vm.state.mu_ledger.total == total_declared
\end{verbatim}

\subsection{Results}

\begin{itemize}
    \item \textbf{Monotonicity}: 100\% of random traces maintain $\mu_{t+1} \ge \mu_t$
    \item \textbf{Conservation}: Declared costs exactly match ledger increments
    \item \textbf{Irreversibility}: Ledger growth bounds irreversible operations
\end{itemize}

\section{Thermodynamic bridge experiment (publishable plan)}

To connect the ledger to a physical observable, we design a narrowly scoped, falsifiable experiment focused on measurement/erasure thermodynamics.

\subsection{Workload construction}
Use the thermodynamic bridge harness to emit four traces that differ only in which singleton module is revealed from a fixed candidate pool: (1) choose 1 of 2 elements, (2) choose 1 of 4, (3) choose 1 of 16, (4) choose 1 of 64. Instruction count, data size, and clocking remain identical so that only the $\Omega \to \Omega'$ reduction changes. The bundle records per-step $\mu$ (raw and normalized), $|\Omega|$, $|\Omega'|$, normalization flags for Python, Coq extraction, and RTL, and an `evidence\_strict` bit indicating whether normalization was allowed.

\subsection{Bridge prediction}
By construction $\mu \ge \log_2(|\Omega|/|\Omega'|)$ for each trace. Under the thermodynamic postulate $Q_{\min} = k_B T \ln 2 \cdot \mu$, measured energy/heat must scale with $\mu$ at slope $k_B T \ln 2$ (within an explicit inefficiency factor $\epsilon$). Genesis-only traces remain the lone legitimate zero-$\mu$ run; a zero $\mu$ on any nontrivial trace is treated as a test failure, not “alignment.”

\subsection{Instrumentation and analysis}
Run the three traces on instrumented hardware (or a calibrated switching-energy simulator) at fixed temperature $T$. Record per-run energy and environmental metadata. Fit measured energy against $k_B T \ln 2 \cdot \mu$ and report residuals. A sustained sub-linear slope falsifies the bridge; a super-linear slope quantifies overhead. Publish both ledger outputs and raw measurements so reviewers can recompute the bound.

\subsection{Executed thermodynamic bundle (Dec 2025)}
We executed the four Ω→Ω′ traces with the bridge harness (`scripts/thermo_experiment.py`), exporting a JSON artifact (default: `results/thermo_experiment.json`, gitignored; override with `--out`). The runs charge μ via partition discovery only (explicit `MDLACC` omitted to mirror the RTL harness) and capture normalization flags and `evidence\_strict` for μ propagation across layers. Each scenario fails fast if the requested region is not representable in RTL (operand-encoded singleton).

\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
Scenario & $\mu_{\text{python}}$ & $\mu_{\text{raw,extracted}}$ / $\mu_{\text{raw,rtl}}$ & Normalized? & $\log_2(|\Omega|/|\Omega'|)$ & $k_B T \ln 2 \cdot \mu$ (J) & $\mu / \log_2(|\Omega|/|\Omega'|)$ \\
\hline
singleton\_from\_2 & 2 & 2 / 2 & no & 1 & $5.74 \times 10^{-21}$ & 2.0 \\
singleton\_from\_4 & 3 & 3 / 3 & no & 2 & $8.61 \times 10^{-21}$ & 1.5 \\
singleton\_from\_16 & 5 & 5 / 5 & no & 4 & $1.44 \times 10^{-20}$ & 1.25 \\
singleton\_from\_64 & 7 & 7 / 7 & no & 6 & $2.02 \times 10^{-20}$ & 1.167 \\
\hline
\end{tabular}
\end{center}

All four traces satisfy $\mu \ge \log_2(|\Omega|/|\Omega'|)$ and align on regs/mem/μ without normalization. The harness encodes an explicit μ-delta into the Coq trace and RTL instruction word, and the Python VM consumes the same μ-delta (disabling implicit MDLACC) so that $\mu_{\text{raw}}$ matches across layers. With this encoding in place, `EVIDENCE\_STRICT` runs succeed for these workloads.

\subsection{Structural heat anomaly workload}
To mirror the thesis claim that structured insight carries binding energy, the structural-heat harness (`scripts/structural_heat_experiment.py`) executes two erase tasks over the same 1~GiB buffer: \texttt{erase\_random\_noise} (no certificate) and \texttt{erase\_structured\_sorted} (explicit $\log_2(n!)$ certificate for $n=2^{20}$ sorted records). The emitted artifact (`results/structural_heat_experiment.json`) records μ totals, Landauer lower bounds, and slack for both workloads. The structured erase pays dramatically larger μ—raising the Landauer floor accordingly—while keeping instruction count and data volume fixed, turning the “structural heat” prediction into a measurable differential once hardware power logging is connected.

\subsection{Ledger-constrained time dilation workload}
\label{sec:ledger_time_dilation}
To test the “speed limit” claim, the time-dilation harness (`scripts/time_dilation_experiment.py`) fixes a μ budget per global tick (32 μ-bits) and varies only the communication payload queued each tick before local computation is allowed. The emitted JSON artifact (default: `results/time_dilation_experiment.json`, gitignored; override with `--out`) reports four scenarios: no communication (32 compute steps/tick), light (28 steps/tick), moderate (20 steps/tick), and heavy (8 steps/tick) while holding the per-tick μ spend constant at 2048 μ over 64 ticks. As communication μ increases, the compute rate monotonically falls, demonstrating that signal propagation consumes the same finite μ budget that would otherwise power local evolution—a ledger-level analogue of time dilation under a fixed speed-of-ledger constraint. Evidence-strict extensions can wire this harness to EMIT traces and RTL exports to enforce the same trade-off across layers.

\section{Performance Benchmarks}

\subsection{Instruction Throughput}

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Mode} & \textbf{Ops/sec} & \textbf{Overhead} \\
\hline
Raw Python VM & $\sim 10^6$ & Baseline \\
Receipt Generation & $\sim 10^4$ & 100$\times$ \\
Full Tracing & $\sim 10^3$ & 1000$\times$ \\
\hline
\end{tabular}
\end{center}

\subsection{Receipt Chain Overhead}

Each step generates:
\begin{itemize}
    \item Pre-state SHA-256 hash: 32 bytes
    \item Post-state SHA-256 hash: 32 bytes
    \item Instruction encoding: $\sim$50 bytes
    \item Chain link: 32 bytes
\end{itemize}

Total per-step overhead: $\sim$150 bytes

\subsection{Hardware Synthesis Results}

From \texttt{scripts/run\_synthesis.sh}:

\textbf{YOSYS\_LITE Configuration:}
\begin{verbatim}
NUM_MODULES = 4
REGION_SIZE = 16
\end{verbatim}
\begin{itemize}
    \item LUTs: $\sim$2,500
    \item Flip-Flops: $\sim$1,200
    \item Target: Xilinx 7-series
\end{itemize}

\textbf{Full Configuration:}
\begin{verbatim}
NUM_MODULES = 64
REGION_SIZE = 1024
\end{verbatim}
\begin{itemize}
    \item LUTs: $\sim$45,000
    \item Flip-Flops: $\sim$35,000
    \item Target: Xilinx UltraScale+
\end{itemize}

\section{Comprehensive Test Suite}

\subsection{Test Categories}

The repository contains 156 test files covering:

\begin{center}
\begin{tabular}{|l|c|}
\hline
\textbf{Category} & \textbf{Test Count} \\
\hline
Isomorphism (Python/Coq/RTL) & 15 \\
Partition Operations & 12 \\
$\mu$-Ledger & 8 \\
CHSH/Bell Tests & 10 \\
Receipt Verification & 6 \\
Security/Adversarial & 5 \\
Performance Benchmarks & 8 \\
\hline
\textbf{Total} & $>$60 core tests \\
\hline
\end{tabular}
\end{center}

\subsection{CI Integration}

Every commit triggers:
\begin{verbatim}
make -C coq core                           # Coq compilation
pytest tests/test_partition_isomorphism_minimal.py
pytest tests/test_rtl_compute_isomorphism.py
python scripts/inquisitor.py               # Admit/axiom scan
\end{verbatim}

\subsection{Execution Gates (from AGENTS.md)}

\textbf{Fast Local Gates:}
\begin{verbatim}
make -C coq core
pytest -q tests/test_partition_isomorphism_minimal.py
pytest -q tests/test_rtl_compute_isomorphism.py
\end{verbatim}

\textbf{Full Foundry Gate:}
\begin{verbatim}
bash scripts/forge_artifact.sh
\end{verbatim}

\section{Reproducibility}

\subsection{Artifact Directory}

Key artifacts in \texttt{artifacts/}:
\begin{itemize}
    \item \texttt{isomorphism\_test\_results.json}: 3-way comparison results
    \item \texttt{cross\_platform\_isomorphism\_results.json}: Platform-specific tests
    \item \texttt{mu\_core\_synth.json}: Synthesis reports
    \item \texttt{MANIFEST.sha256}: Content hashes for all artifacts
\end{itemize}

\subsection{Docker Reproducibility}

\begin{verbatim}
docker build -t thiele-machine .
docker run thiele-machine make -C coq core
docker run thiele-machine pytest tests/
\end{verbatim}

\section{Summary}

The evaluation demonstrates:
\begin{enumerate}
    \item \textbf{3-Layer Isomorphism}: Python, Coq extraction, and RTL produce identical state projections for all tested instruction sequences
    \item \textbf{CHSH Correctness}: Supra-quantum certification requires revelation as predicted by theory
    \item \textbf{$\mu$-Conservation}: The ledger is monotonic and exactly tracks declared costs
    \item \textbf{Scalability}: Hardware synthesis targets modern FPGAs with reasonable resource utilization
    \item \textbf{Reproducibility}: All results can be reproduced via the provided test suite and artifacts
\end{enumerate}

The empirical results validate the theoretical claims: the Thiele Machine enforces structural accounting as a physical law, not merely as a convention.
